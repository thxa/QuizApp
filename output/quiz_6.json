[{"_id": 305, "question": "305 # A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?", "options": ["A. Use an Amazon Aurora global database with a pilot light deployment.", "B. Use an Amazon Aurora global database with a warm standby deployment.", "C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.", "D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment. Correct Answer: B Question #:274"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248551393458&usg=AOvVaw0xc6z-0i0_3gH9kM90zRFU"]}, {"_id": 306, "question": "306 # A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally efficient way?", "options": ["A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.", "B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.", "C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.", "D. Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times. Correct Answer: B Question #:275"], "explain": "", "answers": [], "resources": []}, {"_id": 307, "question": "307 # A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?", "options": ["A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.", "B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.", "C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.", "D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens. Correct Answer: C Answers A & D are incorrect because the question states to keep costs to a minimum. This means, NOT running 20 instances from the start. Answers B & C are both a better options. The problem in the morning is not that there should have been 20 instances running and that they are not running. The problem is that the auto scaling is not responding fast enough to the increase in demand. That is why decreasing the cool down period will make the auto scaling more aggressive (and responsive) but will still run less than 20 instances from the get go, and therefore will cost less money. Also, AWS recommends using target scaling as much as possible. Question #:276"], "explain": "", "answers": [], "resources": []}, {"_id": 308, "question": "308 # A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application\u2019 s data layer that uses Oracle- https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248552620886&usg=AOvVaw2u0NknkAteqxyI9HbIBwxv 149 specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off. Whatshould a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)", "options": ["A. Configure storage Auto Scaling on the RDS for Oracle instance.", "B. Migrate the database to Amazon Aurora to use Auto Scaling storage.", "C. Configure an alarm on the RDS for Oracle instance for low free storage space.", "D. Configure the Auto Scaling group to use the average CPU as the scaling metric.", "E. Configure the Auto Scaling group to use the average free memory as the scaling metric. Correct Answer: AD Question #:277"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248552620886&usg=AOvVaw2u0NknkAteqxyI9HbIBwxv"]}, {"_id": 309, "question": "309 # A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?", "options": ["A. Use AWS Storage Gateway for files to store and process the video content.", "B. Use AWS Storage Gateway for volumes to store and process the video content.", "C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).", "D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing. Correct Answer: D Question #:278"], "explain": "", "answers": [], "resources": []}, {"_id": 310, "question": "310 # A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.", "B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.", "C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.", "D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.", "E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. Correct Answer: BE Question #:279"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248553341726&usg=AOvVaw02l8b-mojj77q7J1OUyCNz"]}, {"_id": 311, "question": "311 # A company has an application that is backed by an Amazon DynamoDB table. The company\u2019s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?", "options": ["A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.", "B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.", "C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.", "D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years. Correct Answer: A Question #:280"], "explain": "", "answers": [], "resources": []}, {"_id": 312, "question": "312 # A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company\u2019s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.", "B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.", "C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.", "D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight. Correct Answer: B Question #:281"], "explain": "", "answers": [], "resources": []}, {"_id": 313, "question": "313 # A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?", "options": ["A. Enable a Multi-AZ deployment for the DB instance.", "B. Enable auto scaling for the DB instance in one Availability Zone.", "C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.", "D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks. Correct Answer: A Question #:282"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248553967759&usg=AOvVaw00Yp8xyBJrTCdrtD5b7DT6"]}, {"_id": 314, "question": "314 # A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?", "options": ["A. Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.", "B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.", "C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.", "D. Configure the security group for the ALB to allow any TCP traffic on any port. Correct Answer: B Question #:283"], "explain": "", "answers": [], "resources": []}, {"_id": 315, "question": "315 # A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?", "options": ["A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.", "B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.", "C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.", "D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage. Correct Answer: D Question #:284"], "explain": "", "answers": [], "resources": []}, {"_id": 316, "question": "316 # As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information. Which solution meets these requirements?", "options": ["A. Run a query with Amazon Athena to generate the report.", "B. Create a report in Cost Explorer and download the report.", "C. Access the bill details from the billing dashboard and download the bill.", "D. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES). Correct Answer: B Question #:285"], "explain": "", "answers": [], "resources": []}, {"_id": 317, "question": "317 # A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost- effectively? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248554553361&usg=AOvVaw3LCwJSw1hu972ojqC0kIVb 152", "options": ["A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.", "B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).", "C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.", "D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail. Correct Answer: B Question #:286"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248554553361&usg=AOvVaw3LCwJSw1hu972ojqC0kIVb"]}, {"_id": 318, "question": "318 # A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website\u2019s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?", "options": ["A. Add an Application Load Balancer.", "B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.", "C. Invalidate the CloudFront cache.", "D. Use AWS Certificate Manager (ACM) to validate the website\u2019s SSL certificate. Correct Answer: C Question #:287"], "explain": "", "answers": [], "resources": []}, {"_id": 319, "question": "319 # A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?", "options": ["A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.", "B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.", "C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.", "D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers. Correct Answer: B Question #:288"], "explain": "", "answers": [], "resources": []}, {"_id": 320, "question": "320 # A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Create an Amazon S3 Standard bucket with access to the web servers.", "B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248555496287&usg=AOvVaw2GFk55VI55VjJckj-pyVWN 153", "C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248555496287&usg=AOvVaw2GFk55VI55VjJckj-pyVWN"]}, {"_id": 321, "question": "321 # A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner?", "options": ["A. Apply an S3 bucket policy that grants read access to the S3 bucket.", "B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.", "C. Embed an access key and a secret key in the Lambda function\u2019s code to grant the required IAM permissions for read access to the S3 bucket.", "D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account. Correct Answer: B Question #:290"], "explain": "", "answers": [], "resources": []}, {"_id": 322, "question": "322 # A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment. Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?", "options": ["A. Dedicated Instances only", "B. On-Demand Instances only", "C. A mix of On-Demand Instances and Spot Instances", "D. A mix of On-Demand Instances and Reserved Instances Correct Answer: C Question #:: 271"], "explain": "", "answers": [], "resources": []}, {"_id": 323, "question": "323 # A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the \u2018same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete. Whatshould the solutions architect do to meet these requirements?", "options": ["A. Increase the minimum capacity for the Auto Scaling group.", "B. Increase the maximum capacity for the Auto Scaling group.", "C. Configure scheduled scaling to scale up to the desired compute level.", "D. Change the scaling policy to add more EC2 instances during each scaling operation. Selected Answer: C", "C. Configure scheduled scaling to scale up to the desired compute level. By configuring scheduled scaling, the solutions architect can set the Auto Scaling group to automatically scale up to the desired compute level at a specific time (1AM) when the batch job starts and then automatically scale down after the job is complete. This will allow the desired EC2 capacity to be reached quickly and also help in reducing the cost."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248556104501&usg=AOvVaw1XWeBs2-nyfWmUcc5rC6OW"]}, {"_id": 324, "question": "324 # A company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website\u2019s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and efficiently regardless of a user\u2019s location. However, the company does not want to recreate the existing architecture across multiple Regions. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.", "B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.", "C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.", "D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy. Selected Answer: B Configuring caching based on the language of the viewer If you want CloudFront to cache different versions of your objects based on the language specified in the request, configure CloudFront to forward the Accept-Language header to your origin."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html", "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-"]}, {"_id": 325, "question": "325 # A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?", "options": ["A. Use an Amazon Aurora global database with a pilot light deployment.", "B. Use an Amazon Aurora global database with a warm standby deployment.", "C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.", "D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment. Selected Answer: B Option A is incorrect because while Amazon Aurora global database is a good solution for disaster recovery, pilot light deployment provides only a minimalistic setup and would require manual intervention to make the DR Region fully operational, which increases the recovery time. Option B is a better choice than Option A as it provides a warm standby deployment, which is an automated and more scalable setup than pilot light deployment. In this setup, the database is replicated to the DR"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248556779298&usg=AOvVaw3ve2gVq5ZaE2d02K8hSLhQ", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html&sa=D&source=apps-viewer-frontend&ust=1720248556779334&usg=AOvVaw1hkriKns_xxshh63kQ4vu9", "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html%23header-caching-web-language&sa=D&source=apps-viewer-frontend&ust=1720248556779351&usg=AOvVaw1UrPeDkbbyF2FzOaVwNGHA", "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html#header-caching-web-language"]}, {"_id": 326, "question": "326 # A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally efficient way?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 326, "question": "326 # A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally efficient way? A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts. B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation. C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times. D. Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times. Selected Answer: B Option A: Add complexity and management overhead. Option B: Creating AMIs for backup and using AWS CloudFormation for infrastructure deployment in the secondary Region is a more streamlined and automated approach. CloudFormation allows you to define and provision resources in a declarative manner, making it easier to maintain and update your infrastructure. This solution is more operationally efficient compared to Option", "options": ["A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.", "B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.", "C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.", "A. Option C: could be expensive and not fully aligned with the requirement of using the fewest possible AWS resources during normal operations. Option D: might not be sufficient for meeting the DR requirements, as Availability Zones are still within the same AWS Region and might be subject to the same regional-level failures. Question #:: 275"], "explain": "", "answers": [], "resources": []}, {"_id": 327, "question": "327 # A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 327, "question": "327 # A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum? A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens. B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period. C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period. D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens. Selected Answer: C At first, I thought the answer is", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 327, "question": "327 # A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum? A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens. B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period. C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period. D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens. Selected Answer: C At first, I thought the answer is A. But it is C. It seems that there is no information in the question about CPU or Memory usage. So, we might think the answer is", "options": ["A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.", "B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.", "C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.", "A. But it is", "A. why? because what we need is to have the"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248557369400&usg=AOvVaw1J1U6HM2cAQcwGs2e8QNpu"]}, {"_id": 328, "question": "328 # A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application\u2019 s data layer that uses Oracle- specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off. Whatshould a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)", "options": ["A. Configure storage Auto Scaling on the RDS for Oracle instance.", "B. Migrate the database to Amazon Aurora to use Auto Scaling storage.", "C. Configure an alarm on the RDS for Oracle instance for low free storage space.", "D. Configure the Auto Scaling group to use the average CPU as the scaling metric.", "E. Configure the Auto Scaling group to use the average free memory as the scaling metric. Selected Answer: AD A) Configure storage Auto Scaling on the RDS for Oracle instance. = Makes sense. With RDS Storage Auto Scaling, you simply set your desired maximum storage limit, and Auto Scaling takes care of the rest. B) Migrate the database to Amazon Aurora to use Auto Scaling storage. = Scenario specifies application's data layer uses Oracle-specific PL/SQL functions. This rules out migration to Aurora. C) Configure an alarm on the RDS for Oracle instance for low free storage space. = You could do this but what does it fix? Nothing. The CW notification isn't going to trigger anything. D) Configure the Auto Scaling group to use the average CPU as the scaling metric. = Makes sense. The CPU utilization is the precursor to the storage outage. When the ec2 instances are overloaded, the RDS instance storage hits its limits, too. Question #:: 277"], "explain": "", "answers": [], "resources": []}, {"_id": 329, "question": "329 # A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?", "options": ["A. Use AWS Storage Gateway for files to store and process the video content.", "B. Use AWS Storage Gateway for volumes to store and process the video content.", "C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).", "D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing. Selected Answer: D he most cost-effective storage solution in this scenario would be:", "D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing. This option provides the lowest-cost storage by using: Amazon S3 for large-scale, durable, and inexpensive storage of the video content. S3 storage costs are significantly"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248557967783&usg=AOvVaw0vuy2PJY5fJDu0U-pXOi6N"]}, {"_id": 330, "question": "330 # A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.", "B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.", "C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.", "D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.", "E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. Selected Answer: BE Data in hierarchies : Amazon DynamoDB", "B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month. Sensitive Info: Amazon Macie", "E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription. Question #:: 279"], "explain": "", "answers": [], "resources": []}, {"_id": 331, "question": "331 # A company has an application that is backed by an Amazon DynamoDB table. The company\u2019s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?", "options": ["A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.", "B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.", "C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.", "D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years. Selected Answer: A This solution satisfies the requirements in the following ways: AWS Backup will automatically take full backups of the DynamoDB table on the schedule defined in the backup plan (the first of each month)."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248558584218&usg=AOvVaw21JgZ0CPoAvsLsgACjZ95t"]}, {"_id": 332, "question": "332 # A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company\u2019s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.", "B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.", "C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.", "D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight. Selected Answer: B OptionB: Amazon Athena allows you to run standard SQL queries directly on the data stored in the S3 bucket. Amazon QuickSight is a business intelligence (BI) service that allows you to create interactive and visual dashboards to analyze data. You can connect Amazon QuickSight to Amazon Athena to visualize the results of your SQL queries from the CloudFront logs. Question #:: 281"], "explain": "", "answers": [], "resources": []}, {"_id": 333, "question": "333 # A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?", "options": ["A. Enable a Multi-AZ deployment for the DB instance.", "B. Enable auto scaling for the DB instance in one Availability Zone.", "C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.", "D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks. Read Replicas: Read Replicas are asynchronous and support read scalability. It is uese to improve performance. Read Replicas can be in the same region or in a different region for disaster recovery purposes, but this involves manual intervention, which means Read Replicas do not provide automatic failover and requires DNS updates and application changes Multi-AZ: Multi-AZ maintains a synchronous standby replica of the primary instance in a different Availability Zone within the same region. Multi-AZ deployments provide high availability and automatic failover. Option A is better choice with respect to below statement, \"the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases.\" Selected Answer: A Question #:: 282"], "explain": "", "answers": [], "resources": []}, {"_id": 334, "question": "334 # A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248559201226&usg=AOvVaw3HRkaPaZOWY8bjAPyyheAZ 159 traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?", "options": ["A. Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.", "B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.", "C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.", "D. Configure the security group for the ALB to allow any TCP traffic on any port. Selected Answer: B configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB. This ensures that only the traffic originating from the ALB is allowed access to the EC2 instances in the private subnet, while denying any other traffic from other sources. The other options do not provide a suitable solution to meet the stated requirements. Question #:: 283"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248559201226&usg=AOvVaw3HRkaPaZOWY8bjAPyyheAZ"]}, {"_id": 335, "question": "335 # A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?", "options": ["A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.", "B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.", "C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.", "D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage. Selected Answer: D This solution satisfies the needs in the following ways: Amazon EC2 provides a seamless migration path for the existing server-based applications without code changes. The simulation app can run on Linux EC2 instances and the visualization app on Windows EC2 instances. Amazon FSx for NetApp ONTAP provides highly performant file storage that is accessible via both NFS and SMB. This allows the simulation app to write to NFS shares as currently designed, and the visualization app to access the same data via SMB. FSx for NetApp ONTAP ensures the data is synchronized and up to date across the file systems. This addresses the data duplication issues of the current setup. Resources can be scaled efficiently since EC2 and FSx provide scalable compute and storage on demand. Question #:: 284"], "explain": "", "answers": [], "resources": []}, {"_id": 336, "question": "336 # As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information. Which solution meets these requirements?", "options": ["A. Run a query with Amazon Athena to generate the report.", "B. Create a report in Cost Explorer and download the report.", "C. Access the bill details from the billing dashboard and download the bill.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248559746982&usg=AOvVaw0xvm9tr1nizpK0sqJMVNO6 160"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248559746982&usg=AOvVaw0xvm9tr1nizpK0sqJMVNO6"]}, {"_id": 337, "question": "337 # A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost- effectively?", "options": ["A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.", "B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).", "C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.", "D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail. Selected Answer: B This solution is the most cost-efficient for the anticipated 100 monthly visits because: API Gateway charges are based on API calls. With only 100 visits, charges would be minimal. AWS Lambda provides compute time for the backend code in increments of 100ms, so charges would also be negligible for this workload. Amazon SES is used only for sending emails from the submitted contact forms. SES has a generous free tier of 62,000 emails per month, so there would be no charges for sending the contact emails. No EC2 instances or other infrastructure needs to be run and paid for. Correct answer is", "B."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-"]}, {"_id": 338, "question": "338 # A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website\u2019s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?", "options": ["A. Add an Application Load Balancer.", "B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.", "C. Invalidate the CloudFront cache.", "D. Use AWS Certificate Manager (ACM) to validate the website\u2019s SSL certificate. Selected Answer: C", "C. Invalidate the CloudFront cache. Problem is the CF cache. After invalidating the CloudFront cache, CF will be forces to read the updated static page from the S3 and the S3 changes will start being visible. Question #:: 287"], "explain": "", "answers": [], "resources": []}, {"_id": 339, "question": "339 # A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248560269641&usg=AOvVaw2FnFLHDOK-q48-kTnBk8GQ https://www.google.com/url?q=https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/&sa=D&source=apps-viewer-frontend&ust=1720248560269677&usg=AOvVaw3ymKue_01JGZrgJ_cEkiJt https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/ 161 SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?", "options": ["A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.", "B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.", "C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.", "D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers. It is B: A: Incorrect> FSx file Gateway designed for low latency and efficient access to in-cloud FSx for Windows File Server file shares from your on-premises facility. B: Correct> This solution will allow the company to host all three tiers on Amazon EC2 instances while using Amazon FSx for Windows File Server to provide Windows-based file sharing between the tiers. This will allow the company to use specific features of SQL Server, such as native backups and Data Quality Services, while sharing files for processing between the tiers. C: Incorrect> Currently, Amazon EFS supports the NFSv4.1 protocol and does not natively support the SMB protocol, and can't be used in Windows instances yet. D: Incorrect> Amazon EBS is a block-level storage solution that is typically used to store data at the operating system level, rather than for file sharing between servers. Question #:: 288"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248560269641&usg=AOvVaw2FnFLHDOK-q48-kTnBk8GQ", "https://www.google.com/url?q=https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/&sa=D&source=apps-viewer-frontend&ust=1720248560269677&usg=AOvVaw3ymKue_01JGZrgJ_cEkiJt", "https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/"]}, {"_id": 340, "question": "340 # A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. Whatshould a solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 340, "question": "340 # A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. Whatshould a solutions architect do to meet these requirements? A. Create an Amazon S3 Standard bucket with access to the web servers. B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers. D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers. Selected Answer: C This solution satisfies the needs in the following ways: EFS provides a fully managed elastic network file system that can be mounted on multiple EC2 instances concurrently. The EFS file system appears as a standard file system mount on the Linux web servers, requiring no application changes. The servers can access shared files as if they were on local storage. EFS is highly available, durable, and scalable, providing a robust shared storage solution. The other options would require modifying the application or do not provide a standard file system:", "options": ["A. Create an Amazon S3 Standard bucket with access to the web servers.", "B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.", "C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.", "A. S3 does not provide a standard file system mount or share. The application would need to be changed to access S3 storage.", "B. CloudFront is a content delivery network and caching service. It does not provide a file system mount or share and would require application changes.", "D. EBS volumes can only attach to a single EC2 instance. They cannot be mounted by multiple servers concurrently and do not provide a shared file system. Question #:: 289"], "explain": "", "answers": [], "resources": []}, {"_id": 341, "question": "341 # A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248560816082&usg=AOvVaw1O1hpnfWr_xnSSPE9i8uzl 162", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 341, "question": "341 # A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248560816082&usg=AOvVaw1O1hpnfWr_xnSSPE9i8uzl 162 A. Apply an S3 bucket policy that grants read access to the S3 bucket. B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket. C. Embed an access key and a secret key in the Lambda function\u2019s code to grant the required IAM permissions for read access to the S3 bucket. D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account. Selected Answer: B This solution satisfies the needs in the most secure manner: An IAM role provides temporary credentials to the Lambda function to access AWS resources. The function does not have persistent credentials. The IAM policy grants least privilege access by specifying read access only to the specific S3 bucket needed. Access is not granted to all S3 buckets. If the Lambda function is compromised, the attacker would only gain access to the one specified S3 bucket. They would not receive broad access to resources. The other options are less secure:", "options": ["A. Apply an S3 bucket policy that grants read access to the S3 bucket.", "B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.", "C. Embed an access key and a secret key in the Lambda function\u2019s code to grant the required IAM permissions for read access to the S3 bucket.", "A. A bucket policy grants open access to a resource. It is a less granular way to provide access and grants more privilege than needed.", "C. Embedding access keys in code is extremely insecure and against best practices. The keys provide full access and are at major risk of compromise if the code leaks.", "D. Granting access to all S3 buckets provides far too much privilege if only one bucket needs access. It greatly expands the impact if compromised. Question #:: 290"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248560816082&usg=AOvVaw1O1hpnfWr_xnSSPE9i8uzl"]}, {"_id": 342, "question": "342 # A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment. Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?", "options": ["A. Dedicated Instances only", "B. On-Demand Instances only", "C. A mix of On-Demand Instances and Spot Instances", "D. A mix of On-Demand Instances and Reserved Instances Selected Answer: C To optimize cost savings without making a long-term commitment, a mix of On-Demand Instances and Spot Instances would be the best EC2 instance purchasing option to recommend. By combining On- Demand and Spot Instances, the company can take advantage of the cost savings offered by Spot Instances during periods of low demand while maintaining the reliability and stability of On-Demand Instances during periods of high demand. This provides a cost-effective solution that can scale with user demand without making a long-term commitment."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-"]}, {"_id": 343, "question": "343 # A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company\u2019s users are using a custom HTTP client that does not support cookies. Some of the company\u2019s users are unable to change the hardcoded URLs that they are using for access. Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)", "options": ["A. Signed cookies", "B. Signed URLs", "C. AWS AppSync", "D. JSON Web Token (JWT)", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248561366232&usg=AOvVaw2KtJwyzvAIUyx16TxyQZXt https://www.google.com/url?q=https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html&sa=D&source=apps-viewer-frontend&ust=1720248561366293&usg=AOvVaw2ijAqqUmHKB6fyxbqoG2gx https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html 163", "E. AWS Secrets Manager Selected Answer: AB Signed cookies would allow the media company to authorize access to related content (like HLS video segments) with a single signature, minimizing implementation overhead. This works for users that can support cookies. Signed URLs would allow the media company to sign each URL individually to control access, supporting users that cannot use cookies. By embedding the signature in the URL, existing hardcoded URLs would not need to change.", "C. AWS AppSync - This is for building data-driven apps with real-time and offline capabilities. It does not directly help with securing streaming content.", "D. JSON Web Token (JWT) - Although JWTs can be used for authorization, they would require the client to get a token and validate/check access on the server for each request. This does not work for hardcoded URLs and minimizes impact."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248561366232&usg=AOvVaw2KtJwyzvAIUyx16TxyQZXt", "https://www.google.com/url?q=https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html&sa=D&source=apps-viewer-frontend&ust=1720248561366293&usg=AOvVaw2ijAqqUmHKB6fyxbqoG2gx", "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html"]}, {"_id": 344, "question": "344 # A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)", "options": ["A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.", "B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.", "C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.", "D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.", "E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3. Selected Answer: AB The solutions that meet the requirements of streaming real-time data, transforming the data before writing to S3, and querying the transformed data using SQL are A and", "B. Option C: This option is not ideal for streaming real-time data as AWS DMS is not optimized for real-time data ingestion. Option D & E: These option are not recommended as the Amazon RDS query editor is not designed for querying data in S3, and it is not efficient for running complex queries. Question #:: 293"], "explain": "", "answers": [], "resources": []}, {"_id": 345, "question": "345 # A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred. Which solution meets these requirements?", "options": ["A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on- premises systems to mount the Snowball S3 endpoint to provide local access to the data.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248561906153&usg=AOvVaw2vzo8gRJhK2m3fI6qgtD2p 164", "B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.", "C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248561906153&usg=AOvVaw2vzo8gRJhK2m3fI6qgtD2p", "https://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html"]}, {"_id": 346, "question": "346 # An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet. How should a solutions architect configure access to meet these requirements?", "options": ["A. Create a private hosted zone by using Amazon Route 53.", "B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.", "C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.", "D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket. ANSWER - B"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.htmlR"]}, {"_id": 347, "question": "347 # An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.", "B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.", "C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.", "D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table. Selected Answer: B Storing the raw data in S3 provides a durable, scalable data lake. S3 requires little ongoing management overhead. S3 Object Lambda can be used to filter and process the data on retrieval transparently. This minimizes operational overhead by avoiding the need to preprocess and store multiple transformed copies of the data. Only one copy of the data needs to be stored and maintained in S3. S3 Object Lambda will transform the data on read based on the requesting application. No additional applications"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248562691241&usg=AOvVaw1c_W8kAxfBaRvb5Q2tCPog", "https://www.google.com/url?q=https://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html&sa=D&source=apps-viewer-frontend&ust=1720248562691279&usg=AOvVaw3rGe-b-Fg1BRMtOZaAmQDf", "https://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.htmlR&sa=D&source=apps-viewer-frontend&ust=1720248562691293&usg=AOvVaw3XHh2R0gGAxVneaoiPE50X", "https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.htmlR"]}, {"_id": 348, "question": "348 # A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. Whatis the SMALLEST CIDR block that meets these requirements?", "options": ["A. 10.0.1.0/32", "B. 192.168.0.0/24", "C. 192.168.1.0/32", "D. 10.0.1.0/24 Selected Answer: D Option A (10.0.1.0/32) is invalid - a /32 CIDR prefix is a host route, not a VPC range. Option B (192.168.0.0/24) overlaps the development VPC and so cannot be used. Option C (192.168.1.0/32) is invalid - a /32 CIDR prefix is a host route, not a VPC range. Option D (10.0.1.0/24) satisfies the non- overlapping CIDR requirement but is a larger block than needed. Since only two VPCs need to be peered, a /24 block provides more addresses than necessary. Question #:: 297"], "explain": "", "answers": [], "resources": []}, {"_id": 349, "question": "349 # A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?", "options": ["A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.", "B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.", "C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.", "D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248563508285&usg=AOvVaw1RiT6qKT9xIuWHkG-TDKEl"]}, {"_id": 350, "question": "350 # A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?", "options": ["A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.", "B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.", "C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.", "D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment. Selected Answer: C D is completely wrong, because each subnet must reside entirely within one Availability Zone and cannot span zones. By launching AWS resources in separate Availability Zones, you can protect your applications from the failure of a single Availability Zone. A subnet must reside within a single Availability Zone."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within%20a%2"]}, {"_id": 351, "question": "351 # A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub- millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?", "options": ["A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume\u2019 tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248564195447&usg=AOvVaw2ZYgwaVG1rTbxidBUGLT9S https://www.google.com/url?q=https://aws.amazon.com/vpc/faqs/%23:~:text%3DCan%2520a%2520subnet%2520span%2520Availability,within%2520a%252&sa=D&source=apps-viewer-frontend&ust=1720248564195491&usg=AOvVaw2nX_or9vt30205_jOep8vz https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within%20a%2 167", "B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.", "C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248564195447&usg=AOvVaw2ZYgwaVG1rTbxidBUGLT9S", "https://www.google.com/url?q=https://aws.amazon.com/vpc/faqs/%23:~:text%3DCan%2520a%2520subnet%2520span%2520Availability,within%2520a%252&sa=D&source=apps-viewer-frontend&ust=1720248564195491&usg=AOvVaw2nX_or9vt30205_jOep8vz", "https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within%20a%2", "https://aws.amazon.com/fsx/lustre/faqs/?nc=sn&loc=5", "https://aws.amazon.com/fsx/netapp-"]}, {"_id": 352, "question": "352 # A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application\u2019s database storage continues to grow over time. Whatshould a solutions architect do to meet these requirements MOST cost-effectively?", "options": ["A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.", "B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.", "C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.", "D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances. Selected Answer: C Amazon EC2 Reserved Instances allow for significant cost savings compared to On-Demand instances for long-running, steady-state workloads like this one. Reserved Instances provide a capacity reservation, so the instances are guaranteed to be available for the duration of the reservation period. Amazon Aurora is a highly scalable, cloud-native relational database service that is designed to be compatible with MySQL and PostgreSQL. It can automatically scale up to meet growing storage requirements, so it can accommodate the application's database storage needs over time. By using Reserved Instances for Aurora, the cost savings will be significant over the long term."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html"]}, {"_id": 353, "question": "353 # A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248564943183&usg=AOvVaw2Ewt9XJXNUdzRnTTB8nJLC https://www.google.com/url?q=https://aws.amazon.com/fsx/lustre/faqs/?nc%3Dsn%26loc%3D5&sa=D&source=apps-viewer-frontend&ust=1720248564943223&usg=AOvVaw1tsDmmcQ9R1zVpuDV2Vf25 https://aws.amazon.com/fsx/lustre/faqs/?nc=sn&loc=5 https://www.google.com/url?q=https://aws.amazon.com/fsx/netapp-ontap/faqs/&sa=D&source=apps-viewer-frontend&ust=1720248564943234&usg=AOvVaw07eQsKQFUdRTWYDSDVmLET https://aws.amazon.com/fsx/netapp-ontap/faqs/ https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html&sa=D&source=apps-viewer-frontend&ust=1720248564943243&usg=AOvVaw324fBIlwCSphk9KrjyXAP4 https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html 168 must be able to reply to the SMS messages. The company must store the responses for a year for analysis. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.", "B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.", "C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.", "D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving. Correct Answer: B Question #:202"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248564943183&usg=AOvVaw2Ewt9XJXNUdzRnTTB8nJLC", "https://www.google.com/url?q=https://aws.amazon.com/fsx/lustre/faqs/?nc%3Dsn%26loc%3D5&sa=D&source=apps-viewer-frontend&ust=1720248564943223&usg=AOvVaw1tsDmmcQ9R1zVpuDV2Vf25", "https://aws.amazon.com/fsx/lustre/faqs/?nc=sn&loc=5", "https://www.google.com/url?q=https://aws.amazon.com/fsx/netapp-ontap/faqs/&sa=D&source=apps-viewer-frontend&ust=1720248564943234&usg=AOvVaw07eQsKQFUdRTWYDSDVmLET", "https://aws.amazon.com/fsx/netapp-ontap/faqs/", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html&sa=D&source=apps-viewer-frontend&ust=1720248564943243&usg=AOvVaw324fBIlwCSphk9KrjyXAP4", "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html"]}, {"_id": 354, "question": "354 # A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.", "B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket\u2019s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.", "C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket\u2019s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.", "D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation. Correct Answer: B Question #:203"], "explain": "", "answers": [], "resources": []}, {"_id": 355, "question": "355 # The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. Whatshould a solutions architect recommend to resolve this issue?", "options": ["A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.", "B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.", "C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.", "D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue. Correct Answer: D Question #:204"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248565707463&usg=AOvVaw093Ud2hUQZ9YLF4eHFXEv4"]}, {"_id": 356, "question": "356 # An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?", "options": ["A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.", "B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.", "C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.", "D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access. Correct Answer: C Question #:205"], "explain": "", "answers": [], "resources": []}, {"_id": 357, "question": "357 # A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to host its website on AWS and to use Amazon CloudFront. The company\u2019s solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin. Which solution will meet these requirements?", "options": ["A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.", "B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.", "C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.", "D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client. Correct Answer: C Question #:206"], "explain": "", "answers": [], "resources": []}, {"_id": 358, "question": "358 # A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company\u2019s account. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.", "B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected.", "C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248566501701&usg=AOvVaw0ikGLUDxkLAYMZAPE1EKUP 170"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248566501701&usg=AOvVaw0ikGLUDxkLAYMZAPE1EKUP"]}, {"_id": 359, "question": "359 # A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices. The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests. Whatshould a solutions architect do to address this issue without impacting existing users?", "options": ["A. Add throttling on the API Gateway with server-side throttling limits.", "B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.", "C. Create a secondary index in DynamoDB for the table with the user requests.", "D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB. Correct Answer: D Question #:208"], "explain": "", "answers": [], "resources": []}, {"_id": 360, "question": "360 # A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?", "options": ["A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access.", "B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access.", "C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket\u2019s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access.", "D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket\u2019s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access. Correct Answer: A Question #:209"], "explain": "", "answers": [], "resources": []}, {"_id": 361, "question": "361 # A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed. Whatshould the solutions architect do to ensure that the architecture supports distributed session data management? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248567122175&usg=AOvVaw2PWM4RGfUl1WUaDNf3eqFe 171", "options": ["A. Use Amazon ElastiCache to manage and store session data.", "B. Use session affinity (sticky sessions) of the ALB to manage session data.", "C. Use Session Manager from AWS Systems Manager to manage the session.", "D. Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session. Correct Answer: A Question #:210"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248567122175&usg=AOvVaw2PWM4RGfUl1WUaDNf3eqFe"]}, {"_id": 362, "question": "362 # A company offers a food delivery service that is growing rapidly. Because of the growth, the company\u2019s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following: A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company\u2019s AWS resources. Which solution meets these requirements?", "options": ["A. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group\u2019s minimum capacity according to peak workload values.", "B. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.", "C. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.", "D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric. Correct Answer: D Question #:211"], "explain": "", "answers": [], "resources": []}, {"_id": 363, "question": "363 # A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of \u201capplication\u201d and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components. Which solution meets these requirements?", "options": ["A. Use AWS CloudTrail to generate a list of resources with the application tag.", "B. Use the AWS CLI to query each service across all Regions to report the tagged components.", "C. Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.", "D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag. Correct Answer: D Question #:212"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248567663792&usg=AOvVaw0L1kS_i0nk49Uu51bzvwYQ"]}, {"_id": 364, "question": "364 # A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time. Which S3 storage class should the company use to meet these requirements?", "options": ["A. S3 Intelligent-Tiering", "B. S3 Glacier Instant Retrieval", "C. S3 Standard", "D. S3 Standard-Infrequent Access (S3 Standard-IA) Correct Answer: A Question #:213"], "explain": "", "answers": [], "resources": []}]