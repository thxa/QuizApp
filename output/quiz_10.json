[{"_id": 558, "question": "558 # A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access. Which solution will meet these requirements?", "options": ["A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.", "B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.", "C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.", "D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports. Correct Answer: B"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248623063259&usg=AOvVaw1Nx5uXZTwaC65siDruJn56"]}, {"_id": 559, "question": "559 # A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket. Whatshould the solutions architect do to meet this requirement?", "options": ["A. Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.", "B. Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.", "C. Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.", "D. Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances. Correct Answer: A Question #:18"], "explain": "", "answers": [], "resources": []}, {"_id": 560, "question": "560 # An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket. A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically. Which combination of actions will meet these requirements? (Choose two.)", "options": ["A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.", "B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.", "C. Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.", "D. Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.", "E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing. Correct Answer: AB Question #:19"], "explain": "", "answers": [], "resources": []}, {"_id": 561, "question": "561 # A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets. A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.", "B. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248623876341&usg=AOvVaw3X7pF4DZp0R4W7SyBnpwWy 236", "C. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248623876341&usg=AOvVaw3X7pF4DZp0R4W7SyBnpwWy"]}, {"_id": 562, "question": "562 # A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance. A solutions architect needs to minimize the time that is required to clone the production data into the test environment. Which solution will meet these requirements?", "options": ["A. Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.", "B. Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.", "C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.", "D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment. Correct Answer: D Question #:21"], "explain": "", "answers": [], "resources": []}, {"_id": 563, "question": "563 # An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.", "B. Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.", "C. Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.", "D. Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB. Correct Answer: D Question #:22"], "explain": "", "answers": [], "resources": []}, {"_id": 564, "question": "564 # A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248624496376&usg=AOvVaw37Vg-hcV6ssf7-F5ZFG4ks 237 minimize the costs of storing and retrieving the media files. Which storage option meets these requirements?", "options": ["A. S3 Standard", "B. S3 Intelligent-Tiering", "C. S3 Standard-Infrequent Access (S3 Standard-IA)", "D. S3 One Zone-Infrequent Access (S3 One Zone-IA) Correct Answer: B Question #:23"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248624496376&usg=AOvVaw37Vg-hcV6ssf7-F5ZFG4ks"]}, {"_id": 565, "question": "565 # A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely. Which storage solution will meet these requirements MOST cost-effectively?", "options": ["A. Configure S3 Intelligent-Tiering to automatically migrate objects.", "B. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.", "C. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.", "D. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month. Correct Answer: B Question #:24"], "explain": "", "answers": [], "resources": []}, {"_id": 566, "question": "566 # A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling. How should the solutions architect generate the information with the LEAST operational overhead?", "options": ["A. Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.", "B. Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.", "C. Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.", "D. Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types. Correct Answer: B Question #:25"], "explain": "", "answers": [], "resources": []}, {"_id": 567, "question": "567 # A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database. During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort. Which solution will meet these requirements?", "options": ["A. Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.", "B. Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248625411452&usg=AOvVaw3GnNQ6stQM5M0AcjH-1aq1 238", "C. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS)."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248625411452&usg=AOvVaw3GnNQ6stQM5M0AcjH-1aq1"]}, {"_id": 568, "question": "568 # A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes. Whatshould a solutions architect do to accomplish this goal?", "options": ["A. Turn on AWS Config with the appropriate rules.", "B. Turn on AWS Trusted Advisor with the appropriate checks.", "C. Turn on Amazon Inspector with the appropriate assessment template.", "D. Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events). Correct Answer: A Question #:27"], "explain": "", "answers": [], "resources": []}, {"_id": 569, "question": "569 # A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege. Which solution will meet these requirements?", "options": ["A. Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.", "B. Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.", "C. Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.", "D. Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard. Correct Answer: A Question #:28"], "explain": "", "answers": [], "resources": []}, {"_id": 570, "question": "570 # A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory. Which solution will meet these requirements?", "options": ["A. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248626151110&usg=AOvVaw2CXINX9sQ1oBkQc6c5C7L1 239", "B. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.", "C. Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248626151110&usg=AOvVaw2CXINX9sQ1oBkQc6c5C7L1"]}, {"_id": 571, "question": "571 # A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions. The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions. Which solution will meet these requirements?", "options": ["A. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.", "B. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.", "C. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.", "D. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin. Correct Answer: A Question #:30"], "explain": "", "answers": [], "resources": []}, {"_id": 572, "question": "572 # A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance. Which solution meets these requirements MOST cost-effectively?", "options": ["A. Stop the DB instance when tests are completed. Restart the DB instance when required.", "B. Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.", "C. Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.", "D. Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required. Correct Answer: C Question #:31"], "explain": "", "answers": [], "resources": []}, {"_id": 573, "question": "573 # A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check. Whatshould a solutions architect do to accomplish this?", "options": ["A. Use AWS Config rules to define and detect resources that are not properly tagged.", "B. Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248626821321&usg=AOvVaw0NvxKNNEOHE1OACU7hXoD2 240", "C. Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248626821321&usg=AOvVaw0NvxKNNEOHE1OACU7hXoD2"]}, {"_id": 574, "question": "574 # A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost- effective for hosting the website?", "options": ["A. Containerize the website and host it in AWS Fargate.", "B. Create an Amazon S3 bucket and host the website there.", "C. Deploy a web server on an Amazon EC2 instance to host the website.", "D. Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework. Correct Answer: B Question #:33"], "explain": "", "answers": [], "resources": []}, {"_id": 575, "question": "575 # A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low- latency retrieval. Whatshould a solutions architect recommend to meet these requirements?", "options": ["A. Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.", "B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.", "C. Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.", "D. Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3. Correct Answer: C Question #:34"], "explain": "", "answers": [], "resources": []}, {"_id": 576, "question": "576 # A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.", "B. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.", "C. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.", "D. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls. Correct Answer: B Question #:35"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248627371517&usg=AOvVaw0OHswyPOk6ZLFjtzCjAZoD"]}, {"_id": 577, "question": "577 # A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?", "options": ["A. Enable Amazon GuardDuty on the account.", "B. Enable Amazon Inspector on the EC2 instances.", "C. Enable AWS Shield and assign Amazon Route 53 to it.", "D. Enable AWS Shield Advanced and assign the ELB to it. Correct Answer: D Question #:36"], "explain": "", "answers": [], "resources": []}, {"_id": 578, "question": "578 # A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.", "B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.", "C. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.", "D. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets. Correct Answer: B Question #:37"], "explain": "", "answers": [], "resources": []}, {"_id": 579, "question": "579 # A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Use the EC2 serial console to directly access the terminal interface of each instance for administration.", "B. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.", "C. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.", "D. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel. Correct Answer: B Question #:38"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248627917259&usg=AOvVaw0Fc0I4xmlQWXIzB6R0MWCH"]}, {"_id": 580, "question": "580 # A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website. Which solution meets these requirements MOST cost-effectively?", "options": ["A. Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.", "B. Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.", "C. Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.", "D. Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint. Correct Answer: C Question #:39"], "explain": "", "answers": [], "resources": []}, {"_id": 581, "question": "581 # A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website. The company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem. Which solution addresses this performance issue?", "options": ["A. Change the storage type to Provisioned IOPS SSD.", "B. Change the DB instance to a memory optimized instance class.", "C. Change the DB instance to a burstable performance instance class.", "D. Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication. Correct Answer: A Question #:40"], "explain": "", "answers": [], "resources": []}, {"_id": 582, "question": "582 # A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis. The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days. Whatis the MOST operationally efficient solution that meets these requirements?", "options": ["A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.", "B. Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.", "C. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.", "D. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue. Correct Answer: A"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248628510201&usg=AOvVaw1dZpTGx8MRkO-DFOhTcr86"]}, {"_id": 583, "question": "583 # A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.", "B. Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.", "C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.", "D. Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete. Correct Answer: B Question #:42"], "explain": "", "answers": [], "resources": []}, {"_id": 584, "question": "584 # A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges. Whatis the MOST cost-effective way for the company to avoid Regional data transfer charges?", "options": ["A. Launch the NAT gateway in each Availability Zone.", "B. Replace the NAT gateway with a NAT instance.", "C. Deploy a gateway VPC endpoint for Amazon S3.", "D. Provision an EC2 Dedicated Host to run the EC2 instances. Correct Answer: C Question #:43"], "explain": "", "answers": [], "resources": []}, {"_id": 585, "question": "585 # A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users. Which solution meets these requirements?", "options": ["A. Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.", "B. Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.", "C. Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248629304815&usg=AOvVaw0RAGMLiimA9bNrLyMSD2QR 244"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248629304815&usg=AOvVaw0RAGMLiimA9bNrLyMSD2QR"]}, {"_id": 586, "question": "586 # A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Enable versioning on the S3 bucket.", "B. Enable MFA Delete on the S3 bucket.", "C. Create a bucket policy on the S3 bucket.", "D. Enable default encryption on the S3 bucket.", "E. Create a lifecycle policy for the objects in the S3 bucket. Correct Answer: AB Question #:45"], "explain": "", "answers": [], "resources": []}, {"_id": 587, "question": "587 # A company has a data ingestion workflow that consists of the following: An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries An AWS Lambda function to process the data and record metadata The company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job. Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)", "options": ["A. Deploy the Lambda function in multiple Availability Zones.", "B. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.", "C. Increase the CPU and memory that are allocated to the Lambda function.", "D. Increase provisioned throughput for the Lambda function.", "E. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue. Correct Answer: BE Question #:46"], "explain": "", "answers": [], "resources": []}, {"_id": 588, "question": "588 # A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size. Recently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included. The company wants administrators to be alerted if PII is shared again. The company also wants to automate remediation. Whatshould a solutions architect do to meet these requirements with the LEAST development effort?", "options": ["A. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.", "B. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.", "C. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248629923036&usg=AOvVaw0z_UBNHPhD_FYfwZhAnRLf 245"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248629923036&usg=AOvVaw0z_UBNHPhD_FYfwZhAnRLf"]}, {"_id": 589, "question": "589 # A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week. Whatshould the company do to guarantee the EC2 capacity?", "options": ["A. Purchase Reserved Instances that specify the Region needed.", "B. Create an On-Demand Capacity Reservation that specifies the Region needed.", "C. Purchase Reserved Instances that specify the Region and three Availability Zones needed.", "D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed. Correct Answer: D Question #:48"], "explain": "", "answers": [], "resources": []}, {"_id": 590, "question": "590 # A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Move the catalog to Amazon ElastiCache for Redis.", "B. Deploy a larger EC2 instance with a larger instance store.", "C. Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.", "D. Move the catalog to an Amazon Elastic File System (Amazon EFS) file system. Correct Answer: D Question #:49"], "explain": "", "answers": [], "resources": []}, {"_id": 591, "question": "591 # A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.", "B. Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.", "C. Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.", "D. Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive. Correct Answer: B Question #:50"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248630493956&usg=AOvVaw2WTW8EcYsADorj_Wqqx60x"]}, {"_id": 592, "question": "592 # A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability. Whatshould a solutions architect do to meet these requirements?", "options": ["A. Create an AWS Lambda function to apply the patch to all EC2 instances.", "B. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.", "C. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.", "D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances. Correct Answer: D Question #: 584"], "explain": "", "answers": [], "resources": []}, {"_id": 600, "question": "600# A company is deploying an application that processes large quantities of data in parallel. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of nodes from sharing the same underlying hardware. Which networking solution meets these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 600, "question": "600# A company is deploying an application that processes large quantities of data in parallel. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of nodes from sharing the same underlying hardware. Which networking solution meets these requirements? A. Run the EC2 instances in a spread placement group. B. Group the EC2 instances in separate accounts. C. Configure the EC2 instances with dedicated tenancy. D. Configure the EC2 instances with shared tenancy. Selected Answer: C", "options": ["A. Run the EC2 instances in a spread placement group.", "B. Group the EC2 instances in separate accounts.", "C. Configure the EC2 instances with dedicated tenancy.", "A. Spread placement groups are designed to distribute instances across underlying hardware to reduce the risk of simultaneous failures, but they do not guarantee isolation from other customers.", "B. While using separate AWS accounts can provide isolation, it does not guarantee that the underlying hardware is dedicated to a single customer, which is the specific requirement mentioned.", "D. Configure the EC2 instances with shared tenancy: Shared tenancy means that instances can share the same hardware. This option goes against the requirement of preventing groups of nodes from sharing the same underlying hardware. Dedicated tenancy ensures that the EC2 instances run on hardware that is dedicated to a single customer, providing isolation from other customers. This is especially important for workloads that require high levels of security or compliance, or in situations where it's critical to avoid the possibility of co-location with other customers on the same physical hardware."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248631087039&usg=AOvVaw0jOM-duv-_uQ8NON6JCbt5"]}, {"_id": 601, "question": "601# A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover AWS Region. Business requirements state that the DR strategy must meet capacity in the failover Region. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 601, "question": "601# A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover AWS Region. Business requirements state that the DR strategy must meet capacity in the failover Region. Which solution will meet these requirements? A. Purchase On-Demand Instances in the failover Region. B. Purchase an EC2 Savings Plan in the failover Region. C. Purchase regional Reserved Instances in the failover Region. D. Purchase a Capacity Reservation in the failover Region. Selected Answer: D A Capacity Reservation allows you to reserve capacity in the specified Availability Zone in the failover Region, ensuring that you have the reserved capacity available when needed. This is suitable for scenarios where you want to guarantee capacity for specific instances in the failover Region during a disaster recovery event. The other options are less appropriate for the specific requirement:", "options": ["A. Purchase On-Demand Instances in the failover Region.", "B. Purchase an EC2 Savings Plan in the failover Region.", "C. Purchase regional Reserved Instances in the failover Region.", "A. Purchase On-Demand Instances in the failover Region: On-Demand Instances are not reserved in advance and may not be readily available in the event of a disaster recovery scenario.", "B. Purchase an EC2 Savings Plan in the failover Region: Savings Plans provide cost savings but do not reserve capacity in advance, and they may not meet the immediate capacity requirement during a disaster recovery event.", "C. Purchase regional Reserved Instances in the failover Region: While Reserved Instances offer cost savings, they are specific to a Region and do not necessarily guarantee immediate capacity in the failover Region during a disaster recovery scenario. In summary, option D (Purchase a Capacity Reservation in the failover Region) is the most suitable solution to meet the business requirement of ensuring EC2 capacity in the failover AWS Region for disaster recovery."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248631639788&usg=AOvVaw04OD1I6fTmxmyadFDng-Yy"]}, {"_id": 602, "question": "602# A company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates to the five businesses that the company owns. The company's research and development (R&D) business is separating from the company and will need its own organization. A solutions architect creates a separate new management account for this purpose. What should the solutions architect do next in the new management account?", "options": ["A. Have the R&D AWS account be part of both organizations during the transition.", "B. Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization.", "C. Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D AWS account to the new R&D AWS account.", "D. Have the R&D AWS account join the new organization. Make the new management account a member of the prior organization. Selected Answer: B"], "explain": "", "answers": [], "resources": ["https://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-"]}, {"_id": 603, "question": "603# A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 603, "question": "603# A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes. Which solution will meet these requirements? A. Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB. B. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632178441&usg=AOvVaw2jIuAYcFTXvJ5zumf8rkFM https://www.google.com/url?q=https://repost.aws/knowledge-center/organizations-move-accounts%23:~:text%3DIn%2520either%2520case%252C-,perform%2520these%2520actions,-for%2520each%2520member&sa=D&source=apps-viewer-frontend&ust=1720248632178494&usg=AOvVaw0xEBV6XjBPtDa-VHP5GAO2 https://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-,perform%20these%20actions,-for%20each%20member 249 C. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization. D. Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization. Selected Answer: C Amazon API Gateway: Acts as a managed service for creating, publishing, and securing APIs at scale. It allows for the creation of API endpoints that can be integrated with other web applications. Amazon Kinesis Data Firehose: Is used to capture and load streaming data into other AWS services. In this case, it can store the information in an Amazon S3 bucket. API Gateway Lambda Authorizer: Provides a way to control access to your APIs using Lambda functions. It allows for implementing custom authorization logic. This solution offers scalability, the ability to handle unpredictable increases in activity, and integration capabilities. The use of an API Gateway Lambda authorizer ensures that the authorization step is performed securely. The other options have some limitations or are less aligned with the specified requirements:", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 603, "question": "603# A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes. Which solution will meet these requirements? A. Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB. B. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632178441&usg=AOvVaw2jIuAYcFTXvJ5zumf8rkFM https://www.google.com/url?q=https://repost.aws/knowledge-center/organizations-move-accounts%23:~:text%3DIn%2520either%2520case%252C-,perform%2520these%2520actions,-for%2520each%2520member&sa=D&source=apps-viewer-frontend&ust=1720248632178494&usg=AOvVaw0xEBV6XjBPtDa-VHP5GAO2 https://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-,perform%20these%20actions,-for%20each%20member 249 C. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization. D. Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization. Selected Answer: C Amazon API Gateway: Acts as a managed service for creating, publishing, and securing APIs at scale. It allows for the creation of API endpoints that can be integrated with other web applications. Amazon Kinesis Data Firehose: Is used to capture and load streaming data into other AWS services. In this case, it can store the information in an Amazon S3 bucket. API Gateway Lambda Authorizer: Provides a way to control access to your APIs using Lambda functions. It allows for implementing custom authorization logic. This solution offers scalability, the ability to handle unpredictable increases in activity, and integration capabilities. The use of an API Gateway Lambda authorizer ensures that the authorization step is performed securely. The other options have some limitations or are less aligned with the specified requirements: A. GWLB in front of Amazon ECS: This option involves a load balancer in front of ECS, which might be more suitable for scenarios requiring container orchestration but might introduce unnecessary complexity for the given requirements. B. Amazon API Gateway in front of Amazon Kinesis data stream: This option lacks the Lambda authorizer for resolving authorization and might not be as straightforward for integrating with other web applications. D. GWLB in front of Amazon ECS with Lambda function for authorization: Similar to option A, this introduces a load balancer and ECS, which might be more complex than necessary for the given requirements. In summary, option C offers a streamlined solution with the necessary scalability, integration capabilities, and authorization control. Question #: 588 https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632832024&usg=AOvVaw3mQyH7IKAHP8Ghp0EED645 250 604# An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24 hours. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB.", "B. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632178441&usg=AOvVaw2jIuAYcFTXvJ5zumf8rkFM https://www.google.com/url?q=https://repost.aws/knowledge-center/organizations-move-accounts%23:~:text%3DIn%2520either%2520case%252C-,perform%2520these%2520actions,-for%2520each%2520member&sa=D&source=apps-viewer-frontend&ust=1720248632178494&usg=AOvVaw0xEBV6XjBPtDa-VHP5GAO2 https://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-,perform%20these%20actions,-for%20each%20member 249", "C. Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.", "A. GWLB in front of Amazon ECS: This option involves a load balancer in front of ECS, which might be more suitable for scenarios requiring container orchestration but might introduce unnecessary complexity for the given requirements.", "B. Amazon API Gateway in front of Amazon Kinesis data stream: This option lacks the Lambda authorizer for resolving authorization and might not be as straightforward for integrating with other web applications.", "D. GWLB in front of Amazon ECS with Lambda function for authorization: Similar to option A, this introduces a load balancer and ECS, which might be more complex than necessary for the given requirements. In summary, option C offers a streamlined solution with the necessary scalability, integration capabilities, and authorization control. Question #: 588", "A. Create a cross-Region read replica and promote the read replica to the primary instance.", "B. Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.", "C. Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.", "D. Copy automatic snapshots to another Region every 24 hours. Selected Answer: D Amazon RDS creates and saves automated backups of your DB instance or Multi-AZ DB cluster during the backup window of your DB instance. RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases. RDS saves the automated backups of your DB instance according to the backup retention period that you specify. If necessary, you can recover your DB instance to any point in time during the backup retention period. Question #: 589"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632178441&usg=AOvVaw2jIuAYcFTXvJ5zumf8rkFM", "https://www.google.com/url?q=https://repost.aws/knowledge-center/organizations-move-accounts%23:~:text%3DIn%2520either%2520case%252C-,perform%2520these%2520actions,-for%2520each%2520member&sa=D&source=apps-viewer-frontend&ust=1720248632178494&usg=AOvVaw0xEBV6XjBPtDa-VHP5GAO2", "https://repost.aws/knowledge-center/organizations-move-accounts#:~:text=In%20either%20case%2C-,perform%20these%20actions,-for%20each%20member", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248632832024&usg=AOvVaw3mQyH7IKAHP8Ghp0EED645"]}, {"_id": 605, "question": "605# A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer that has sticky sessions enabled. The web server currently hosts the user session state. The company wants to ensure high availability and avoid user session state loss in the event of a web server outage. Which solution will meet these requirements?", "options": ["A. Use an Amazon ElastiCache for Memcached instance to store the session data. Update the application to use ElastiCache for Memcached to store the session state.", "B. Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.", "C. Use an AWS Storage Gateway cached volume to store session data. Update the application to use AWS Storage Gateway cached volume to store the session state.", "D. Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store the session state. Selected Answer: B"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248633328533&usg=AOvVaw2eBEjHYu7f9bhvD22ppjSu"]}, {"_id": 606, "question": "606# A company migrated a MySQL database from the company's on-premises data center to an Amazon RDS for MySQL DB instance. The company sized the RDS DB instance to meet the company's average daily workload. Once a month, the database performs slowly when the company runs queries for a report. The company wants to have the ability to run reports and maintain the performance of the daily workloads. Which solution will meet these requirements?", "options": ["A. Create a read replica of the database. Direct the queries to the read replica.", "B. Create a backup of the database. Restore the backup to another DB instance. Direct the queries to the new database.", "C. Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.", "D. Resize the DB instance to accommodate the additional workload."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248634320078&usg=AOvVaw014gTyWijr47wHiw9pFtDi"]}, {"_id": 607, "question": "607# A company runs a container application by using Amazon Elastic Kubernetes Service (Amazon EKS). The application includes microservices that manage customers and place orders. The company needs to route incoming requests to the appropriate microservices. Which solution will meet this requirement MOST cost-effectively?", "options": ["A. Use the AWS Load Balancer Controller to provision a Network Load Balancer.", "B. Use the AWS Load Balancer Controller to provision an Application Load Balancer.", "C. Use an AWS Lambda function to connect the requests to Amazon EKS.", "D. Use Amazon API Gateway to connect the requests to Amazon EKS. Selected Answer: D Both ALB and API gateway can be used to route traffic to the microservices, but the question seeks the most 'cost effective' option. You are charged for each hour or partial hour that an Application Load Balancer is running, and the number of Load Balancer Capacity Units (LCU) used per hour. With Amazon API Gateway, you only pay when your APIs are in use."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/"]}, {"_id": 608, "question": "608# A company uses AWS and sells access to copyrighted images. The company\u2019s global customer base needs to be able to access these images quickly. The company must deny access to users from specific countries. The company wants to minimize costs as much as possible. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248635024687&usg=AOvVaw1IjKVKM3lEt4WBF5e8FtB8 https://www.google.com/url?q=https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/&sa=D&source=apps-viewer-frontend&ust=1720248635024730&usg=AOvVaw0BZB9eGrTipVjF1l7cviKf https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/ 253 Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 608, "question": "608# A company uses AWS and sells access to copyrighted images. The company\u2019s global customer base needs to be able to access these images quickly. The company must deny access to users from specific countries. The company wants to minimize costs as much as possible. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248635024687&usg=AOvVaw1IjKVKM3lEt4WBF5e8FtB8 https://www.google.com/url?q=https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/&sa=D&source=apps-viewer-frontend&ust=1720248635024730&usg=AOvVaw0BZB9eGrTipVjF1l7cviKf https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/ 253 Which solution will meet these requirements? A. Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket access. Provide customers with a link to the S3 bucket. B. Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a group that has permission to access the S3 bucket. C. Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the images. Deploy the instances only in the countries the company services. Provide customers with links to the ALBs for their specific country's instances. D. Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions. Provide a signed URL for each customer to access the data in CloudFront. Selected Answer: D Amazon S3 for Storage: Amazon S3 is used for storing the images. It provides scalable, durable, and low- latency storage for the images. Amazon CloudFront for Content Delivery: CloudFront is used as a content delivery network (CDN) to distribute the images globally. This reduces latency and ensures quick access for customers worldwide. Geographic Restrictions in CloudFront: CloudFront supports geographic restrictions, allowing the company to deny access to users from specific countries. This satisfies the requirement to control access based on the user's location. Signed URLs for Secure Access: Signed URLs are provided to customers for secure access to the images. This ensures that only authorized customers can access the content. Minimizing Costs: CloudFront is a cost-effective solution for content delivery, and it can significantly reduce data transfer costs by serving content from edge locations close to end-users. The other options have drawbacks: https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248635589891&usg=AOvVaw3jKGBnOqzhkvplAhfnCCIp 254", "options": ["A. Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket access. Provide customers with a link to the S3 bucket.", "B. Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a group that has permission to access the S3 bucket.", "C. Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the images. Deploy the instances only in the countries the company services. Provide customers with links to the ALBs for their specific country's instances.", "D. Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions. Provide a signed URL for each customer to access the data in CloudFront. Selected Answer: D Amazon S3 for Storage: Amazon S3 is used for storing the images. It provides scalable, durable, and low- latency storage for the images. Amazon CloudFront for Content Delivery: CloudFront is used as a content delivery network (CDN) to distribute the images globally. This reduces latency and ensures quick access for customers worldwide. Geographic Restrictions in CloudFront: CloudFront supports geographic restrictions, allowing the company to deny access to users from specific countries. This satisfies the requirement to control access based on the user's location. Signed URLs for Secure Access: Signed URLs are provided to customers for secure access to the images. This ensures that only authorized customers can access the content. Minimizing Costs: CloudFront is a cost-effective solution for content delivery, and it can significantly reduce data transfer costs by serving content from edge locations close to end-users. The other options have drawbacks:", "A. Public Bucket Access with MFA (Option A): Making the S3 bucket publicly accessible poses security risks, and the use of multi-factor authentication (MFA) in this context is not a suitable solution for providing access to global customers.", "B. IAM Users for Each Customer (Option B): Creating IAM users for each customer is not scalable and may result in increased administrative overhead. IAM users are typically used for access control within an AWS account.", "C. EC2 Instances behind ALBs (Option C): Deploying EC2 instances in specific countries is less scalable, adds complexity, and may not provide the global performance benefits that a CDN like CloudFront offers. In summary, option D (Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions. Provide a signed URL for each customer to access the data in CloudFront) is the most suitable solution for meeting the specified requirements. Question #: 593"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248635024687&usg=AOvVaw1IjKVKM3lEt4WBF5e8FtB8", "https://www.google.com/url?q=https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/&sa=D&source=apps-viewer-frontend&ust=1720248635024730&usg=AOvVaw0BZB9eGrTipVjF1l7cviKf", "https://aws.amazon.com/blogs/containers/integrate-amazon-api-gateway-with-amazon-eks/", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248635589891&usg=AOvVaw3jKGBnOqzhkvplAhfnCCIp"]}, {"_id": 609, "question": "609# A solutions architect is designing a highly available Amazon ElastiCache for Redis based solution. The solutions architect needs to ensure that failures do not result in performance degradation or loss of data locally and within an AWS Region. The solution needs to provide high availability at the node level and at the Region level. Which solution will meet these requirements?", "options": ["A. Use Multi-AZ Redis replication groups with shards that contain multiple nodes.", "B. Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.", "C. Use a Multi-AZ Redis cluster with more than one read replica in the replication group.", "D. Use Redis shards that contain multiple nodes with Auto Scaling turned on. Selected Answer: A Multi-AZ Redis Replication Groups: Amazon ElastiCache provides Multi-AZ support for Redis, allowing the creation of replication groups that span multiple Availability Zones (AZs) within a Region. This ensures high availability at the Region level."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248636228329&usg=AOvVaw2fzqAJLkc0scMgNvWEAl4D"]}, {"_id": 610, "question": "610# A company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its application. During the migration testing phase, a technical team observes that the application takes a long time to launch and load memory to become fully productive. Which solution will reduce the launch time of the application during the next testing phase?", "options": ["A. Launch two or more EC2 On-Demand Instances. Turn on auto scaling features and make the EC2 On- Demand Instances available during the next testing phase.", "B. Launch EC2 Spot Instances to support the application and to scale the application so it is available during the next testing phase.", "C. Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling warm pools during the next testing phase.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248636852066&usg=AOvVaw3ZIGy_CRgghqC1qIj1yQrg 256"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248636852066&usg=AOvVaw3ZIGy_CRgghqC1qIj1yQrg"]}, {"_id": 611, "question": "611# A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company notices that its applications experience sudden traffic increases on random days of the week. The company wants to maintain application performance during sudden traffic increases. Which solution will meet these requirements MOST cost-effectively?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 611, "question": "611# A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company notices that its applications experience sudden traffic increases on random days of the week. The company wants to maintain application performance during sudden traffic increases. Which solution will meet these requirements MOST cost-effectively? A. Use manual scaling to change the size of the Auto Scaling group. B. Use predictive scaling to change the size of the Auto Scaling group. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248637589966&usg=AOvVaw3L1w1xiCluJMJ1m-CwFlyh 257 C. Use dynamic scaling to change the size of the Auto Scaling group. D. Use schedule scaling to change the size of the Auto Scaling group. Selected Answer: C Dynamic Scaling: Dynamic scaling adjusts the size of the Auto Scaling group in response to changing demand. It allows the Auto Scaling group to automatically increase or decrease the number of instances based on defined policies. This is well-suited for handling sudden traffic increases as the group scales in or out as needed. Predictive Scaling (Option B): While predictive scaling uses machine learning algorithms to predict future demand and adjusts the group size accordingly, it might be an overkill for sudden and unpredictable traffic increases. Dynamic scaling can respond quickly without the need for extensive predictive analysis. Manual Scaling (Option A): Manual scaling requires human intervention and may not be timely enough to address sudden traffic increases effectively. Schedule Scaling (Option D): Schedule scaling is based on predefined schedules and may not be suitable for handling random and unpredictable traffic increases. In summary, option C (Use dynamic scaling to change the size of the Auto Scaling group) is the most suitable and cost-effective solution for addressing sudden traffic increases by allowing the Auto Scaling group to automatically adjust its size based on demand. Question #: 596 612# An ecommerce application uses a PostgreSQL database that runs on an Amazon EC2 instance. During a monthly sales event, database usage increases and causes database connection issues for the application. The traffic is unpredictable for subsequent monthly sales events, which impacts the sales forecast. The company needs to maintain performance when there is an unpredictable increase in traffic. Which solution resolves this issue in the MOST cost-effective way?", "options": ["A. Use manual scaling to change the size of the Auto Scaling group.", "B. Use predictive scaling to change the size of the Auto Scaling group.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248637589966&usg=AOvVaw3L1w1xiCluJMJ1m-CwFlyh 257", "C. Use dynamic scaling to change the size of the Auto Scaling group.", "A. Migrate the PostgreSQL database to Amazon Aurora Serverless v2.", "B. Enable auto scaling for the PostgreSQL database on the EC2 instance to accommodate increased usage.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248638202548&usg=AOvVaw1eca3w_6M5r1_NwE331WZf 258", "C. Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a larger instance type."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248637589966&usg=AOvVaw3L1w1xiCluJMJ1m-CwFlyh", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248638202548&usg=AOvVaw1eca3w_6M5r1_NwE331WZf"]}, {"_id": 613, "question": "613# A company hosts an internal serverless application on AWS by using Amazon API Gateway and AWS Lambda. The company\u2019s employees report issues with high latency when they begin using the application each day. The company wants to reduce latency. Which solution will meet these requirements?", "options": ["A. Increase the API Gateway throttling limit.", "B. Set up a scheduled scaling to increase Lambda provisioned concurrency before employees begin to use the application each day.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248638765683&usg=AOvVaw0RSYOh7to3N0j37ZZtENyb 259", "C. Create an Amazon CloudWatch alarm to initiate a Lambda function as a target for the alarm at the beginning of each day."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248638765683&usg=AOvVaw0RSYOh7to3N0j37ZZtENyb"]}, {"_id": 614, "question": "614# A research company uses on-premises devices to generate data for analysis. The company wants to use the AWS Cloud to analyze the data. The devices generate .csv files and support writing the data to an SMB file share. Company analysts must be able to use SQL commands to query the data. The analysts will run queries periodically throughout the day. Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)", "options": ["A. Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248639281869&usg=AOvVaw1mUgWK5EOaiNhl_eYG3hIA 260", "B. Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway made.", "C. Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3.", "D. Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in Amazon S3. Provide access to analysts.", "E. Set up an Amazon Redshift cluster to query the data that is in Amazon S3. Provide access to analysts."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248639281869&usg=AOvVaw1mUgWK5EOaiNhl_eYG3hIA"]}, {"_id": 615, "question": "615# A company wants to use Amazon Elastic Container Service (Amazon ECS) clusters and Amazon RDS DB instances to build and run a payment processing application. The company will run the application in its on-premises data center for compliance purposes. A solutions architect wants to use AWS Outposts as part of the solution. The solutions architect is working with the company's operational team to build the application. Which activities are the responsibility of the company's operational team? (Choose three.) https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248639812567&usg=AOvVaw1wZTOtUjR9HOBU3Jp7tdmV 261", "options": ["A. Providing resilient power and network connectivity to the Outposts racks", "B. Managing the virtualization hypervisor, storage systems, and the AWS services that run on Outposts", "C. Physical security and access controls of the data center environment", "D. Availability of the Outposts infrastructure including the power supplies, servers, and networking equipment within the Outposts racks", "E. Physical maintenance of Outposts components", "F. Providing extra capacity for Amazon ECS clusters to mitigate server failures and maintenance events Suggested Answer: ACF Providing resilient power and network connectivity to the Outposts racks (Option A): This includes ensuring that there is reliable power and network connectivity to the Outposts infrastructure to maintain the availability of the application. Physical security and access controls of the data center environment (Option C): The operational team is responsible for securing the physical data center environment to protect the Outposts infrastructure and the sensitive payment processing application. Physical maintenance of Outposts components (Option E): This involves handling physical maintenance tasks such as replacing faulty hardware components, ensuring proper cooling, and addressing any issues with the Outposts racks. Options B, D, and F are typically managed by AWS as part of the AWS Outposts service: Managing the virtualization hypervisor, storage systems, and AWS services (Option B): AWS manages the underlying infrastructure, including the virtualization hypervisor, storage systems, and AWS services running on Outposts. This is part of the managed service provided by AWS. Availability of the Outposts infrastructure (Option D): AWS is responsible for ensuring the availability of the Outposts infrastructure, including power supplies, servers, and networking equipment within the Outposts racks."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248639812567&usg=AOvVaw1wZTOtUjR9HOBU3Jp7tdmV", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248640380049&usg=AOvVaw2kIxzEbcub7xE1RsSIYbbX"]}, {"_id": 616, "question": "616# A company is planning to migrate a TCP-based application into the company's VPC. The application is publicly accessible on a nonstandard TCP port through a hardware appliance in the company's data center. This public endpoint can process up to 3 million requests per second with low latency. The company requires the same level of performance for the new public endpoint in AWS. What should a solutions architect recommend to meet this requirement?", "options": ["A. Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the TCP port that the application requires.", "B. Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the TCP port that the application requires.", "C. Deploy an Amazon CloudFront distribution that listens on the TCP port that the application requires. Use an Application Load Balancer as the origin.", "D. Deploy an Amazon API Gateway API that is configured with the TCP port that the application requires. Configure AWS Lambda functions with provisioned concurrency to process the requests. Selected Answer: A Network Load Balancer (NLB): NLB is designed to handle TCP traffic with extremely low latency. It is a layer 4 (TCP/UDP) load balancer that provides high performance and scales horizontally. NLB is suitable for scenarios where low latency and high throughput are critical, making it a good fit for TCP-based applications with stringent performance requirements. Publicly Accessible: NLB can be configured to be publicly accessible, allowing it to accept incoming requests from the internet."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248640895844&usg=AOvVaw0Fwvx0cZNwHk5kUOOXWf_r"]}, {"_id": 617, "question": "617# A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora PostgreSQL DB cluster.", "B. Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248641410803&usg=AOvVaw2-TWAWNKq07Vqj9Z-5sNyk 264", "C. Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248641410803&usg=AOvVaw2-TWAWNKq07Vqj9Z-5sNyk", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248641984664&usg=AOvVaw2j7ahKaRqsVRUeB3_RiL0Y"]}, {"_id": 618, "question": "618# A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster. What should the solutions architect do to meet this requirement with the LEAST amount of effort?", "options": ["A. Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS CloudFormation template to launch new EC2 instances from the EBS storage.", "B. Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.", "C. Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.", "D. Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage. Suggested Answer: C AWS Backup: AWS Backup is a fully managed backup service that centralizes and automates the backup of data across AWS services. It supports backing up Amazon EBS volumes and allows for efficient management of backups. Backup Plan: Create a backup plan in AWS Backup that includes the entire group of EC2 instances. This ensures a centralized and consistent backup strategy for all instances. API or CLI: AWS Backup provides an API and CLI that can be used to automate and speed up the restore process for multiple EC2 instances. This allows for a streamlined recovery process in case of a disaster. Advantages of Option C:"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248642680743&usg=AOvVaw2i20HHG_7MQx2R9sX8cErh"]}, {"_id": 619, "question": "619# A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the dataset in parallel. Which solution will meet these requirements with the MOST operational efficiency?", "options": ["A. Use the AWS Step Functions Map state in Inline mode to process the data in parallel.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248643398156&usg=AOvVaw00OmHT1nS9OixmNWx70w9Q 267", "B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.", "C. Use AWS Glue to process the data in parallel."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248643398156&usg=AOvVaw00OmHT1nS9OixmNWx70w9Q", "https://docs.aws.amazon.com/step-functions/latest/dg/use-dist-map-orchestrate-large-scale-parallel-"]}, {"_id": 620, "question": "620# A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task. Which solution will meet these requirements?", "options": ["A. Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.", "B. Use rsync to transfer the data directly to Amazon S3.", "C. Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.", "D. Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3. Selected Answer: D 1Gbps will roughly do 7 TB in 24 hours. This means 400Mbps will only do 3x42TB. AWS Snowball: AWS Snowball is a physical data transport solution that helps transfer large amounts of data into and out of AWS. It comes in a ruggedized device, and multiple devices can be used in parallel."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248643934920&usg=AOvVaw3oGYGCX_R30D6h6KlXuLcA", "https://www.google.com/url?q=https://docs.aws.amazon.com/step-functions/latest/dg/use-dist-map-orchestrate-large-scale-parallel-workloads.html&sa=D&source=apps-viewer-frontend&ust=1720248643934961&usg=AOvVaw3fUfKbyERq3Odm7NiMURXG", "https://docs.aws.amazon.com/step-functions/latest/dg/use-dist-map-orchestrate-large-scale-parallel-workloads.html", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248644497915&usg=AOvVaw3qKp-9XL0raYAgYa7yiuCs"]}, {"_id": 621, "question": "621# A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network storage servers. The company wants to reduce the number of these servers by moving to the AWS Cloud. A solutions architect must provide low-latency access to frequently used data and reduce the dependency on on-premises servers with a minimal number of infrastructure changes. Which solution will meet these requirements?", "options": ["A. Deploy an Amazon S3 File Gateway.", "B. Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.", "C. Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.", "D. Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes. Selected Answer: D AWS Storage Gateway: AWS Storage Gateway is a hybrid cloud storage service that provides seamless and secure integration between on-premises IT environments and AWS storage services. It supports different gateway configurations, including volume gateways. Volume Gateway Types: Stored Volumes: Entire datasets are stored on-premises, and the entire dataset is backed up to Amazon S3. Cached Volumes: Only frequently accessed data is stored on-premises, while the entire dataset is backed up to Amazon S3. Low-Latency Access with Cached Volumes:"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248645325706&usg=AOvVaw3p5MIdO3ig-eJEbvb0Nme6"]}, {"_id": 622, "question": "622# A solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The solution needs to maximize object durability. Objects also must be readily available at any time and for any length of time. Users will access objects frequently within the first 30 days after the objects are uploaded, but users are much less likely to access objects that are older than 30 days. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248646066613&usg=AOvVaw1S0jZxzQLsWaVuZ89gmyA0 271 Which solution meets these requirements MOST cost-effectively?", "options": ["A. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Glacier after 30 days.", "B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard- Infrequent Access (S3 Standard-IA) after 30 days.", "C. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One Zone- Infrequent Access (S3 One Zone-IA) after 30 days.", "D. Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. Selected Answer: B Option A - This option transitions objects to S3 Glacier, which is a suitable archival storage class. However, retrieval times from Glacier are longer, making it less suitable for frequent access. Option C - S3 One Zone-IA is designed for infrequently accessed, non-critical data that can be recreated. However, it lacks the durability of S3 Standard and S3 Intelligent-Tiering. Option D - S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns and in the question they didnt mention any of these conditions S3 Standard: This storage class is designed for frequently accessed data. It ensures low-latency and high- throughput performance, making it suitable for the initial 30 days when users frequently access the objects. Transition to S3 Standard-IA after 30 Days: After 30 days, an S3 Lifecycle rule is configured to transition the objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. S3 Standard-IA is designed for infrequently accessed data and is more cost-effective for objects that are accessed less frequently. Object Durability: Both S3 Standard and S3 Standard-IA provide a high level of durability for stored objects."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248646066613&usg=AOvVaw1S0jZxzQLsWaVuZ89gmyA0", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248646934900&usg=AOvVaw399sYG_vSL5yBQxGaTQGBw"]}, {"_id": 623, "question": "623# A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to process and store documents in the database as binary large objects (blobs) with an average document size of 6 MB. The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient. Which solution will meet these requirements MOST cost-effectively?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 623, "question": "623# A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to process and store documents in the database as binary large objects (blobs) with an average document size of 6 MB. The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient. Which solution will meet these requirements MOST cost-effectively? A. Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic. B. Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage type to Provisioned IOPS. C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database. D. Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS Database Migration Service (AWS DMS) to migrate data from the Oracle database to DynamoDB. Selected Answer: C Storing the blobs in the db is more expensive than s3 with references in the db. DynamoDB's limit on the size of each record is 400KB, so D is wrong.", "options": ["A. Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic.", "B. Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage type to Provisioned IOPS.", "C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.", "A. Concerns: Reducing the RDS DB instance size may impact performance negatively. Changing the storage type to Magnetic can significantly reduce I/O performance, which is typically not suitable for a production workload. Conclusion: This option is not recommended for maintaining or improving performance.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248647823291&usg=AOvVaw3U9KuqKSN4LxxGxiOycx-- 273", "B. Concerns: Increasing the instance size and storage capacity can result in higher costs. While Provisioned IOPS can improve performance, it may contribute to increased costs. Conclusion: This option might provide improved performance but comes with potential cost implications. It should be considered cautiously based on cost and performance considerations.", "C. Considerations: Storing large objects (blobs) in Amazon S3 is a scalable and cost-effective solution. Storing metadata in the existing database allows maintaining necessary information for each document. Reduced load on the RDS instance as large objects are stored in S3. Conclusion: This option is recommended as it leverages the strengths of both Amazon S3 and RDS, providing scalability, cost-effectiveness, and maintaining metadata."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248647823291&usg=AOvVaw3U9KuqKSN4LxxGxiOycx--", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248648437840&usg=AOvVaw1geP9-e882_zfBkBJi24Tw"]}, {"_id": 624, "question": "624# A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP. The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations. What should a solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}]