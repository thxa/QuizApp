[{"_id": 112, "question": "112 # A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible. Which solution will meet these requirements?", "options": ["A. Enable S3 Intelligent-Tiering for the S3 bucket", "B. Enable S3 Transfer Acceleration for the S3 bucket", "C. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC", "D. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC Selected Answer: C Gateway VPC Endpoint: A gateway VPC endpoint enables private connectivity between a VPC and Amazon S3. It allows direct access to Amazon S3 without the need for internet gateways, NAT devices, VPN connections, or AWS Direct Connect. Minimize Internet Traffic: By creating a gateway VPC endpoint for Amazon S3 and associating it with all route tables in the VPC, the traffic between the VPC and Amazon S3 will be kept within the AWS network. This helps in minimizing data transfer costs and prevents the need for traffic to traverse the internet. Cost-Effective: With a gateway VPC endpoint, the data transfer between the application running in the VPC and the S3 bucket stays within the AWS network, reducing the need for data transfer across the internet. This can result in cost savings, especially when dealing with large amounts of data."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248490809409&usg=AOvVaw1xR9b1ekRxobWQkl7LHuDC", "https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/109334-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248490809470&usg=AOvVaw0LHpsI0uwVNlOFaGFPwNCK", "https://www.examtopics.com/discussions/amazon/view/109334-exam-aws-certified-solutions-architect-associate-saa-c03/"]}, {"_id": 111, "question": "111 # A company has a mobile chat application with a data store based in Amazon DynamoDB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes. Which method should the solutions architect select?", "options": ["A. Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.", "B. Add DynamoDB read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas.", "C. Double the number of read capacity units for the new messages table in DynamoDB. Continue to use the existing DynamoDB endpoint.", "D. Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to point to the Redis cache endpoint instead of DynamoDB. Selected Answer: A Amazon DynamoDB Accelerator (DAX): DAX is an in-memory cache for DynamoDB that provides low- latency access to frequently accessed data. By configuring DAX for the new messages table, read requests for the table will be served from the DAX cache, significantly reducing the latency. Minimal Application Changes: With DAX, the application code can be updated to use the DAX endpoint instead of the standard DynamoDB endpoint. This change is relatively minimal and does not require extensive modifications to the application's data access logic. Low Latency: DAX caches frequently accessed data in memory, allowing subsequent read requests for the same data to be served with minimal latency. This ensures that new messages can be read by users with minimal delay. Option B (Add DynamoDB read replicas) involves creating read replicas to handle the increased read load, but it may not directly address the requirement of minimizing latency for new message reads. Question #: 401"], "explain": "", "answers": [], "resources": []}, {"_id": 126, "question": "126 # A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 126, "question": "126 # A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements? A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration. B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery. C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails. D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248491400062&usg=AOvVaw211dVRuQ39mdas9xUtW1yr 61 multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances. Selected Answer: A The correct answer is", "options": ["A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.", "B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.", "C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.", "D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across", "A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration. To make an existing application highly available and resilient while avoiding any single points of failure and giving the application the ability to scale to meet user demand, the best solution would be to deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones and use an Amazon RDS DB instance in a Multi-AZ configuration. By using an Amazon RDS DB instance in a Multi-AZ configuration, the database is automatically replicated across multiple Availability Zones, ensuring that the database is highly available and can withstand the failure of a single Availability Zone. This provides fault tolerance and avoids any single points of failure. Not D, deploying the primary and secondary database servers on EC2 instances across multiple Availability Zones and using Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances, may provide high availability for the database but may introduce additional complexity, and management overhead, and potential performance issues. Question #: 402"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248491400062&usg=AOvVaw211dVRuQ39mdas9xUtW1yr"]}, {"_id": 127, "question": "127 # A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams.What should a solutions architect do to resolve this issue?", "options": ["A. Update the Kinesis Data Streams default settings by modifying the data retention period.", "B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.", "C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.", "D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket. Selected Answer: A keyword here is - default settings and every other day and since \"A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).\""], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html"]}, {"_id": 128, "question": "128 # A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?", "options": ["A. Add required IAM permissions in the resource policy of the Lambda function.", "B. Create a signed request using the existing IAM credentials in the Lambda function.", "C. Create a new IAM user and use the existing IAM credentials in the Lambda function.", "D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248491960176&usg=AOvVaw0nZuSo1O6G7n9dtgMduoi- https://www.google.com/url?q=https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html&sa=D&source=apps-viewer-frontend&ust=1720248491960224&usg=AOvVaw0BcVhWd-RqEyxGvzfuzHHs https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248491960238&usg=AOvVaw1TxrSH5pJk3T5l9ybtOxgg https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/ 62 To grant the necessary permissions to an AWS Lambda function to upload files to Amazon S3, a solutions architect should create an IAM execution role with the required permissions and attach the IAM role to the Lambda function. This approach follows the principle of least privilege and ensures that the Lambda function can only access the resources it needs to perform its specific task. Therefore, the correct answer is"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248491960176&usg=AOvVaw0nZuSo1O6G7n9dtgMduoi-", "https://www.google.com/url?q=https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html&sa=D&source=apps-viewer-frontend&ust=1720248491960224&usg=AOvVaw0BcVhWd-RqEyxGvzfuzHHs", "https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html", "https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248491960238&usg=AOvVaw1TxrSH5pJk3T5l9ybtOxgg", "https://www.examtopics.com/discussions/amazon/view/102175-exam-aws-certified-solutions-architect-associate-saa-c03/"]}, {"_id": 129, "question": "129 # A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?", "options": ["A. Set the Lambda function's runtime timeout value to 15 minutes.", "B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.", "C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.", "D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda. Selected Answer: D To improve the architecture of this application, the best solution would be to use Amazon Simple Queue Service (Amazon SQS) to buffer the requests and decouple the S3 bucket from the Lambda function. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. By using Amazon SQS, the architecture is decoupled and the Lambda function can process the documents in a scalable and fault-tolerant manner. Question #: 405"], "explain": "", "answers": [], "resources": []}, {"_id": 130, "question": "130 # A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 130, "question": "130 # A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.) A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate. B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway. C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions. D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization. E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week. Selected Answer: DE A comparison of Answers D and E VERSUS another possible answer Answers A and E: Answers D and E: D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization. E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week. - Answer D scales the Auto Scaling group based on instance CPU utilization, which ensures that the number of instances in the group can be adjusted to handle the increase in traffic during working hours and reduce capacity during periods of low traffic. - Answer E uses scheduled scaling to change the Auto Scaling group minimum, https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248492517184&usg=AOvVaw2p76oIqehWmL0J3TzFUUMR 63 maximum, and desired capacity to zero for weekends, which ensures that the Auto Scaling group scales down to zero during weekends to save costs. Answers A and E:", "options": ["A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.", "B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.", "C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.", "D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.", "E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week. Selected Answer: DE A comparison of Answers D and E VERSUS another possible answer Answers A and E: Answers D and E:", "D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.", "E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week. - Answer D scales the Auto Scaling group based on instance CPU utilization, which ensures that the number of instances in the group can be adjusted to handle the increase in traffic during working hours and reduce capacity during periods of low traffic. - Answer E uses scheduled scaling to change the Auto Scaling group minimum,", "A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.", "E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week. - Answer A adjusts the capacity of the ALB based on request rate, which ensures that the ALB can handle the increase in traffic during working hours and reduce capacity during periods of low traffic. - Answer E uses scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends, which ensures that the Auto Scaling group scales down to zero during weekends to save costs. Comparing the two options, both Answers D and A are valid choices for scaling the application based on demand. However, Answer D scales the Auto Scaling group based on instance CPU utilization, which is a more granular metric than request rate and can provide better performance and cost optimization. Answer A only scales the ALB based on the request rate, which may not be sufficient for handling sudden spikes in traffic. Answer E is a common choice for scaling down to zero during weekends to save costs. Both Answers D and A can be used in conjunction with Answer E to ensure that the Auto Scaling group scales down to zero during weekends. However, Answer D provides more granular control over the scaling of the Auto Scaling group based on instance CPU utilization, which can result in better performance and cost optimization. In conclusion, answers D and E provide a more granular and flexible solution for scaling the application based on demand and scaling down to zero during weekends, while Answers A and E may not be as granular and may not provide as much performance and cost optimization. From <https://www.examtopics.com/discussions/amazon/view/102181-exam-aws-certified-solutions- architect-associate-saa-c03/> Question #: 406"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248492517184&usg=AOvVaw2p76oIqehWmL0J3TzFUUMR"]}, {"_id": 131, "question": "131 # A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.", "B. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.", "C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.", "D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers\u2019 security group on port 3306.", "E. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers\u2019 security group on port 3306. Selected Answer: CD To meet the requirements of allowing access to the web servers in the public subnet on port 443 and the Amazon RDS for MySQL DB instance in the database subnet on port 3306, the best solution would be to create a security group for the web servers and another security group for the DB instance, and then define the appropriate inbound and outbound rules for each security group. 1. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443. 2. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306. This will allow the web servers in the public subnet to receive traffic from the"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248493075763&usg=AOvVaw0wYI92xJ8wFWFDn-Cgb_K_", "https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/102181-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248493075813&usg=AOvVaw1Ia8zDsq2jdlTUEbLISKGJ", "https://www.examtopics.com/discussions/amazon/view/102181-exam-aws-certified-solutions-architect-associate-saa-c03/", "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"]}, {"_id": 132, "question": "132 # A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?", "options": ["A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.", "B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.", "C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.", "D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system. Selected Answer: D To meet the requirements of a shared storage solution for a gaming application that can be accessed using Lustre clients and is fully managed, the best solution would be to use Amazon FSx for Lustre. Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, such as high-performance computing, machine learning, and gaming. It provides a POSIX-compliant file system that can be accessed using Lustre clients and offers high performance, scalability, and data durability. This solution provides a highly available, scalable, and fully managed shared storage solution that can be accessed using Lustre clients. Amazon FSx for Lustre is optimized for compute-intensive workloads and provides high performance and durability. Answer A, creating an AWS DataSync task that shares the data as a mountable file system and mounting the file system to the application server, may not provide the required performance and scalability for a gaming application. Answer B, creating an AWS Storage Gateway file gateway and connecting the application server to the file share, may not provide the required performance and scalability for a gaming application. Answer C, creating an Amazon Elastic File System (Amazon EFS) file system and configuring it to support Lustre, may not provide the required performance and scalability for a gaming application and may require additional configuration and management overhead. Question #: 408"], "explain": "", "answers": [], "resources": []}, {"_id": 133, "question": "133 # A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?", "options": ["A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.", "B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.", "C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248493699382&usg=AOvVaw3HINqDgsG0535kK2JZEV2j https://www.google.com/url?q=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html&sa=D&source=apps-viewer-frontend&ust=1720248493699444&usg=AOvVaw2a2cl-cuv2JWViTq8gWnzT https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html 65 type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248493699382&usg=AOvVaw3HINqDgsG0535kK2JZEV2j", "https://www.google.com/url?q=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html&sa=D&source=apps-viewer-frontend&ust=1720248493699444&usg=AOvVaw2a2cl-cuv2JWViTq8gWnzT", "https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html", "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html"]}, {"_id": 134, "question": "134 # A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network- attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?", "options": ["A. Migrate the file share to Amazon RDS.", "B. Migrate the file share to AWS Storage Gateway.", "C. Migrate the file share to Amazon FSx for Windows File Server.", "D. Migrate the file share to Amazon Elastic File System (Amazon EFS). Selected Answer: C The most resilient and durable replacement for the on-premises file share in this scenario would be Amazon FSx for Windows File Server. Amazon FSx is a fully managed Windows file system service that is built on Windows Server and provides native support for the SMB protocol. It is designed to be highly available and durable, with built-in backup and restore capabilities. It is also fully integrated with AWS security services, providing encryption at rest and in transit, and it can be configured to meet compliance standards. Migrating the file share to Amazon RDS or AWS Storage Gateway is not appropriate as these services are designed for database workloads and block storage respectively, and do not provide native support for the SMB protocol. Migrating the file share to Amazon EFS (Linux ONLY) could be an option, but Amazon FSx for Windows File Server would be more appropriate in this case because it is specifically designed for Windows file shares and provides better performance for Windows applications. Question #: 410"], "explain": "", "answers": [], "resources": []}, {"_id": 135, "question": "135 # A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?", "options": ["A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.", "B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.", "C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248494273069&usg=AOvVaw1rZM5tRXlMcuvGt35Y3qk_ https://www.google.com/url?q=https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html&sa=D&source=apps-viewer-frontend&ust=1720248494273117&usg=AOvVaw3KLLaMfPG5jk-B7KOICKQp https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html 66", "D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active. Selected Answer: B The solution that will meet the requirement of ensuring that all data that is written to the EBS volumes is encrypted at rest is"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248494273069&usg=AOvVaw1rZM5tRXlMcuvGt35Y3qk_", "https://www.google.com/url?q=https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html&sa=D&source=apps-viewer-frontend&ust=1720248494273117&usg=AOvVaw3KLLaMfPG5jk-B7KOICKQp", "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html"]}, {"_id": 136, "question": "136 # A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost- effective database platform that will not require database modifications. Which solution will meet these requirements?", "options": ["A. Amazon DynamoDB", "B. Amazon RDS for MySQL", "C. MySQL-compatible Amazon Aurora Serverless", "D. MySQL deployed on Amazon EC2 in an Auto Scaling group Selected Answer: C Answer C, MySQL-compatible Amazon Aurora Serverless, would be the best solution to meet the company's requirements. Aurora Serverless can be a cost-effective option for databases with sporadic or unpredictable usage patterns since it automatically scales up or down based on the current workload. Additionally, Aurora Serverless is compatible with MySQL, so it does not require any modifications to the application's database code. A: DynamoDB is a NoSQL B: Fixed cost on RDS class D: More operation requires Question #: 412"], "explain": "", "answers": [], "resources": []}, {"_id": 137, "question": "137 # An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?", "options": ["A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.", "B. Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248494850967&usg=AOvVaw2EEwl8SWnz_VlK28afVQzS 67", "C. Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248494850967&usg=AOvVaw2EEwl8SWnz_VlK28afVQzS", "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html"]}, {"_id": 138, "question": "138 # An ecommerce company is experiencing an increase in user traffic. The company\u2019s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?", "options": ["A. Create a separate application tier using EC2 instances dedicated to email processing.", "B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).", "C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).", "D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group. Selected Answer: B The best option for addressing the company's needs of minimizing operational overhead and reducing time spent resolving email delivery issues is to use Amazon Simple Email Service (Amazon SES). Answer A of creating a separate application tier for email processing may add additional complexity to the architecture and require more operational overhead. Answer C of using Amazon Simple Notification Service (Amazon SNS) is not an appropriate solution for sending marketing and order confirmation emails since Amazon SNS is a messaging service that is designed to send messages to subscribed endpoints or clients. Answer D of creating a separate application tier using EC2 instances dedicated to email processing placed in an Auto Scaling group is a more complex solution than necessary and may result in additional operational overhead. Question #: 414"], "explain": "", "answers": [], "resources": []}, {"_id": 139, "question": "139 # A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?", "options": ["A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.", "B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248495586979&usg=AOvVaw2vw7ekpEQZkPSvcw0lp-Ah https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html&sa=D&source=apps-viewer-frontend&ust=1720248495587018&usg=AOvVaw0KAoLE1ISSpfHdPwh-wHOg https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html 68", "C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.", "D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP. Selected Answer: B The correct solution here is:"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248495586979&usg=AOvVaw2vw7ekpEQZkPSvcw0lp-Ah", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html&sa=D&source=apps-viewer-frontend&ust=1720248495587018&usg=AOvVaw0KAoLE1ISSpfHdPwh-wHOg", "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html"]}, {"_id": 140, "question": "140 # A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational efficiency?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 140, "question": "140 # A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational efficiency? A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering. B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier. C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval. D. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA). Selected Answer: A The correct answer is", "options": ["A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.", "B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.", "C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.", "A. Creating an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering would be the most efficient solution to optimize the cost of S3 usage. S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers (frequent and infrequent) based on changing access patterns. It is a cost-effective solution that does not require any manual intervention to move data to different storage classes, unlike the other options."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/s3/storage-classes/intelligent-tiering/"]}, {"_id": 141, "question": "141 # A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248496396843&usg=AOvVaw0Rj5eswLK9Umc4G_ngDE0q https://www.google.com/url?q=https://aws.amazon.com/s3/storage-classes/intelligent-tiering/&sa=D&source=apps-viewer-frontend&ust=1720248496396912&usg=AOvVaw0yQNHBpQ13iL0OIkbnFzwV https://aws.amazon.com/s3/storage-classes/intelligent-tiering/ 69 processing (OLTP) data in an Amazon RDS database The website\u2019s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)", "options": ["A. Configure an Amazon Redshift cluster.", "B. Set up an Amazon CloudFront distribution.", "C. Host the dynamic web content in Amazon S3.", "D. Create a read replica for the RDS DB instance.", "E. Configure a Multi-AZ deployment for the RDS DB instance. Selected Answer: BD To resolve the issue of slow page loads for a rapidly growing e-commerce website hosted on AWS, a solutions architect can take the following two actions: 1. Set up an Amazon CloudFront distribution 2. Create a read replica for the RDS DB instance Configuring an Amazon Redshift cluster is not relevant to this issue since Redshift is a data warehousing service and is typically used for the analytical processing of large amounts of data. Hosting the dynamic web content in Amazon S3 may not necessarily improve performance since S3 is an object storage service, not a web application server. While S3 can be used to host static web content, it may not be suitable for hosting dynamic web content since S3 doesn't support server-side scripting or processing. Configuring a Multi-AZ deployment for the RDS DB instance will improve high availability but may not necessarily improve performance. Question #: 417"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248496396843&usg=AOvVaw0Rj5eswLK9Umc4G_ngDE0q", "https://www.google.com/url?q=https://aws.amazon.com/s3/storage-classes/intelligent-tiering/&sa=D&source=apps-viewer-frontend&ust=1720248496396912&usg=AOvVaw0yQNHBpQ13iL0OIkbnFzwV", "https://aws.amazon.com/s3/storage-classes/intelligent-tiering/"]}, {"_id": 142, "question": "142 # A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?", "options": ["A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions\u2019 duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.", "B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.", "C. Purchase a Compute Savings Plan. Optimize the Lambda functions\u2019 duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.", "D. Purchase a Compute Savings Plan. Optimize the Lambda functions\u2019 duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC. Selected Answer: C Answer C is the best solution that meets the company\u2019s requirements. By purchasing a Compute Savings Plan, the company can save on the costs of running both EC2 instances and Lambda functions. The Lambda functions can be connected to the private subnet that contains the EC2 instances through a VPC endpoint for AWS services or a VPC peering connection. This provides direct network access to the EC2 instances while keeping the traffic within the private network, which helps to minimize network latency. Optimizing the Lambda functions\u2019 duration, memory usage, number of invocations, and amount of data transferred can help to further minimize costs and improve performance. Additionally, using a private"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248497283987&usg=AOvVaw13DlAn2Dmh5-TJZXs_J4Wn"]}, {"_id": 143, "question": "143 # A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account. The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?", "options": ["A. Attach the Administrator Access policy to the development account users.", "B. Add the development account as a principal in the trust policy of the role in the production account.", "C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.", "D. Create a user in the production account with unique credentials for each team member. Selected Answer: B Answer A, attaching the Administrator Access policy to development account users, provides too many permissions and violates the principle of least privilege. This would give users more access than they need, which could lead to security issues if their credentials are compromised. Answer C, turning off the S3 Block Public Access feature, is not a recommended solution as it is a security best practice to enable S3 Block Public Access to prevent accidental public access to S3 buckets. Answer D, creating a user in the production account with unique credentials for each team member, is also not a recommended solution as it can be difficult to manage and scale for large teams. It is also less secure, as individual user credentials can be more easily compromised."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/"]}, {"_id": 144, "question": "144 # A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)", "options": ["A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248498166100&usg=AOvVaw0o2UucTRFHKUUd62q5pHpz https://www.google.com/url?q=https://aws.amazon.com/savingsplans/faq/&sa=D&source=apps-viewer-frontend&ust=1720248498166155&usg=AOvVaw2Bt3mlHI0Uc6qBgBObdVwP https://aws.amazon.com/savingsplans/faq/ https://www.google.com/url?q=https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/&sa=D&source=apps-viewer-frontend&ust=1720248498166167&usg=AOvVaw1vq9yb2ZPGu6sN5ViMRxaO https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/ 71", "B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.", "C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.", "D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248498166100&usg=AOvVaw0o2UucTRFHKUUd62q5pHpz", "https://www.google.com/url?q=https://aws.amazon.com/savingsplans/faq/&sa=D&source=apps-viewer-frontend&ust=1720248498166155&usg=AOvVaw2Bt3mlHI0Uc6qBgBObdVwP", "https://aws.amazon.com/savingsplans/faq/", "https://www.google.com/url?q=https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/&sa=D&source=apps-viewer-frontend&ust=1720248498166167&usg=AOvVaw1vq9yb2ZPGu6sN5ViMRxaO", "https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/", "https://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/"]}, {"_id": 145, "question": "145 # A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?", "options": ["A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.", "B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.", "C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.", "D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint. Selected Answer: D The correct answer is:", "D. Use an Amazon RDS Multi-AZ DB cluster deployment. Point the read workload to the reader endpoint. Explanation: The company wants high availability, automatic failover support in less than 40 seconds, read offloading from the primary instance, and cost-effectiveness. Answer D is the best choice for several reasons: 1. Amazon RDS Multi-AZ deployments provide high availability and automatic failover support. 2. In a Multi-AZ DB cluster, Amazon RDS automatically provisions and maintains a standby in a different Availability Zone. If a failure occurs, Amazon RDS performs an automatic failover to the standby, minimizing downtime. 3. The \"Reader endpoint\" for an Amazon RDS DB cluster provides load-balancing support for read-only connections to the DB cluster. Directing read traffic to the reader endpoint helps in offloading read operations from the primary instance."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html", "https://aws.amazon.com/rds/features/multi-az/"]}, {"_id": 146, "question": "146 # A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248499009102&usg=AOvVaw2zO3TJZY6Mc18-O_mbOe84 https://www.google.com/url?q=https://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/&sa=D&source=apps-viewer-frontend&ust=1720248499009165&usg=AOvVaw3iLn--svIVL7JJp3iHRcez https://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/ https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html&sa=D&source=apps-viewer-frontend&ust=1720248499009181&usg=AOvVaw0If68Q0_RTDQ_04ZDRcbL_ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html https://www.google.com/url?q=https://aws.amazon.com/rds/features/multi-az/&sa=D&source=apps-viewer-frontend&ust=1720248499009196&usg=AOvVaw2CWs_qP04cpV_wXHSKHafw https://aws.amazon.com/rds/features/multi-az/ 72 high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?", "options": ["A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.", "B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.", "C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.", "D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service. Selected Answer: B EFS is best to serve this purpose. Option D is incorrect because it suggests using an S3 bucket in a private subnet with a VPC endpoint, which may not meet the requirement of maintaining control over user permissions as effectively as the EFS-based solution."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248499009102&usg=AOvVaw2zO3TJZY6Mc18-O_mbOe84", "https://www.google.com/url?q=https://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/&sa=D&source=apps-viewer-frontend&ust=1720248499009165&usg=AOvVaw3iLn--svIVL7JJp3iHRcez", "https://aws.amazon.com/blogs/compute/must-know-best-practices-for-amazon-ebs-encryption/", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html&sa=D&source=apps-viewer-frontend&ust=1720248499009181&usg=AOvVaw0If68Q0_RTDQ_04ZDRcbL_", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html", "https://www.google.com/url?q=https://aws.amazon.com/rds/features/multi-az/&sa=D&source=apps-viewer-frontend&ust=1720248499009196&usg=AOvVaw2CWs_qP04cpV_wXHSKHafw", "https://aws.amazon.com/rds/features/multi-az/", "https://aws.amazon.com/efs/"]}, {"_id": 147, "question": "147 # A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?", "options": ["A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.", "B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.", "C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.", "D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size. Selected Answer: D D, no need for an App Load balancer like C says, no where in the text. SQS is needed to ensure all request gets routed properly in a Microservices architecture and also that it waits until its picked up. ECS with Autoscaling, will scale based on the unknown pattern of usage as mentioned."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248499973496&usg=AOvVaw3SxLR7ywyYXYXH28809g-Z", "https://www.google.com/url?q=https://aws.amazon.com/efs/&sa=D&source=apps-viewer-frontend&ust=1720248499973553&usg=AOvVaw2Jd1f_YUG2Xjq53NS5AB9J", "https://aws.amazon.com/efs/", "https://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling-using-"]}, {"_id": 148, "question": "148 # A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions: Which IAM principals can the solutions architect attach this policy to? (Choose two.)", "options": ["A. Role", "B. Group", "C. Organization", "D. Amazon Elastic Container Service (Amazon ECS) resource", "E. Amazon EC2 resource Selected Answer: AB identity-based policy used for role and group Question #: 424"], "explain": "", "answers": [], "resources": []}, {"_id": 149, "question": "149 # A company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day. The company needs to scale out and scale in more instances based on workload. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.", "B. Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.", "C. Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.", "D. Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes. Selected Answer: B Question keyword \"scale out and scale in more instances\". Therefore not related Kubernetes. Choose B, reserved instance for front-end and spot instance for back-end. Question #: 425"], "explain": "", "answers": [], "resources": []}, {"_id": 150, "question": "150 # A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity. Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?", "options": ["A. GP2 volume type", "B. io2 volume type", "C. GP3 volume type", "D. io1 volume type Selected Answer: C The GP3 (General Purpose SSD) volume type in Amazon Elastic Block Store (EBS) is the most cost- effective option for the given requirements. GP3 volumes offer a balance of price and performance and are suitable for a wide range of workloads, including those with moderate I/O needs. GP3 volumes allow you to provision performance independently from storage capacity, which means you can adjust the baseline performance (measured in IOPS) and throughput (measured in MiB/s) separately from the volume size. This flexibility allows you to optimize your costs while meeting the workload requirements. In this case, since the company's daily peak input and output transactions per second are not more than 15,000 IOPS, GP3 volumes provide a suitable and cost-effective option for their workloads."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248500542571&usg=AOvVaw1KlktbKvKwFhJT9rdA1DWL", "https://www.google.com/url?q=https://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling-using-custom-metrics/&sa=D&source=apps-viewer-frontend&ust=1720248500542606&usg=AOvVaw3oN_cQ9HvnLho3OkW3a91m", "https://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling-using-custom-metrics/", "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-"]}, {"_id": 151, "question": "151 # A company needs to store data from its healthcare application. The application\u2019s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?", "options": ["A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.", "B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.", "C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.", "D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events. Selected Answer: A Datasync, this way we can monitor and audit all of the data at all times. With Snowcone / Snowball we lose access to audit the data while it arrives into AWS Data centers / Region / Availability Zone. AWS DataSync is a data transfer service that simplifies and accelerates moving large amounts of data to and from AWS. It is designed to securely and efficiently migrate data from on-premises storage systems to AWS services like Amazon S3. In this scenario, the company needs to securely migrate its healthcare application data to AWS while satisfying the new regulation for audit access. By using AWS DataSync, the existing data can be securely transferred to Amazon S3, ensuring the data is stored in a scalable and durable storage service. Additionally, using AWS CloudTrail to log data events ensures that all access and activity related to the data stored in Amazon S3 is audited. This helps meet the regulatory requirement for audit access at all levels of the stored data."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/ja_jp/datasync/latest/userguide/encryption-in-transit.html"]}, {"_id": 152, "question": "152 # A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?", "options": ["A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.", "B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.", "C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.", "D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group. B AWS Elastic Beanstalk provides an easy and quick way to deploy, manage, and scale applications. It supports a variety of platforms, including Java and Apache Tomcat. By using Elastic Beanstalk, the solutions architect can upload the Java application and configure the environment to run Apache Tomcat."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248501093301&usg=AOvVaw06hauWwIDQxjVn6TP3bZnv", "https://www.google.com/url?q=https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/&sa=D&source=apps-viewer-frontend&ust=1720248501093341&usg=AOvVaw0l3pdqlQpMZOmGRVrUh-qp", "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/", "https://www.google.com/url?q=https://docs.aws.amazon.com/ja_jp/datasync/latest/userguide/encryption-in-transit.html&sa=D&source=apps-viewer-frontend&ust=1720248501093356&usg=AOvVaw23O44mvogblYJTu4coxEaX", "https://docs.aws.amazon.com/ja_jp/datasync/latest/userguide/encryption-in-transit.html"]}, {"_id": 153, "question": "153 # A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?", "options": ["A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.", "B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.", "C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.", "D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role. Option B suggests creating an IAM role that includes Lambda as a trusted service, meaning the role is specifically designed for Lambda functions. The role should have a policy attached to it that grants the required read and write access to the DynamoDB table. Question #: 429"], "explain": "", "answers": [], "resources": []}, {"_id": 154, "question": "154 # The following IAM policy is attached to an IAM group. This is the only policy applied to the group. What are the effective IAM permissions of this policy for group members? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248501723527&usg=AOvVaw0HEoMMlhRIqdmxhHSbOndJ 76", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 154, "question": "154 # The following IAM policy is attached to an IAM group. This is the only policy applied to the group. What are the effective IAM permissions of this policy for group members? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248501723527&usg=AOvVaw0HEoMMlhRIqdmxhHSbOndJ 76 A. Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied. B. Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA). C. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action. D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region. Selected Answer: D", "options": ["A. Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.", "B. Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).", "C. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action.", "A. \"Statements after the Allow permission are not applied.\" --> Wrong.", "B. \"denied any Amazon EC2 permissions in the us-east-1 Region\" --> Wrong. Just deny 2 items.", "C. \"allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions\" --> Wrong. Just region us-east-1.", "D. ok. Question #: 430"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248501723527&usg=AOvVaw0HEoMMlhRIqdmxhHSbOndJ"]}, {"_id": 155, "question": "155 # A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost- effectively? (Choose two.) https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248502263512&usg=AOvVaw1Y5JwiMwygL1OWqr5-rog8 77", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 155, "question": "155 # A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost- effectively? (Choose two.) https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248502263512&usg=AOvVaw1Y5JwiMwygL1OWqr5-rog8 77 A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket. B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded. C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days. D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days. E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS). Selected Answer: BC", "options": ["A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.", "B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.", "C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.", "D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.", "A. Wrong because Lifecycle rule is not mentioned.", "B. CORRECT", "C. CORRECT", "D. Why Store on S3 One Zone-Infrequent Access (S3 One Zone-IA) when the files are going to irrelevant after 1 month? (Availability 99.99% - consider cost)", "E. again, Why use Reduced Redundancy Storage (RRS) when the files are irrelevant after 1 month? (Availability 99.99% - consider cost)"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248502263512&usg=AOvVaw1Y5JwiMwygL1OWqr5-rog8", "https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-"]}, {"_id": 156, "question": "156 # A company has developed a new video game as a web application. The application is in a three- tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game\u2019s developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?", "options": ["A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.", "B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.", "C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.", "D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application. Selected Answer: B See case study of leaderboard with Redis at"], "explain": "", "answers": [], "resources": ["https://redis.io/docs/data-types/sorted-sets/", "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html"]}, {"_id": 157, "question": "157 # An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248502851657&usg=AOvVaw0jMXIcnocxTRenZ-GIqIFl https://www.google.com/url?q=https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/&sa=D&source=apps-viewer-frontend&ust=1720248502851725&usg=AOvVaw2ZxBD3hw_i1ulRC-MoV55P https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/ https://www.google.com/url?q=https://redis.io/docs/data-types/sorted-sets/&sa=D&source=apps-viewer-frontend&ust=1720248502851734&usg=AOvVaw3s2Hd4MdVlekakIgDdVrle https://redis.io/docs/data-types/sorted-sets/ https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html&sa=D&source=apps-viewer-frontend&ust=1720248502851741&usg=AOvVaw0iu1RRGujRYovvlY-BLnZo https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html 78", "options": ["A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.", "B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.", "C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.", "D. Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon QuickSight to visualize the data. Selected Answer: B Question keyword \"machine learning\", answer keyword \"Amazon SageMaker\". Choose", "B. Use Amazon QuickSight for visualization. See \"Gaining insights with machine learning (ML) in Amazon QuickSight\" at"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248502851657&usg=AOvVaw0jMXIcnocxTRenZ-GIqIFl", "https://www.google.com/url?q=https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/&sa=D&source=apps-viewer-frontend&ust=1720248502851725&usg=AOvVaw2ZxBD3hw_i1ulRC-MoV55P", "https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class-amazon-s3-glacier-flexible-retrieval/", "https://www.google.com/url?q=https://redis.io/docs/data-types/sorted-sets/&sa=D&source=apps-viewer-frontend&ust=1720248502851734&usg=AOvVaw3s2Hd4MdVlekakIgDdVrle", "https://redis.io/docs/data-types/sorted-sets/", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html&sa=D&source=apps-viewer-frontend&ust=1720248502851741&usg=AOvVaw0iu1RRGujRYovvlY-BLnZo", "https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html", "https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-"]}, {"_id": 158, "question": "158 # A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 158, "question": "158 # A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags. Which solution will meet these requirements? A. Create a custom AWS Config rule to prevent tag modification except by authorized principals. B. Create a custom trail in AWS CloudTrail to prevent tag modification. C. Create a service control policy (SCP) to prevent tag modification except by authorized principals. D. Create custom Amazon CloudWatch logs to prevent tag modification. Selected Answer: C D \"Amazon CloudWatch\" just for logging, not for prevent tag modification https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies- cwe.html Amazon Organziaton has \"Service Control Policy (SCP)\" with \"tag policy\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html . Choose C. AWS Config for technical stuff, not for tag policies. Not", "options": ["A. Create a custom AWS Config rule to prevent tag modification except by authorized principals.", "B. Create a custom trail in AWS CloudTrail to prevent tag modification.", "C. Create a service control policy (SCP) to prevent tag modification except by authorized principals.", "D. Create custom Amazon CloudWatch logs to prevent tag modification. Selected Answer: C D \"Amazon CloudWatch\" just for logging, not for prevent tag modification", "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies- cwe.html Amazon Organziaton has \"Service Control Policy (SCP)\" with \"tag policy\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html . Choose", "A. Question #: 434"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies-", "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html"]}, {"_id": 159, "question": "159 # A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?", "options": ["A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.", "B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.", "C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.", "D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248503446100&usg=AOvVaw2t7HHXgTyHo9Ds8uwTdklz", "https://www.google.com/url?q=https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html&sa=D&source=apps-viewer-frontend&ust=1720248503446150&usg=AOvVaw0stQMfxBHAVPOQ6EYmgi3O", "https://docs.aws.amazon.com/quicksight/latest/user/making-data-driven-decisions-with-ml-in-quicksight.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies-cwe.html&sa=D&source=apps-viewer-frontend&ust=1720248503446160&usg=AOvVaw2lo7wPcUKFhjHdC47h23MB", "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies-cwe.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html&sa=D&source=apps-viewer-frontend&ust=1720248503446170&usg=AOvVaw2f1A8o2Mrz1EKtvXN1l4mb", "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html"]}, {"_id": 160, "question": "160 # A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 160, "question": "160 # A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively? A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication. B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication. C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Selected Answer: A Keyword \"20 TB\", choose \"AWS Snowball\", there are A or C. C has word \"GPU\" what is not related, therefore choose", "options": ["A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.", "B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication.", "C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication", "D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Selected Answer: A Keyword \"20 TB\", choose \"AWS Snowball\", there are A or", "A."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html"]}, {"_id": 161, "question": "161 # A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.", "B. Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.", "C. Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.", "D. Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance. Selected Answer: A Keyword \"Amazon RDS for PostgreSQL instance large\" . See list of size of instance at"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/rds/instance-types/"]}, {"_id": 162, "question": "162 # A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248503996300&usg=AOvVaw2tbnPc1aR1G4wAkSaW9DxQ https://www.google.com/url?q=https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html&sa=D&source=apps-viewer-frontend&ust=1720248503996351&usg=AOvVaw1hFXX1tn8wqnQ7zBtoSIei https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html https://www.google.com/url?q=https://aws.amazon.com/rds/instance-types/&sa=D&source=apps-viewer-frontend&ust=1720248503996364&usg=AOvVaw1Ltz5VRRmaRg9AQjJNPVs9 https://aws.amazon.com/rds/instance-types/ 80 requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?", "options": ["A. Deploy Amazon Inspector and associate it with the ALB.", "B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.", "C. Deploy rules to the network ACLs associated with the ALB to block the incomingtraffic.", "D. Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty. Selected Answer: B AWS Web Application Firewall (WAF) + ALB (Application Load Balancer) See image at"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248503996300&usg=AOvVaw2tbnPc1aR1G4wAkSaW9DxQ", "https://www.google.com/url?q=https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html&sa=D&source=apps-viewer-frontend&ust=1720248503996351&usg=AOvVaw1hFXX1tn8wqnQ7zBtoSIei", "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html", "https://www.google.com/url?q=https://aws.amazon.com/rds/instance-types/&sa=D&source=apps-viewer-frontend&ust=1720248503996364&usg=AOvVaw1Ltz5VRRmaRg9AQjJNPVs9", "https://aws.amazon.com/rds/instance-types/", "https://aws.amazon.com/waf/", "https://docs.aws.amazon.com/waf/latest/developerguide/ddos-", "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-", "https://aws.amazon.com/guardduty/"]}, {"_id": 163, "question": "163 # A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?", "options": ["A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.", "B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.", "C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.", "D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key. Selected Answer: D The most secure way for the company to share the database with the auditor is option D: Create an encrypted snapshot of the database, share the snapshot with the auditor, and allow access to the AWS Key Management Service (AWS KMS) encryption key. By creating an encrypted snapshot, the company ensures that the database data is protected at rest. Sharing the encrypted snapshot with the auditor allows them to have their own copy of the database securely. In addition, granting access to the AWS KMS encryption key ensures that the auditor has the necessary permissions to decrypt and access the encrypted snapshot. This allows the auditor to restore the snapshot and access the data securely. This approach provides both data protection and access control, ensuring that the database is securely shared with the auditor while maintaining the confidentiality and integrity of the data. Question #: 439"], "explain": "", "answers": [], "resources": []}, {"_id": 164, "question": "164 # A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?", "options": ["A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.", "B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.", "C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248504573611&usg=AOvVaw2_Z7RI61NyQ3jFzC1REK5z https://www.google.com/url?q=https://aws.amazon.com/waf/&sa=D&source=apps-viewer-frontend&ust=1720248504573666&usg=AOvVaw0WvaSeOGDsHq3kga_iM4sc https://aws.amazon.com/waf/ https://www.google.com/url?q=https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html&sa=D&source=apps-viewer-frontend&ust=1720248504573675&usg=AOvVaw1lGIwAhZN_1GcRNPK2NlRv https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html https://www.google.com/url?q=https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-keys.html&sa=D&source=apps-viewer-frontend&ust=1720248504573684&usg=AOvVaw2I_JvWgZfbp681zedSiDrp https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-keys.html https://www.google.com/url?q=https://aws.amazon.com/guardduty/&sa=D&source=apps-viewer-frontend&ust=1720248504573692&usg=AOvVaw2z3CihKtwsxUsXltYxioOz https://aws.amazon.com/guardduty/ 81"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248504573611&usg=AOvVaw2_Z7RI61NyQ3jFzC1REK5z", "https://www.google.com/url?q=https://aws.amazon.com/waf/&sa=D&source=apps-viewer-frontend&ust=1720248504573666&usg=AOvVaw0WvaSeOGDsHq3kga_iM4sc", "https://aws.amazon.com/waf/", "https://www.google.com/url?q=https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html&sa=D&source=apps-viewer-frontend&ust=1720248504573675&usg=AOvVaw1lGIwAhZN_1GcRNPK2NlRv", "https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-keys.html&sa=D&source=apps-viewer-frontend&ust=1720248504573684&usg=AOvVaw2I_JvWgZfbp681zedSiDrp", "https://docs.aws.amazon.com/waf/latest/developerguide/waf-rate-based-example-limit-login-page-keys.html", "https://www.google.com/url?q=https://aws.amazon.com/guardduty/&sa=D&source=apps-viewer-frontend&ust=1720248504573692&usg=AOvVaw2z3CihKtwsxUsXltYxioOz", "https://aws.amazon.com/guardduty/"]}, {"_id": 165, "question": "165 # A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)", "options": ["A. Import the RDS snapshot directly into Aurora.", "B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.", "C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.", "D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.", "E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora. Selected Answer: AC Amazon RDS for MySQL --> Amazon Aurora MySQL-compatible. * mysqldump, database dump --> (C) Upload to Amazon S3, Import dump to Aurora. * DB snapshot --> (A) Import RDS Snapshot directly Aurora. The correct word should be \"migration\". \"Use console to migrate the DB snapshot and create an Aurora MySQL DB cluster with the same databases as the original MySQL DB instance.\" Exclude B, because no need upload DB snapshot to Amazon S3. Exclude D, because no need Migration service. Exclude E, because no need Migration service. Use exclusion method is more easy for this question. Related links: - Amazon RDS create database snapshot"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html", "https://aws.amazon.com/rds/aurora/"]}, {"_id": 166, "question": "166 # A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?", "options": ["A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.", "B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.", "C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.", "D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents. Selected Answer: C Keyword \"Amazon CloudFront\", \"high volumes of static web content\", choose", "C. Static Web Content = S3 Always. CloudFront = Closer to the users locations since it will cache in the Edge nodes."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248505246722&usg=AOvVaw2Ks3LVLljxhPmqSIKmmiXX", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html&sa=D&source=apps-viewer-frontend&ust=1720248505246768&usg=AOvVaw3gZaSyjzGwmljZ4j225ZkD", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html", "https://www.google.com/url?q=https://aws.amazon.com/rds/aurora/&sa=D&source=apps-viewer-frontend&ust=1720248505246778&usg=AOvVaw1CSQXWfxQ1GPjZXdk0y948", "https://aws.amazon.com/rds/aurora/"]}, {"_id": 167, "question": "167 # A company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Copy the required data to a common account. Create an IAM access role in that account. Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.", "B. Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.", "C. Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.", "D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts. Selected Answer: D By utilizing Lake Formation's tag-based access control, you can define tags and tag-based policies to grant selective access to the required data for the engineering team accounts. This approach allows you to control access at a granular level without the need to copy or move the data to a common account or manage permissions individually in each account. It provides a centralized and scalable solution for securely sharing data across accounts with minimal operational overhead. Selected Answer: D"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/blogs/big-data/securely-share-your-data-across-aws-accounts-using-aws-lake-"]}, {"_id": 168, "question": "168 # A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?", "options": ["A. Use Amazon S3 with Transfer Acceleration to host the application.", "B. Use Amazon S3 with CacheControl headers to host the application.", "C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.", "D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application. Selected Answer: A Amazon S3 (Simple Storage Service) is a highly scalable object storage service provided by AWS. It allows you to store and retrieve any amount of data from anywhere on the web. With Amazon S3, you can host static websites, store and deliver large media files, and manage data for backup and restore. Transfer Acceleration is a feature of Amazon S3 that utilizes the AWS global infrastructure to accelerate file transfers to and from Amazon S3. It uses optimized network paths and parallelization techniques to speed up data transfer, especially for large files and over long distances. By using Amazon S3 with Transfer Acceleration, the web application can benefit from faster upload and download speeds,"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248505883245&usg=AOvVaw0_O5c0sexD7pprlQDN58P4", "https://www.google.com/url?q=https://aws.amazon.com/blogs/big-data/securely-share-your-data-across-aws-accounts-using-aws-lake-formation/&sa=D&source=apps-viewer-frontend&ust=1720248505883282&usg=AOvVaw0fSG_YzjY0VPyxQ1Tl3Ojn", "https://aws.amazon.com/blogs/big-data/securely-share-your-data-across-aws-accounts-using-aws-lake-formation/", "https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html"]}, {"_id": 169, "question": "169 # A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?", "options": ["A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.", "B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.", "C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.", "D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection. Selected Answer: B It is the only one with High Availability. Amazon RDS with Multi AZ EC2 with Auto Scaling Group in Multi Az Question #: 445"], "explain": "", "answers": [], "resources": []}, {"_id": 170, "question": "170 # A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data efficiently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?", "options": ["A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.", "B. Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.", "C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.", "D. Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system. Selected Answer: A Access during the transfer window -> DataSync By leveraging AWS DataSync in combination with AWS Direct Connect, the company can efficiently and securely transfer its 700 terabytes of data to an Amazon S3 bucket without disruption. The solution"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248506443138&usg=AOvVaw2Xf8ZOMKHu_ztcxPS3gdV0", "https://www.google.com/url?q=https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html&sa=D&source=apps-viewer-frontend&ust=1720248506443194&usg=AOvVaw3vDiZSpea9TWMUPM5b4kks", "https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html", "https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html"]}, {"_id": 171, "question": "171 # A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data after 7 years. Configure multi-factor authentication (MFA) delete for all S3 objects.", "B. Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.", "C. Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.", "D. Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance. Selected Answer: D Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html"]}, {"_id": 172, "question": "172 # A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route traffic to multiple Regions?", "options": ["A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.", "B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route traffic.", "C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.", "D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region. Selected Answer: A To route traffic to multiple AWS Regions and provide regional failover capabilities for a stateless web application running on AWS Lambda functions invoked by Amazon API Gateway, you can use Amazon Route 53 with an active-active failover configuration. By creating Amazon Route 53 health checks for each Region and configuring an active-active failover configuration, Route 53 can monitor the health of the endpoints in each Region and route traffic to healthy endpoints. In the event of a failure in one Region, Route 53 automatically routes traffic to the healthy endpoints in other Regions. This setup ensures high availability and failover capabilities for your web application across multiple AWS Regions."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"]}, {"_id": 173, "question": "173 # A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248507237348&usg=AOvVaw2ik-jB-3k2clX5t7XaXagj https://www.google.com/url?q=https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html&sa=D&source=apps-viewer-frontend&ust=1720248507237399&usg=AOvVaw1suaGyoRqZ9stGBeGD-ap5 https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html&sa=D&source=apps-viewer-frontend&ust=1720248507237412&usg=AOvVaw1IPnAP2Y0T_MZrcm5K72WI https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html https://www.google.com/url?q=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html&sa=D&source=apps-viewer-frontend&ust=1720248507237421&usg=AOvVaw33OncSy7dAHtgeaJ29oBib https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html 85 Production VPCs both use a single VPC peering connection to allow communication between the applications. What should a solutions architect do to mitigate any single point of failure in this architecture?", "options": ["A. Add a set of VPNs between the Management and Production VPCs.", "B. Add a second virtual private gateway and attach it to the Management VPC.", "C. Add a second set of VPNs to the Management VPC from a second customer gateway device.", "D. Add a second VPC peering connection between the Management VPC and the Production VPC. Selected Answer: C option D is not a valid solution for mitigating single points of failure in the architecture. I apologize for the confusion caused by the incorrect information. To mitigate single points of failure in the architecture, you can consider implementing option C: adding a second set of VPNs to the Management VPC from a second customer gateway device. This will introduce redundancy at the VPN connection level for the Management VPC, ensuring that if one customer gateway or VPN connection fails, the other connection can still provide connectivity to the data center. Question #: 449"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248507237348&usg=AOvVaw2ik-jB-3k2clX5t7XaXagj", "https://www.google.com/url?q=https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html&sa=D&source=apps-viewer-frontend&ust=1720248507237399&usg=AOvVaw1suaGyoRqZ9stGBeGD-ap5", "https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html&sa=D&source=apps-viewer-frontend&ust=1720248507237412&usg=AOvVaw1IPnAP2Y0T_MZrcm5K72WI", "https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html&sa=D&source=apps-viewer-frontend&ust=1720248507237421&usg=AOvVaw33OncSy7dAHtgeaJ29oBib", "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html"]}, {"_id": 174, "question": "174 # A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access. Which solution will help the company migrate the database to AWS MOST cost-effectively?", "options": ["A. Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services.", "B. Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.", "C. Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third-party features.", "D. Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX. Selected Answer: B Custom database features = Amazon RDS Custom for Oracle"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/"]}, {"_id": 175, "question": "175 # A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)", "options": ["A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).", "B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.", "C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.", "D. Use a single Amazon RDS database. Allow database access only from the application tier security group.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248507949790&usg=AOvVaw3lfjKtKlByqfAhk8dfihgZ https://www.google.com/url?q=https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/&sa=D&source=apps-viewer-frontend&ust=1720248507949835&usg=AOvVaw0b8aYaahlOqVfkRXIjMsCk https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/ 86", "E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.", "F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups. Selected Answer: C This solution follows the recommended architecture pattern of separating the web, application, and database tiers into different subnets. It provides better security, scalability, and fault tolerance. E.By using Elastic Load Balancers (ELBs), you can distribute traffic to multiple instances of the web tier, increasing scalability and availability. Controlling access through security groups allows for fine-grained control and ensures only authorized traffic reaches each layer."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248507949790&usg=AOvVaw3lfjKtKlByqfAhk8dfihgZ", "https://www.google.com/url?q=https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/&sa=D&source=apps-viewer-frontend&ust=1720248507949835&usg=AOvVaw0b8aYaahlOqVfkRXIjMsCk", "https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/"]}, {"_id": 176, "question": "176 # A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. Which activities will be managed by the company's operational team? (Choose three.)", "options": ["A. Management of the Amazon RDS infrastructure layer, operating system, and platforms", "B. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window", "C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection", "D. Installation of patches for all minor and major database versions for Amazon RDS", "E. Ensure the physical security of the Amazon RDS infrastructure in the data center", "F. Encryption of the data that moves in transit through Direct Connect From <https://www.examtopics.com/discussions/amazon/view/109408-exam-aws-certified-solutions- architect-associate-saa-c03/> Selected Answer: BCF In question has 3 keyword \"Amazon ECS\", \"AWS Direct Connect\", \"Amazon RDS\". With per Amazon services, choose 1 according answer. Has 6 items, need pick 3 items. ECS --> choose", "C. Direct Connect --> choose", "F. RDS --> Excluse A (by keyword \"infrastructure layer\"), Choose", "B. Exclusive D (by keyword \"patches for all minor and major database versions for Amazon RDS\"). Exclusive E (by keyword \"Ensure the physical security of the Amazon RDS\"). Easy question. Question #: 452"], "explain": "", "answers": [], "resources": []}, {"_id": 177, "question": "177 # A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?", "options": ["A. Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.", "B. Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.", "C. Use AWS App2Container (A2C) to containerize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task finishes.", "D. Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248508760896&usg=AOvVaw1jEtvRhaKhkOiEnEMIzHK- https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/109408-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248508760944&usg=AOvVaw3py2DAYoHQRJUhpLY5FIXz https://www.examtopics.com/discussions/amazon/view/109408-exam-aws-certified-solutions-architect-associate-saa-c03/ 87 Selected Answer: B \"AWS Batch jobs as EventBridge targets\" at https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html AWS Batch + Amazon EventBridge https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html . AWS Lambda just for a point of time per period. Choose"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248508760896&usg=AOvVaw1jEtvRhaKhkOiEnEMIzHK-", "https://www.google.com/url?q=https://www.examtopics.com/discussions/amazon/view/109408-exam-aws-certified-solutions-architect-associate-saa-c03/&sa=D&source=apps-viewer-frontend&ust=1720248508760944&usg=AOvVaw3py2DAYoHQRJUhpLY5FIXz", "https://www.examtopics.com/discussions/amazon/view/109408-exam-aws-certified-solutions-architect-associate-saa-c03/", "https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html", "https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html"]}, {"_id": 178, "question": "178 # A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specific time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?", "options": ["A. Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.", "B. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.", "C. Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.", "D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan. D, Governance is like the goverment, they can do things you cannot , like delete files or backups :D Compliance, nobody can!"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html"]}, {"_id": 179, "question": "179 # A company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory. The solutions architect needs to build and map the relationship details of the various workloads across all accounts. Which solution will meet these requirements in the MOST operationally efficient way?", "options": ["A. Use AWS Systems Manager Inventory to generate a map view from the detailed view report.", "B. Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads manually.", "C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.", "D. Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships. Selected Answer: C Option A: AWS SSM offers \"Software inventory\": Collect software catalog and configuration for your instances. Option C: Workload Discovery on AWS: is a tool for maintaining an inventory of the AWS resources across your accounts and various Regions and mapping relationships between them, and displaying them in a web UI."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/jp/builders-flash/202209/workload-discovery-on-aws/?awsf.filter-name=*all"]}, {"_id": 180, "question": "180 # A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period. Which combination of solutions will meet these requirements? (Choose three.)", "options": ["A. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.", "B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248509821370&usg=AOvVaw0oWoXFme1fUpQDRHJWeTRM https://www.google.com/url?q=https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html&sa=D&source=apps-viewer-frontend&ust=1720248509821416&usg=AOvVaw2jWM3uTIt5DxfitXDIF0M6 https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html https://www.google.com/url?q=https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html&sa=D&source=apps-viewer-frontend&ust=1720248509821430&usg=AOvVaw1uG5ApB8CHo07QSarbw97C https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html https://www.google.com/url?q=https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html&sa=D&source=apps-viewer-frontend&ust=1720248509821441&usg=AOvVaw0_3ofPaSg2q6nyJQRnoz-K https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html https://www.google.com/url?q=https://aws.amazon.com/jp/builders-flash/202209/workload-discovery-on-aws/?awsf.filter-name%3D*all&sa=D&source=apps-viewer-frontend&ust=1720248509821453&usg=AOvVaw3f0jsGKSYSliumZS6WLEH4 https://aws.amazon.com/jp/builders-flash/202209/workload-discovery-on-aws/?awsf.filter-name=*all 88", "C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.", "D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.", "E. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248509821370&usg=AOvVaw0oWoXFme1fUpQDRHJWeTRM", "https://www.google.com/url?q=https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html&sa=D&source=apps-viewer-frontend&ust=1720248509821416&usg=AOvVaw2jWM3uTIt5DxfitXDIF0M6", "https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html&sa=D&source=apps-viewer-frontend&ust=1720248509821430&usg=AOvVaw1uG5ApB8CHo07QSarbw97C", "https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html&sa=D&source=apps-viewer-frontend&ust=1720248509821441&usg=AOvVaw0_3ofPaSg2q6nyJQRnoz-K", "https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html", "https://www.google.com/url?q=https://aws.amazon.com/jp/builders-flash/202209/workload-discovery-on-aws/?awsf.filter-name%3D*all&sa=D&source=apps-viewer-frontend&ust=1720248509821453&usg=AOvVaw3f0jsGKSYSliumZS6WLEH4", "https://aws.amazon.com/jp/builders-flash/202209/workload-discovery-on-aws/?awsf.filter-name=*all", "https://docs.aws.amazon.com/ja_jp/awsaccountbilling/latest/aboutv2/view-billing-dashboard.html"]}, {"_id": 181, "question": "181 # A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region. Configure data replication.", "B. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.", "C. Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.", "D. Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data from the source Region to the second Region. Selected Answer: C Using AWS Backup, you can create backup plans that automate the backup process for your EC2 instances. By configuring cross-Region backup, you can ensure that backups are replicated to the second Region, providing a disaster recovery capability. This solution is cost-effective as it leverages AWS Backup's built-in features and eliminates the need for manual snapshot management or deploying and managing additional EC2 instances in the second Region. Question #: 457"], "explain": "", "answers": [], "resources": []}]