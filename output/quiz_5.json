[{"_id": 243, "question": "243 # A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance. Which solution meets these requirements?", "options": ["A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248528932027&usg=AOvVaw2jOX706gSqcFN72GZekeaM https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html&sa=D&source=apps-viewer-frontend&ust=1720248528932079&usg=AOvVaw3CFEq2-WQ02B0_TiiyxIUT https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html 117", "B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.", "C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248528932027&usg=AOvVaw2jOX706gSqcFN72GZekeaM", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html&sa=D&source=apps-viewer-frontend&ust=1720248528932079&usg=AOvVaw3CFEq2-WQ02B0_TiiyxIUT", "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html", "https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html", "https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/"]}, {"_id": 244, "question": "244 # A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application\u2019s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally efficient way?", "options": ["A. Write an AWS Lambda function that schedules nightly snapshots of the application\u2019s EBS volumes and copies the snapshots to a different Region.", "B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application\u2019s EC2 instances as resources.", "C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application\u2019s EBS volumes as resources.", "D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone. Selected Answer: B B is the most appropriate solution because it allows you to create a backup plan to automate the backup process of EC2 instances and EBS volumes, and copy backups to another region. Additionally, you can add the application's EC2 instances as resources to ensure their configuration and data are backed up nightly."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248529629324&usg=AOvVaw1hIlNyFvEblv9y5dc6gtdn", "https://www.google.com/url?q=https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html&sa=D&source=apps-viewer-frontend&ust=1720248529629382&usg=AOvVaw2eU2PmUrfNu-d4PNLlk3MQ", "https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html", "https://www.google.com/url?q=https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/&sa=D&source=apps-viewer-frontend&ust=1720248529629398&usg=AOvVaw1yB3i9vR3JT-ixbfkMqlRp", "https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/", "https://aws.amazon.com/vi/blogs/aws/aws-backup-ec2-instances-efs-single-file-restore-and-cross-"]}, {"_id": 245, "question": "245 # A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company\u2019s content on their mobile devices. What should a solutions architect recommend to meet these requirements?", "options": ["A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.", "B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.", "C. Use Amazon CloudFront. Provide signed URLs to stream content.", "D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content. Selected Answer: C Amazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront supports signed URLs that provide authorized access to your content. This feature allows the company to control who can access their content and for how long, providing a secure and scalable solution for millions of users."], "explain": "", "answers": [], "resources": ["https://www.amazonaws.cn/en/cloudfront/"]}, {"_id": 246, "question": "246 # A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?", "options": ["A. Amazon Aurora MySQL", "B. Amazon Aurora Serverless for MySQL", "C. Amazon Redshift Spectrum", "D. Amazon RDS for MySQL Selected Answer: B Amazon Aurora Serverless for MySQL is a fully managed, auto-scaling relational database service that scales up or down automatically based on the application demand. This service provides all the capabilities of Amazon Aurora, such as high availability, durability, and security, without requiring the customer to provision any database instances. With Amazon Aurora Serverless for MySQL, the sales team can enjoy minimal downtime since the database is designed to automatically scale to accommodate the increased traffic. Additionally, the service allows the customer to pay only for the capacity used, making it cost-effective for infrequent access patterns."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248530412015&usg=AOvVaw2MbQcQZSgYHPhpk8S3FnwO", "https://www.google.com/url?q=https://aws.amazon.com/vi/blogs/aws/aws-backup-ec2-instances-efs-single-file-restore-and-cross-region-backup/&sa=D&source=apps-viewer-frontend&ust=1720248530412059&usg=AOvVaw1cVaeBZONMOBL2-SDH2Uz9", "https://aws.amazon.com/vi/blogs/aws/aws-backup-ec2-instances-efs-single-file-restore-and-cross-region-backup/", "https://www.google.com/url?q=https://www.amazonaws.cn/en/cloudfront/&sa=D&source=apps-viewer-frontend&ust=1720248530412069&usg=AOvVaw06e6BaV40Dc-ZLIaysheHB", "https://www.amazonaws.cn/en/cloudfront/", "https://aws.amazon.com/rds/aurora/serverless/"]}, {"_id": 247, "question": "247 # A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings. Which solution will meet these requirements?", "options": ["A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any findings to AWS CloudTrail.", "B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any findings to AWS CloudTrail.", "C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.", "D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings. Selected Answer: D Amazon Inspector: Performs active vulnerability scans of EC2 instances. It looks for software vulnerabilities, unintended network accessibility, and other security issues. Requires installing an agent on EC2 instances to perform scans. The agent must be deployed to each instance. Provides scheduled scan reports detailing any findings of security risks or vulnerabilities. These reports can be used to patch or remediate issues. Is best suited for proactively detecting security weaknesses and misconfigurations in your AWS environment. C: Monitors for malicious activity like unusual API calls, unauthorized infrastructure deployments, or compromised EC2 instances. It uses machine learning and behavioral analysis of logs. Does not require installing any agents. It relies on analyzing AWS CloudTrail, VPC Flow Logs, and DNS logs. Alerts you to any detected threats, suspicious activity or policy violations in your AWS accounts. These alerts warrant investigation but may not always require remediation. Is focused on detecting active threats, unauthorized behavior, and signs of a compromise in your AWS environment. Can also detect some vulnerabilities and misconfigurations but coverage is not as broad as a dedicated service like Inspector. Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/inspector/features/?nc=sn&loc=2"]}, {"_id": 248, "question": "248 # A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?", "options": ["A. Increase the size of the EC2 instance to process messages faster.", "B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.", "C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.", "D. Use AWS Systems Manager Run Command to run the script on demand. Selected Answer: C"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248531744370&usg=AOvVaw3i6Tazq_SIH0p9tu5gWDRm", "https://www.google.com/url?q=https://aws.amazon.com/rds/aurora/serverless/&sa=D&source=apps-viewer-frontend&ust=1720248531744448&usg=AOvVaw0TNFbGGOfw3psjOm2mWSxz", "https://aws.amazon.com/rds/aurora/serverless/", "https://www.google.com/url?q=https://aws.amazon.com/inspector/features/?nc%3Dsn%26loc%3D2&sa=D&source=apps-viewer-frontend&ust=1720248531744457&usg=AOvVaw26N7eU5JMv8h82hAXaoD5u", "https://aws.amazon.com/inspector/features/?nc=sn&loc=2"]}, {"_id": 249, "question": "249 # A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.", "B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.", "C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.", "D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table. Selected Answer: A A would be the best solution as it involves the least operational overhead. With this solution, an AWS Glue ETL job is created to process the .csv files and store the processed data directly in Amazon Redshift. This is a serverless approach that does not require any infrastructure to be provisioned, configured, or maintained. AWS Glue provides a fully managed, pay-as-you-go ETL service that can be easily configured to process data from S3 and load it into Amazon Redshift. This approach allows the legacy application to continue to produce data in the CSV format that it currently uses, while providing the new COTS application with the ability to analyze the data using complex SQL queries."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html"]}, {"_id": 250, "question": "250 # A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Enable AWS CloudTrail and use it for auditing.", "B. Use data lifecycle policies for the Amazon EC2 instances.", "C. Enable AWS Trusted Advisor and reference the security dashboard.", "D. Enable AWS Config and create rules for auditing and compliance purposes.", "E. Restore previous resource configurations with an AWS CloudFormation template. Selected Answer: AD A) Enable AWS CloudTrail and use it for auditing. AWS CloudTrail provides a record of API calls and can be used to audit changes made to EC2 instances and security groups. By analyzing CloudTrail logs, the"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248532468023&usg=AOvVaw2TQvFz6L3i2U8Al77EuC-K", "https://www.google.com/url?q=https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html&sa=D&source=apps-viewer-frontend&ust=1720248532468062&usg=AOvVaw10GVweT9vIr2gO30L_KL9y", "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html"]}, {"_id": 251, "question": "251 # A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company\u2019s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?", "options": ["A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.", "B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.", "C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.", "D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key. Selected Answer: A AWS Systems Manager Session Manager provides secure shell access to EC2 instances without the need for SSH keys. It meets the security requirement to remove shared SSH keys while minimizing administrative overhead. Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also allows you to comply with corporate policies that require controlled access to managed nodes, strict security practices, and fully auditable logs with node access details, while providing end users with simple one-click cross-platform access to your managed nodes."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248533076497&usg=AOvVaw3w22toLRdbI7mgYqK_5akB", "https://www.google.com/url?q=https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html&sa=D&source=apps-viewer-frontend&ust=1720248533076541&usg=AOvVaw1e833uM596Dso0BMuiIuXC", "https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"]}, {"_id": 252, "question": "252 # A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company\u2019s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?", "options": ["A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.", "B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.", "C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.", "D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data. Selected Answer: A Provide near-real-time data ingestion into Kinesis Data Streams with the ability to handle the 1 MB/s ingestion rate. Data would be stored redundantly across shards. Enable near-real-time querying of the data using Kinesis Data Analytics. SQL queries can be run directly against the Kinesis data stream. Minimize data loss since data is replicated across shards. If an EC2 instance is rebooted, the data stream is still accessible. Scale seamlessly to handle varying ingestion and query rates. The other options would not fully meet the requirements: B) Kinesis Firehose + Redshift would introduce latency since data must be loaded from Firehose into Redshift before querying. Redshift would lack real-time capabilities. C) An EC2 instance store and Kinesis Firehose to S3 with Athena querying would risk data loss from instance store if an instance reboots. Athena querying data in S3 also lacks real-time capabilities. D) Using EBS storage, Kinesis Firehose to Redis and subscribing to Redis may provide near-real-time ingestion and querying but risks data loss if an EBS volume or EC2 instance fails. Recovery requires re-hydrating data from a backup which impacts real-time needs."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html"]}, {"_id": 253, "question": "253 # What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?", "options": ["A. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.", "B. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.", "C. Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.", "D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set. Selected Answer: D To encrypt an object at the time of upload, you need to add a header called x-amz-server-side- encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248533708547&usg=AOvVaw0aor-sJh3cRe3tr-Hy_l_l", "https://www.google.com/url?q=https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html&sa=D&source=apps-viewer-frontend&ust=1720248533708588&usg=AOvVaw2LVkFcyH34nxo62U77XWSY", "https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html"]}, {"_id": 254, "question": "254 # A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. Whatshould the solutions architect do to meet these requirements?", "options": ["A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.", "B. Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.", "C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.", "D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete. Selected Answer: C Answers A, B and D alert the user when thumbnail generation is complete. Answer C alerts the user through an application message that the image was received. Creating an Amazon Simple Queue Service (SQS) message queue and placing messages on the queue for thumbnail generation can help separate the image upload and thumbnail generation processes. Question #: 323"], "explain": "", "answers": [], "resources": []}, {"_id": 255, "question": "255 # A company\u2019s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company\u2019s security team to analyze. Which system architecture should the solutions architect recommend?", "options": ["A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.", "B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.", "C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248534336760&usg=AOvVaw3VHxgPyCHIr2XAvxke6tha 124"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248534336760&usg=AOvVaw3VHxgPyCHIr2XAvxke6tha"]}, {"_id": 256, "question": "256 # A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?", "options": ["A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.", "B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.", "C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.", "D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance. Selected Answer: D"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/storagegateway/", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248534874688&usg=AOvVaw3zoMNSCLun6iFzPd_u_eKQ", "https://www.google.com/url?q=https://aws.amazon.com/storagegateway/&sa=D&source=apps-viewer-frontend&ust=1720248534874732&usg=AOvVaw2z7lBZsM0TDu59PX4LDH-J", "https://aws.amazon.com/storagegateway/"]}, {"_id": 257, "question": "257 # A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?", "options": ["A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.", "B. Update the S3 ACL to allow the application to access the protected content.", "C. Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.", "D. Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content. Selected Answer: A Explanation: Amazon Cognito provides authentication and user management services for web and mobile applications. In this scenario, the application is using Amazon Cognito as an identity provider to authenticate users and obtain JSON Web Tokens (JWTs). The JWTs are used to access protected resources stored in another S3 bucket. To grant users access to the protected content, the proper IAM role needs to be assumed by the identity pool in Amazon Cognito. By updating the Amazon Cognito identity pool with the appropriate IAM role, users will be authorized to access the protected content in the S3 bucket. Option B is incorrect because updating the S3 ACL (Access Control List) will only affect the permissions of the application, not the users accessing the content. Option C is incorrect because redeploying the application to Amazon S3 will not resolve the issue related to user access permissions. Option D is incorrect because updating custom attribute mappings in Amazon Cognito will not directly grant users the proper permissions to access the protected content. Question #: 326"], "explain": "", "answers": [], "resources": []}, {"_id": 258, "question": "258 # An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 258, "question": "258 # An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.) A. Move assets to S3 Intelligent-Tiering after 30 days. B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads. C. Configure an S3 Lifecycle policy to clean up expired object delete markers. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248535577179&usg=AOvVaw3cCckUSr3FfnjxhMfjX_B0 126 D. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days. E. Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days. Selected Answer: AB Explanation:", "options": ["A. Move assets to S3 Intelligent-Tiering after 30 days.", "B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.", "C. Configure an S3 Lifecycle policy to clean up expired object delete markers.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248535577179&usg=AOvVaw3cCckUSr3FfnjxhMfjX_B0 126", "D. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.", "A. This storage class automatically analyzes the access patterns of objects and moves them between frequent access and infrequent access tiers. Since the objects will be accessed frequently for the first 30 days, storing them in the frequent access tier during that period optimizes performance. After 30 days, when the access patterns become inconsistent, S3 Intelligent-Tiering will automatically move the objects to the infrequent access tier, reducing storage costs.", "B. Multipart uploads are used for large objects, and incomplete multipart uploads can consume storage space if not cleaned up. By configuring an S3 Lifecycle policy to clean up incomplete multipart uploads, unnecessary storage costs can be avoided. Question #: 327"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248535577179&usg=AOvVaw3cCckUSr3FfnjxhMfjX_B0"]}, {"_id": 259, "question": "259 # A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party\u2019s URL. Other internet traffic must be blocked. Which solution meets these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 259, "question": "259 # A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party\u2019s URL. Other internet traffic must be blocked. Which solution meets these requirements? A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups. B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets. C. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs. D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB\u2019s target group for outbound access to the internet. Selected Answer: A Correct Answer", "options": ["A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.", "B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.", "C. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.", "A. Send the outbound connection from EC2 to Network Firewall. In Network Firewall, create stateful outbound rules to allow certain domains for software patch download and deny all other domains."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-"]}, {"_id": 260, "question": "260 # A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products. What Should a solutions architect recommend to ensure that all the requests are processed successfully?", "options": ["A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.", "B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248536124916&usg=AOvVaw0A0t3x1CA9XmsrdXipH5Gy https://www.google.com/url?q=https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html%23suricata-example-domain-filtering&sa=D&source=apps-viewer-frontend&ust=1720248536124955&usg=AOvVaw2I4T4oIDm-pAvYAulEHlIO https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html#suricata-example-domain-filtering 127", "C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248536124916&usg=AOvVaw0A0t3x1CA9XmsrdXipH5Gy", "https://www.google.com/url?q=https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html%23suricata-example-domain-filtering&sa=D&source=apps-viewer-frontend&ust=1720248536124955&usg=AOvVaw2I4T4oIDm-pAvYAulEHlIO", "https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata-examples.html#suricata-example-domain-filtering"]}, {"_id": 261, "question": "261 # A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance\u2019s patch status. Which solution will meet these requirements?", "options": ["A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.", "B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.", "C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.", "D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule. Selected Answer: D"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/detective/", "https://aws.amazon.com/macie/", "https://aws.amazon.com/guardduty/", "https://aws.amazon.com/vi/inspector/faqs/?nc1=f_ls"]}, {"_id": 262, "question": "262 # A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?", "options": ["A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.", "B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.", "C. Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.", "D. Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certificate."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248536907058&usg=AOvVaw3N7nxl61Ix5E3R4JKRk8_9", "https://www.google.com/url?q=https://aws.amazon.com/detective/&sa=D&source=apps-viewer-frontend&ust=1720248536907104&usg=AOvVaw3C-VaSsuMPswD8k1nuDsnH", "https://aws.amazon.com/detective/", "https://www.google.com/url?q=https://aws.amazon.com/macie/&sa=D&source=apps-viewer-frontend&ust=1720248536907119&usg=AOvVaw02VShULczD3RUFK84n3f9z", "https://aws.amazon.com/macie/", "https://www.google.com/url?q=https://aws.amazon.com/guardduty/&sa=D&source=apps-viewer-frontend&ust=1720248536907130&usg=AOvVaw0FD9X58QcmmAKHlbfoOvnh", "https://aws.amazon.com/guardduty/", "https://www.google.com/url?q=https://aws.amazon.com/vi/inspector/faqs/?nc1%3Df_ls&sa=D&source=apps-viewer-frontend&ust=1720248536907141&usg=AOvVaw1KymQZ2RzuHmsRIiOSKXwG", "https://aws.amazon.com/vi/inspector/faqs/?nc1=f_ls", "https://aws.amazon.com/certificate-manager/"]}, {"_id": 263, "question": "263 # A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company\u2019s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?", "options": ["A. Use AWS Snowball.", "B. Use AWS DataSync.", "C. Use a secure VPN connection.", "D. Use Amazon S3 Transfer Acceleration. Selected Answer: A Don't mix up between Mbps and Mbs. The proper calculation is: 10 MB/s x 86,400 seconds per day x 30 days/8 = 3,402,000 MB or approximately 3.4 TB"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html", "https://aws.amazon.com/s3/transfer-acceleration/"]}, {"_id": 264, "question": "264 # A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees\u2019 devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. Which solution will meet these requirements?", "options": ["A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees\u2019 IP addresses.", "B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.", "C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.", "D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On). Selected Answer: B This solution addresses the need for secure access to confidential and sensitive files, as well as the increase in remote usage. Migrating the files to Amazon FSx for Windows File Server provides a scalable, fully managed file storage solution in the AWS Cloud that is accessible from on-premises and cloud environments. Integration with the on-premises Active Directory allows for a consistent user experience and centralized access control. AWS Client VPN provides a secure and managed VPN solution that can be used by employees to access the files securely. 1-"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248537544214&usg=AOvVaw11fvzS-t5uX-wrCrmt9A94", "https://www.google.com/url?q=https://aws.amazon.com/certificate-manager/&sa=D&source=apps-viewer-frontend&ust=1720248537544254&usg=AOvVaw0g3J4QfA3Oqt9qzQGiVuki", "https://aws.amazon.com/certificate-manager/", "https://www.google.com/url?q=https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html&sa=D&source=apps-viewer-frontend&ust=1720248537544262&usg=AOvVaw0Weo2MkBjHHjTbTomFVMCs", "https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html", "https://www.google.com/url?q=https://aws.amazon.com/s3/transfer-acceleration/&sa=D&source=apps-viewer-frontend&ust=1720248537544269&usg=AOvVaw2z6SUE0apARKxuOfvF-Mkk", "https://aws.amazon.com/s3/transfer-acceleration/", "https://www.google.com/url?q=https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html&sa=D&source=apps-viewer-frontend&ust=1720248537544276&usg=AOvVaw0v2nR-dPUT8JwYAu-6lklQ", "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html", "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html"]}, {"_id": 265, "question": "265 # A company\u2019s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?", "options": ["A. Configure an Amazon CloudFront distribution in front of the ALB.", "B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.", "C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.", "D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances. Selected Answer: C C: because it allows for the proactive scaling of the EC2 instances before the monthly batch run begins. This will ensure that the application is able to handle the increased workload without experiencing downtime. The scheduled scaling policy can be configured to increase the number of instances in the Auto Scaling group a few hours before the batch run and then decrease the number of instances after the batch run is complete. This will ensure that the resources are available when needed and not wasted when not needed."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html"]}, {"_id": 266, "question": "266 # A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer\u2019s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer\u2019s application?", "options": ["A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.", "B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.", "C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).", "D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM). Selected Answer: A"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/vi/blogs/architecture/managed-file-transfer-using-aws-transfer-family-and-", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248538265963&usg=AOvVaw0LAQ4yfpu9J0ZWWp0oqaYN", "https://www.google.com/url?q=https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html&sa=D&source=apps-viewer-frontend&ust=1720248538266025&usg=AOvVaw0EmZzzG5pU5O4ZRkrTEpkf", "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html", "https://www.google.com/url?q=https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html&sa=D&source=apps-viewer-frontend&ust=1720248538266040&usg=AOvVaw0SUmKfsL3nRFEk9kWR05KF", "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html", "https://www.google.com/url?q=https://aws.amazon.com/vi/blogs/architecture/managed-file-transfer-using-aws-transfer-family-and-amazon-s3/&sa=D&source=apps-viewer-frontend&ust=1720248538266052&usg=AOvVaw13htZLANmkUF_o-XPsfOUy", "https://aws.amazon.com/vi/blogs/architecture/managed-file-transfer-using-aws-transfer-family-and-amazon-s3/"]}, {"_id": 267, "question": "267 # A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?", "options": ["A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.", "B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.", "C. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.", "D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge. Selected Answer: B Enabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows for rapid restoration of EBS volumes from snapshots. This reduces the time required to create an AMI from a snapshot, which is useful for quickly provisioning large Amazon EC2 instances. Provisioning an AMI by using the fast snapshot restore feature is a fast and efficient way to create an AMI. Once the AMI is created, it can be replaced in the Auto Scaling group without any downtime or disruption to running instances."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html"]}, {"_id": 268, "question": "268 # A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company\u2019s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?", "options": ["A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.", "B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.", "C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.", "D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket. Selected Answer: A"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248538992743&usg=AOvVaw2iiU4L7vJDwmm5Rk4YaA8y", "https://www.google.com/url?q=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html&sa=D&source=apps-viewer-frontend&ust=1720248538992820&usg=AOvVaw0gLkmBKMTGYlFGkA7B0ws2", "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html", "https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-"]}, {"_id": 269, "question": "269 # A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?", "options": ["A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.", "B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.", "C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.", "D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams. Selected Answer: A Option A is the most appropriate solution for reducing replication lag without significant changes to the application code and minimizing ongoing operational overhead. Migrating the database to Amazon Aurora MySQL allows for improved replication performance and higher scalability compared to Amazon RDS for MySQL. Aurora Replicas provide faster replication, reducing the replication lag, and Aurora Auto Scaling ensures that there are enough Aurora Replicas to handle the incoming traffic. Additionally, Aurora MySQL native functions can replace the stored procedures, reducing the load on the database and improving performance. Option B is not the best solution since adding an ElastiCache for Redis cluster does not address the replication lag issue, and the cache may not have the most up-to-date information. Additionally, replacing the stored procedures with AWS Lambda functions adds additional complexity and may not improve performance. Question #: 338"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248539693936&usg=AOvVaw2ElugIS10LHKU0mEjZK3DN", "https://www.google.com/url?q=https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/&sa=D&source=apps-viewer-frontend&ust=1720248539693978&usg=AOvVaw2vvNdTfbnO-ymyfdVPjsWi", "https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/"]}, {"_id": 270, "question": "270 # A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.", "B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.", "C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.", "D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region. Selected Answer: D I don' think A is intended because that would require knowledge of MySQL, which isn't what they are testing us on. Not option C because the question states large volume. If the volume were low, then DMS would be better. D provides automatic replication to a secondary Region through the Aurora global database feature. This feature provides automatic replication of data across AWS Regions, with the ability to control and configure the replication process. By specifying a minimum of one DB instance in the secondary Region, you can ensure that your secondary database is always available and up-to-date, allowing for quick failover in the event of a disaster."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.htmhttps://doc"]}, {"_id": 271, "question": "271 # A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?", "options": ["A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.", "B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.", "C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.", "D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store. Selected Answer: C"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248540272320&usg=AOvVaw2R0DCl9q8B0miBAsyG5oJ3", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.htmhttps://doc&sa=D&source=apps-viewer-frontend&ust=1720248540272367&usg=AOvVaw3Nt0lxFLDb7Yjw5xOs8LD_", "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.htmhttps://doc", "https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-"]}, {"_id": 272, "question": "272 # A media company hosts its website on AWS. The website application\u2019s architecture includes a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company\u2019s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?", "options": ["A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.", "B. Create an ALB listener rule to reply to SQL injections with a fixed response.", "C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.", "D. Set up Amazon Inspector to block all SQL injection attempts automatically. Selected Answer: A AWS WAF is a managed service that protects web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF enables customers to create custom rules that block common attack patterns, such as SQL injection attacks. By using AWS WAF in front of the ALB and associating the appropriate web ACLs with AWS WAF, the company can protect its website application from SQL injection attacks. AWS WAF will inspect incoming traffic to the website application and block requests that match the defined SQL injection patterns in the web ACLs. This will help to prevent SQL injection attacks from reaching the application, thereby improving the overall security posture of the application. B, C, and D are not the best solutions for this issue. Replying to SQL injections with a fixed response (B) is not a recommended approach as it does not actually fix the vulnerability, but only masks the issue. Subscribing to AWS Shield Advanced (C) is useful to protect against DDoS attacks but does not protect against SQL injection vulnerabilities. Amazon Inspector (D) is a vulnerability assessment tool and can identify vulnerabilities but cannot block attacks in real-time. Question #: 341"], "explain": "", "answers": [], "resources": []}, {"_id": 273, "question": "273 # A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company\u2019s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.", "B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248540995988&usg=AOvVaw1bDAwk9A245zoVEl0UlA4p https://www.google.com/url?q=https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/&sa=D&source=apps-viewer-frontend&ust=1720248540996034&usg=AOvVaw3YF-eeveRhq-GTqVr31ELw https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/ 134", "C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column-level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248540995988&usg=AOvVaw1bDAwk9A245zoVEl0UlA4p", "https://www.google.com/url?q=https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/&sa=D&source=apps-viewer-frontend&ust=1720248540996034&usg=AOvVaw3YF-eeveRhq-GTqVr31ELw", "https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/", "https://aws.amazon.com/blogs/big-data/enforce-column-level-authorization-with-amazon-quicksight-"]}, {"_id": 274, "question": "274 # A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group\u2019s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.", "B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.", "C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.", "D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group\u2019s desired capacity and maximum capacity by 20%. Selected Answer: C Use predictive scaling to increase the number of EC2 instances in your Auto Scaling group in advance of daily and weekly patterns in traffic flows. Predictive scaling is well suited for situations where you have: Cyclical traffic, such as high use of resources during regular business hours and low use of resources during evenings and weekends Recurring on-and-off workload patterns, such as batch processing, testing, or periodic data analysis Applications that take a long time to initialize, causing a noticeable latency impact on application performance during scale-out events"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248542246057&usg=AOvVaw2sl9FIbOw7qIBYxNxXVEA3", "https://www.google.com/url?q=https://aws.amazon.com/blogs/big-data/enforce-column-level-authorization-with-amazon-quicksight-and-aws-lake-formation/&sa=D&source=apps-viewer-frontend&ust=1720248542246110&usg=AOvVaw3PAe7xHaURRYeA6Wn9yGWI", "https://aws.amazon.com/blogs/big-data/enforce-column-level-authorization-with-amazon-quicksight-and-aws-lake-formation/", "https://www.google.com/url?q=https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html&sa=D&source=apps-viewer-frontend&ust=1720248542246120&usg=AOvVaw0EUW9F2dh_vODTlRav5o8i", "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html"]}, {"_id": 275, "question": "275 # A solutions architect is designing a company\u2019s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 275, "question": "275 # A solutions architect is designing a company\u2019s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead? A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication. B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones. C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region. D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region. Selected Answer: C", "options": ["A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.", "B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.", "C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.", "A. Multiple EC2 instances to be configured and updated manually in case of DR.", "B. Amazon RDS=Multi-AZ while it asks to be multi-region", "D. Manual process to start the DR, therefore same limitation as answer A C: is the best solution because it requires minimal operational overhead. Aurora is a managed service that provides automatic failover, so standby instances do not need to be manually configured. The primary DB cluster can be hosted in the primary Region, and the secondary DB cluster can be hosted in the DR Region. This approach ensures that the data is always available and up-to-date in multiple Regions, without requiring significant manual intervention. With dynamic scaling, the Auto Scaling group will automatically adjust the number of instances based on the actual workload. The target value for the CPU utilization metric is set to 60%, which is the baseline CPU utilization that is noted on each run, indicating that this is a reasonable level of utilization for the workload. This solution does not require any scheduling or forecasting, reducing the operational overhead. Question #: 344"], "explain": "", "answers": [], "resources": []}, {"_id": 276, "question": "276 # A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?", "options": ["A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.", "B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.", "C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.", "D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages. Selected Answer: A Amazon SQS has a limit of 256 KB for the size of messages. To handle messages larger than 256 KB, the Amazon SQS Extended Client Library for Java can be used. This library allows messages larger than 256 KB to be stored in Amazon S3 and provides a way to retrieve and process them. Using this solution, the application code can remain largely unchanged while still being able to process messages up to 50 MB in size."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248543775377&usg=AOvVaw1PugDtMmrOWJ8k4anp2JsO", "https://www.google.com/url?q=https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html&sa=D&source=apps-viewer-frontend&ust=1720248543775437&usg=AOvVaw3vf3L4-YbR5fEfBpPIWvL7", "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html"]}, {"_id": 277, "question": "277 # A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.", "B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.", "C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.", "D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally. Selected Answer: A Amazon CloudFront is a global content delivery network (CDN) service that can securely deliver web content, videos, and APIs at scale. It integrates with Cognito for authentication and with Lambda@Edge for authorization, making it an ideal choice for serving web content globally. Lambda@Edge is a service that lets you run AWS Lambda functions globally closer to users, providing lower latency and faster response times. It can also handle authorization logic at the edge to secure content in CloudFront. For this scenario, Lambda@Edge can provide authorization for the web application while leveraging the low-latency benefit of running at the edge. CloudFront=globally Lambda@edge = Authorization/ Latency Cognito=Authentication for Web apps"], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-"]}, {"_id": 278, "question": "278 # A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array\u2019s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?", "options": ["A. Volume Gateway", "B. Tape Gateway", "C. Amazon FSx File Gateway", "D. Amazon S3 File Gateway Selected Answer: D Amazon S3 File Gateway provides on-premises applications with access to virtually unlimited cloud storage using NFS and SMB file interfaces."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248544420213&usg=AOvVaw2oLSmE-hGDsIIDlBO35dNz", "https://www.google.com/url?q=https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/&sa=D&source=apps-viewer-frontend&ust=1720248544420253&usg=AOvVaw2Hhb4j5IPvxJp_TH5RheXw", "https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/", "https://aws.amazon.com/storagegateway/volume/", "https://aws.amazon.com/storagegateway/vtl/", "https://aws.amazon.com/storagegateway/file/fsx/", "https://aws.amazon.com/storagegateway/file/s3/"]}, {"_id": 279, "question": "279 # A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company. The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Compute Savings Plan", "B. EC2 Instance Savings Plan", "C. Zonal Reserved Instances", "D. Standard Reserved Instances Selected Answer: A Savings Plans offer a flexible pricing model that provides savings on AWS usage. You can save up to 72 percent on your AWS compute workloads. Compute Savings Plans provide lower prices on Amazon EC2 instance usage regardless of instance family, size, OS, tenancy, or AWS Region. This also applies to AWS Fargate and AWS Lambda usage. SageMaker Savings Plans provide you with lower prices for your Amazon SageMaker instance usage, regardless of your instance family, size, component, or AWS Region."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"]}, {"_id": 280, "question": "280 # A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.", "B. Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248544979856&usg=AOvVaw0krEKTQNJhwh-BzSK5RwSA https://www.google.com/url?q=https://aws.amazon.com/storagegateway/volume/&sa=D&source=apps-viewer-frontend&ust=1720248544979902&usg=AOvVaw1kvSuEuc8M1nxrX1B2unvz https://aws.amazon.com/storagegateway/volume/ https://www.google.com/url?q=https://aws.amazon.com/storagegateway/vtl/&sa=D&source=apps-viewer-frontend&ust=1720248544979914&usg=AOvVaw0lZ7qLm_ZQeVHhk5qXM_aq https://aws.amazon.com/storagegateway/vtl/ https://www.google.com/url?q=https://aws.amazon.com/storagegateway/file/fsx/&sa=D&source=apps-viewer-frontend&ust=1720248544979925&usg=AOvVaw2SZVXn_kIySDu3pOS5GzwS https://aws.amazon.com/storagegateway/file/fsx/ https://www.google.com/url?q=https://aws.amazon.com/storagegateway/file/s3/&sa=D&source=apps-viewer-frontend&ust=1720248544979936&usg=AOvVaw3N6x5O432F6wTQHjxBZb7Q https://aws.amazon.com/storagegateway/file/s3/ https://www.google.com/url?q=https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html&sa=D&source=apps-viewer-frontend&ust=1720248544979946&usg=AOvVaw3WdVV9mPy5U26Xd5YxDSew https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html 138", "C. Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248544979856&usg=AOvVaw0krEKTQNJhwh-BzSK5RwSA", "https://www.google.com/url?q=https://aws.amazon.com/storagegateway/volume/&sa=D&source=apps-viewer-frontend&ust=1720248544979902&usg=AOvVaw1kvSuEuc8M1nxrX1B2unvz", "https://aws.amazon.com/storagegateway/volume/", "https://www.google.com/url?q=https://aws.amazon.com/storagegateway/vtl/&sa=D&source=apps-viewer-frontend&ust=1720248544979914&usg=AOvVaw0lZ7qLm_ZQeVHhk5qXM_aq", "https://aws.amazon.com/storagegateway/vtl/", "https://www.google.com/url?q=https://aws.amazon.com/storagegateway/file/fsx/&sa=D&source=apps-viewer-frontend&ust=1720248544979925&usg=AOvVaw2SZVXn_kIySDu3pOS5GzwS", "https://aws.amazon.com/storagegateway/file/fsx/", "https://www.google.com/url?q=https://aws.amazon.com/storagegateway/file/s3/&sa=D&source=apps-viewer-frontend&ust=1720248544979936&usg=AOvVaw3N6x5O432F6wTQHjxBZb7Q", "https://aws.amazon.com/storagegateway/file/s3/", "https://www.google.com/url?q=https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html&sa=D&source=apps-viewer-frontend&ust=1720248544979946&usg=AOvVaw3WdVV9mPy5U26Xd5YxDSew", "https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html", "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapaci"]}, {"_id": 281, "question": "281 # A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap- southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company\u2019s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?", "options": ["A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company\u2019s AWS account.", "B. Create a database snapshot. Add the acquiring company\u2019s AWS account to the KMS key policy. Share the snapshot with the acquiring company\u2019s AWS account.", "C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company\u2019s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.", "D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company\u2019s AWS account. Selected Answer: B Option A is not recommended because copying the snapshot to a new unencrypted snapshot will compromise the confidentiality of the data. Option C is not recommended because using a different AWS managed KMS key will not allow the acquiring company's AWS account to access the encrypted data. Option D is not recommended because downloading the database snapshot and uploading it to an Amazon S3 bucket will increase the risk of data leakage or loss of confidentiality during the transfer process."], "explain": "", "answers": [], "resources": ["https://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/"]}, {"_id": 282, "question": "282 # A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us- east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance. The company must also run reports on the RDS database several times a year. The report process causes transactions to take longer than usual to post to the customers\u2019 accounts. The company needs a solution that will improve the performance of the report process. Which combination of steps will meet these requirements? (Choose two.)", "options": ["A. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.", "B. Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another Availability Zone.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248545788098&usg=AOvVaw3Zt9RbkNnndUPpbUSFaCjY https://www.google.com/url?q=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapaci&sa=D&source=apps-viewer-frontend&ust=1720248545788137&usg=AOvVaw3VsC9BUwO4UPrb9onpOS-9 https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapaci https://www.google.com/url?q=https://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/&sa=D&source=apps-viewer-frontend&ust=1720248545788146&usg=AOvVaw01JhvcKs1Y6HKxKlLa0lPn https://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/ 139", "C. Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to the read replica.", "D. Migrate the database to RDS Custom.", "E. Use RDS Proxy to limit reporting requests to the maintenance window. Selected Answer: AC", "B. It will not help improve the performance of the report process.", "D. Migrating to RDS Custom does not address the issue of high availability and automatic recovery."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248545788098&usg=AOvVaw3Zt9RbkNnndUPpbUSFaCjY", "https://www.google.com/url?q=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapaci&sa=D&source=apps-viewer-frontend&ust=1720248545788137&usg=AOvVaw3VsC9BUwO4UPrb9onpOS-9", "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapaci", "https://www.google.com/url?q=https://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/&sa=D&source=apps-viewer-frontend&ust=1720248545788146&usg=AOvVaw01JhvcKs1Y6HKxKlLa0lPn", "https://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/"]}, {"_id": 283, "question": "283 # An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?", "options": ["A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.", "B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.", "C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.", "D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route. Correct Answer: B Question #:252"], "explain": "", "answers": [], "resources": []}, {"_id": 284, "question": "284 # A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?", "options": ["A. Amazon Elastic File System (Amazon EFS)", "B. Amazon Elastic Block Store (Amazon EBS)", "C. Amazon S3 Glacier Deep Archive", "D. AWS Backup Correct Answer: A Question #:253"], "explain": "", "answers": [], "resources": []}, {"_id": 285, "question": "285 # A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group. A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248546367923&usg=AOvVaw1HUnkDZzlhsC0iTeWHCo1F 140", "options": ["A. Deleting IAM users", "B. Deleting directories", "C. Deleting Amazon EC2 instances", "D. Deleting logs from Amazon CloudWatch Logs Correct Answer: C Question #:254"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248546367923&usg=AOvVaw1HUnkDZzlhsC0iTeWHCo1F"]}, {"_id": 286, "question": "286 # A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?", "options": ["A. Create security group rules using the instance ID as the source or destination.", "B. Create security group rules using the security group ID as the source or destination.", "C. Create security group rules using the VPC CIDR blocks as the source or destination.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248547328117&usg=AOvVaw22Gme7dEfp0hJ9CNKYbyNg 141"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248547328117&usg=AOvVaw22Gme7dEfp0hJ9CNKYbyNg"]}, {"_id": 287, "question": "287 # A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workflow to prevent the creation of multiple orders?", "options": ["A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.", "B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.", "C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.", "D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue. Correct Answer: D Question #:256"], "explain": "", "answers": [], "resources": []}, {"_id": 288, "question": "288 # A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)", "options": ["A. Enable a read-only bucket ACL.", "B. Enable versioning on the bucket.", "C. Attach an IAM policy to the bucket.", "D. Enable MFA Delete on the bucket.", "E. Encrypt the bucket using AWS KMS. Correct Answer: BD Question #:257"], "explain": "", "answers": [], "resources": []}, {"_id": 289, "question": "289 # A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?", "options": ["A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.", "B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.", "C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248547905945&usg=AOvVaw22MlSKGeiuAK_73YnhmHZo 142"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248547905945&usg=AOvVaw22MlSKGeiuAK_73YnhmHZo"]}, {"_id": 290, "question": "290 # A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.", "B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.", "C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.", "D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job. Correct Answer: D Question #:259"], "explain": "", "answers": [], "resources": []}, {"_id": 291, "question": "291 # A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?", "options": ["A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.", "B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.", "C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.", "D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years. Correct Answer: A"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.ht", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER.SQLServer.AddlFeat.TransactionLog", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248548519807&usg=AOvVaw180xyRSlw1XjIZQZirFwwJ", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.ht&sa=D&source=apps-viewer-frontend&ust=1720248548519863&usg=AOvVaw1p_leP2vs7kUF8a9J5OFgz", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.ht", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER.SQLServer.AddlFeat.TransactionLog&sa=D&source=apps-viewer-frontend&ust=1720248548519878&usg=AOvVaw2btV-Gtq4V7v_M_uzki51k", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER.SQLServer.AddlFeat.TransactionLog"]}, {"_id": 292, "question": "292 # A company\u2019s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?", "options": ["A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.", "B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.", "C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.", "D. Join the file system to the Active Directory to restrict access. Correct Answer: D"], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD-best-practices.html"]}, {"_id": 293, "question": "293 # A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Configure Amazon CloudFront to cache multiple versions of the content.", "B. Configure a host header in a Network Load Balancer to forward traffic to different instances.", "C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.", "D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.", "E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances. Correct Answer: AC Question #:262"], "explain": "", "answers": [], "resources": []}, {"_id": 294, "question": "294 # A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application\u2019s Amazon EC2 instances. Both VPCs are in the us-east-1 Region. The solutions architect must implement a solution to provide the application\u2019s EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?", "options": ["A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster\u2019s security group to allow inbound connection from the application\u2019s security group.", "B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application\u2019s security group.", "C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection\u2019s security group to allow inbound connection from the application\u2019s security group.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248549091865&usg=AOvVaw0p39h1yGMw7G3NAiWjtIrU https://www.google.com/url?q=https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD-best-practices.html&sa=D&source=apps-viewer-frontend&ust=1720248549091919&usg=AOvVaw1wQEjQtge3TsCZTRQ1uyTh https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD-best-practices.html 144"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248549091865&usg=AOvVaw0p39h1yGMw7G3NAiWjtIrU", "https://www.google.com/url?q=https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD-best-practices.html&sa=D&source=apps-viewer-frontend&ust=1720248549091919&usg=AOvVaw1wQEjQtge3TsCZTRQ1uyTh", "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/self-managed-AD-best-practices.html"]}, {"_id": 295, "question": "295 # A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)", "options": ["A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.", "B. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.", "C. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.", "D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.", "E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice. Correct Answer: AD Question #:264"], "explain": "", "answers": [], "resources": []}, {"_id": 296, "question": "296 # A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?", "options": ["A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.", "B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.", "C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.", "D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53. Correct Answer: D A & B: While associating health checks with each record can help identify unhealthy instances, it does not provide automatic load balancing and distribution of traffic to healthy instances. C: While CloudFront can improve performance and availability, it is primarily a CDN and may not directly address the issue of load balancing and distributing traffic to healthy instances. Therefore, option D is the most appropriate solution to overcome the timeout errors by implementing an ALB with health checks and routing traffic through Route 53. Question #:265"], "explain": "", "answers": [], "resources": []}, {"_id": 297, "question": "297 # A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248549674387&usg=AOvVaw0oRuHHR2rqm7buM6KGrtR_ 145", "options": ["A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.", "B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.", "C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.", "D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin. Correct Answer: C Question #:266"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248549674387&usg=AOvVaw0oRuHHR2rqm7buM6KGrtR_"]}, {"_id": 298, "question": "298 # A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints. Which solution meets these requirements?", "options": ["A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.", "B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.", "C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.", "D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data. Correct Answer: A", "B. While CloudFront can help with caching and content delivery, it does not provide the mechanism to monitor the health of the application or perform traffic redirection based on health checks.", "C. This configuration is suitable for static content delivery but does not address the health monitoring and traffic redirection requirements of the application.", "D. While this can enhance performance, it does not monitor the health of the application or redirect traffic based on health checks. Therefore, option A is the most suitable solution as it leverages AWS Global Accelerator to monitor application health, route traffic to healthy endpoints, and optimize the user experience while addressing latency concerns. Question #:267"], "explain": "", "answers": [], "resources": []}, {"_id": 299, "question": "299 # A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248550222636&usg=AOvVaw2POOy3Rr4CpKWC7LjQ8Miv 146", "options": ["A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.", "B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.", "C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.", "D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Correct Answer: D Question #:268"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248550222636&usg=AOvVaw2POOy3Rr4CpKWC7LjQ8Miv"]}, {"_id": 300, "question": "300 # A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application\u2019s architecture. What should a solutions architect do to meet these requirements?", "options": ["A. Use Amazon ElastiCache in front of the database.", "B. Use RDS Proxy between the application and the database.", "C. Migrate the application from EC2 instances to AWS Lambda.", "D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB. Correct Answer: A Question #:269"], "explain": "", "answers": [], "resources": []}, {"_id": 301, "question": "301 # An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. Whatshould the solutions architect recommend?", "options": ["A. Export the data to Amazon DynamoDB and have the business analysts run their queries.", "B. Load the data into Amazon ElastiCache and have the business analysts run their queries.", "C. Create a read replica of the primary database and have the business analysts run their queries.", "D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries. Correct Answer: C Question #:270"], "explain": "", "answers": [], "resources": []}, {"_id": 302, "question": "302 # A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?", "options": ["A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.", "B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.", "C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.", "D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key. Correct Answer: A Question #:271"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248550813778&usg=AOvVaw3ehCUKAryuwNIk2ubjC4uE"]}, {"_id": 303, "question": "303 # A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the \u2018same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete. Whatshould the solutions architect do to meet these requirements?", "options": ["A. Increase the minimum capacity for the Auto Scaling group.", "B. Increase the maximum capacity for the Auto Scaling group.", "C. Configure scheduled scaling to scale up to the desired compute level.", "D. Change the scaling policy to add more EC2 instances during each scaling operation. Correct Answer: C Question #:272"], "explain": "", "answers": [], "resources": []}, {"_id": 304, "question": "304 # A company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website\u2019s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and efficiently regardless of a user\u2019s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?", "options": ["A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.", "B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.", "C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.", "D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy. Correct Answer: B Question #:273"], "explain": "", "answers": [], "resources": []}]