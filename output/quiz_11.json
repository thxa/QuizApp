[{"_id": 624, "question": "624# A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP. The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations. What should a solutions architect do to meet these requirements? A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses. B. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses. C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses. D. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses. Selected Answer: A", "options": ["A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.", "B. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.", "C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.", "A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses. Explanation: AWS Web Application Firewall (WAF) is designed to protect web applications from common web exploits. By associating a WAF web ACL with the ALB, you can set up IP rule sets to filter incoming traffic based on source IP addresses. Updating the IP addresses in the rule to include the registered IP addresses allows you to control and restrict access only to authorized locations. Conclusion:", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248648984634&usg=AOvVaw2i0tYpX_DEtKZ6C_Ei3yH4 275 This option provides a secure and scalable solution for restricting access to the web application based on registered IP addresses.", "B. Deploy AWS Firewall Manager to manage the ALB. Configure firewall rules to restrict traffic to the ALB. Modify the firewall rules to include the registered IP addresses. Explanation: AWS Firewall Manager is more suitable for managing security policies at an organizational level rather than specific to individual applications. While AWS Firewall Manager can manage WAF policies, using WAF directly with ALB is a more straightforward and common approach. Conclusion: Although Firewall Manager can be used, directly associating WAF with ALB is a more straightforward and common approach for this specific use case.", "C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses. Explanation: While Lambda functions can be used for various purposes, using AWS WAF with IP rule sets is a more direct and efficient approach for IP filtering. DynamoDB might be overkill for a simple list of registered IP addresses. Conclusion: Using Lambda for IP validation introduces unnecessary complexity, and WAF with IP rule sets is a more direct solution."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248648984634&usg=AOvVaw2i0tYpX_DEtKZ6C_Ei3yH4", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248649527688&usg=AOvVaw0fnu_eMKtWCC6ShiglWxX8"]}, {"_id": 625, "question": "625# A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of the data that contain sensitive information. Which solution will meet these requirements with the LEAST operational overhead?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 625, "question": "625# A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of the data that contain sensitive information. Which solution will meet these requirements with the LEAST operational overhead? A. Create an IAM role that includes permissions to access Lake Formation tables. B. Create data filters to implement row-level security and cell-level security. C. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data. D. Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation tables. Selected Answer: B", "options": ["A. Create an IAM role that includes permissions to access Lake Formation tables.", "B. Create data filters to implement row-level security and cell-level security.", "C. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.", "A. IAM roles are typically used for authentication and authorization at the AWS service level. However, when it comes to fine-grained access control at the data level (row-level or cell-level), IAM roles alone might not be sufficient.", "B.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248650078478&usg=AOvVaw0-lXrYZGsBu_fnxS25blJJ 277 Data filters in AWS Lake Formation are designed for implementing row-level and cell-level security. This option aligns with the requirement of controlling access at the data level and is a suitable approach for this scenario.", "C. This option involves preprocessing data before it's ingested into Lake Formation. While it adds a layer of control, it's more about data transformation before storage, not necessarily about access control at the data level."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248650078478&usg=AOvVaw0-lXrYZGsBu_fnxS25blJJ"]}, {"_id": 626, "question": "626# A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source data into Amazon S3 buckets so that the data can be processed in the future. According to compliance laws, the data must not be transmitted over the public internet. Servers in the company's on-premises data center will consume the output from an application that runs on the EC2 instances. Which solution will meet these requirements?", "options": ["A. Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection between the company and the VPC.", "B. Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.", "C. Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS Site-to- Site VPN connection between the company and the VPC.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248650710758&usg=AOvVaw1Sy-y3f2RfobxZ_ZkLkLg1 278"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248650710758&usg=AOvVaw1Sy-y3f2RfobxZ_ZkLkLg1"]}, {"_id": 627, "question": "627# A company has an application with a REST-based interface that allows data to be received in near- real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances. The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests. 279 Which design should a solutions architect recommend to provide a more scalable solution?", "options": ["A. Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.", "B. Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.", "C. Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.", "D. Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group. Selected Answer: A Amazon Kinesis Data Streams can handle large volumes of streaming data, providing a scalable and resilient solution. AWS Lambda functions can be triggered by Kinesis Data Streams, allowing the application to process the data in near-real time. Lambda scales automatically based on the incoming event rate, ensuring that the system can handle spikes in data volume. Option B: API Gateway might not be the best fit for handling near-real-time streaming data. Quota limits in API Gateway are more suited for request-based APIs rather than high-throughput data streaming. Option C: While SNS supports message publishing, it may not be the most suitable choice for high-throughput streaming data. Auto Scaling with an Application Load Balancer is more suitable for traditional request-based workloads. Option D: While containerization provides flexibility and scalability, it might not be the most efficient solution for handling near-real-time streaming data. Options involving streaming services like Kinesis are better suited for this use case. Conclusion:"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248651760991&usg=AOvVaw0AdkPvVqfAxSujfjWPk6Vb"]}, {"_id": 628, "question": "628# A company has an application that runs on Amazon EC2 instances in a private subnet. The application needs to process sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to the S3 bucket. Which solution will meet these requirements?", "options": ["A. Configure an internet gateway. Update the S3 bucket policy to allow access from the internet gateway. Update the application to use the new internet gateway.", "B. Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN connection. Update the application to use the new VPN connection.", "C. Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway. Update the application to use the new NAT gateway.", "D. Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint. Selected Answer: D A VPC endpoint allows your EC2 instances to connect to services like Amazon S3 directly within the AWS network without traversing the internet. No internet gateway or NAT gateway is required for this solution, ensuring that the application does not use the internet to connect to the S3 bucket. It enhances security by keeping the traffic within the AWS network and avoids exposure to the public internet. Option A: This option introduces an internet gateway, which is unnecessary and exposes the application to the public internet, contrary to the requirement. Option B: VPN connections are typically used for secure connections between on-premises networks and AWS, and they don't directly apply to connecting to S3 from within the VPC."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248652330949&usg=AOvVaw1M8ctfhEBIYguJLK6mOck4"]}, {"_id": 629, "question": "629# A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application. The EKS cluster stores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is encrypted. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).", "B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).", "C. Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).", "D. Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service (AWS KMS). Selected Answer: B Amazon EKS provides the option to encrypt Kubernetes secrets at rest using AWS Key Management Service (AWS KMS). This is a native and managed solution within the EKS service, reducing operational overhead. Kubernetes secrets are automatically encrypted using the default AWS KMS key for the EKS cluster. This ensures that sensitive information stored in Kubernetes secrets is encrypted, providing security. Option A: This option would require custom implementation within the container application, increasing operational overhead and complexity. Enabling encryption at the EKS level is a more straightforward approach. Option C:"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248652949768&usg=AOvVaw3vHuCbQsz5cZHV2KXyVClf"]}, {"_id": 630, "question": "630# A company is designing a new multi-tier web application that consists of the following components: \u2022 Web and application servers that run on Amazon EC2 instances as part of Auto Scaling groups \u2022 An Amazon RDS DB instance for data storage A solutions architect needs to limit access to the application servers so that only the web servers can access them. Which solution will meet these requirements?", "options": ["A. Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow only the web servers to access the application servers.", "B. Deploy a VPC endpoint in front of the application servers. Configure the security group to allow only the web servers to access the application servers.", "C. Deploy a Network Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the network ACL to allow only the web servers to access the application servers.", "D. Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers. Selected Answer: D An Application Load Balancer (ALB) can be used to distribute incoming web traffic across multiple Amazon EC2 instances. The ALB can be configured with a target group that contains the Auto Scaling group of application servers."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248653524041&usg=AOvVaw0c4qHpiCQH4eorOBgQm7Nx"]}, {"_id": 631, "question": "631# A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service (Amazon EKS). The application has a microservices architecture. The company needs to implement a solution that collects, aggregates, and summarizes metrics and logs from the application in a centralized location. Which solution meets these requirements?", "options": ["A. Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.", "B. Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh console.", "C. Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon OpenSearch Service.", "D. Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console. Selected Answer: D"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248654121075&usg=AOvVaw2gwOBePhjb-rJP2Y2WalAx"]}, {"_id": 632, "question": "632# A company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network Load Balancer. The company stores the product\u2019s objects in an Amazon S3 bucket. The company recently experienced malicious attacks against its systems. The company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. The solution must also report suspicious activity and display the information on a dashboard. Which solution will meet these requirements?", "options": ["A. Configure Amazon Macie to monitor and report findings to AWS Config.", "B. Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248654677396&usg=AOvVaw2ftfJM8afec6e5SeAiG-EJ 285", "C. Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248654677396&usg=AOvVaw2ftfJM8afec6e5SeAiG-EJ", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248655252587&usg=AOvVaw1N8q6NaMyVXtwFLHcH_NBh"]}, {"_id": 633, "question": "633# A company wants to migrate an on-premises data center to AWS. The data center hosts a storage server that stores data in an NFS-based file system. The storage server holds 200 GB of data. The company needs to migrate the data without interruption to existing services. Multiple resources in AWS must be able to access the data by using the NFS protocol. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)", "options": ["A. Create an Amazon FSx for Lustre file system.", "B. Create an Amazon Elastic File System (Amazon EFS) file system.", "C. Create an Amazon S3 bucket to receive the data.", "D. Manually use an operating system copy command to push the data into the AWS destination.", "E. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on- premises location and AWS. Selected Answer: BE Option A: Amazon FSx for Lustre is a fully managed file system optimized for compute-intensive workloads. It may not be the most suitable option if the requirement is to maintain NFS compatibility, as Lustre is a different file system. It's designed for high-performance computing scenarios. Option C: Amazon S3 is an object storage service, and it doesn't natively support the NFS protocol. If the application or resources accessing the data require an NFS-based file system, S3 might not be the best fit without additional tools or configurations. It could be used in conjunction with other services or tools to provide the required NFS compatibility. Option D: This option involves a manual copy process, and it might be error-prone and time-consuming for large datasets. It doesn't offer a seamless migration experience, and there could be downtime during the migration. AWS provides services like DataSync for efficient and automated data transfer, which could be a better alternative."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248655963519&usg=AOvVaw1o61YmBc7NNOBvoQg6S_a_"]}, {"_id": 634, "question": "634# A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that have an SMB file share mounted as a volume in the us-east-1 Region. The company has a recovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned service disruptions. The company needs to replicate the file system to the us-west-2 Region. The replicated data must not be deleted by any user for 5 years. Which solution will meet these requirements?", "options": ["A. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us- west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.", "B. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us- west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.", "C. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us- west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.", "D. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248661723943&usg=AOvVaw0xyeoGKNfuGihDJxeu9nkL 288 west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years. Selected Answer: C Option A: The Single-AZ 2 deployment type in us-east-1 might not provide the same level of availability as a Multi- AZ deployment. The rest of the configuration is similar to Option"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248661723943&usg=AOvVaw0xyeoGKNfuGihDJxeu9nkL"]}, {"_id": 635, "question": "635# A solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user-level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which action meets these requirements?", "options": ["A. Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248663067939&usg=AOvVaw0Uge-lzeC-j5eMAQezqUhr 289", "B. Create a new trail in CloudTrail from within the developer accounts with the organization trails option enabled.", "C. Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248663067939&usg=AOvVaw0Uge-lzeC-j5eMAQezqUhr"]}, {"_id": 636, "question": "636# A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 636, "question": "636# A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements? A. Instance store volume B. Amazon ElastiCache for Memcached cluster C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665030163&usg=AOvVaw1xI0A3tGuzfO568YsjsM-P 290 Selected Answer: C", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 636, "question": "636# A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements? A. Instance store volume B. Amazon ElastiCache for Memcached cluster C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665030163&usg=AOvVaw1xI0A3tGuzfO568YsjsM-P 290 Selected Answer: C A. Instance store volumes are temporary, ephemeral storage that is directly attached to the physical host of the instance. They are suitable for temporary data, caching, and other scenarios where data persistence is not critical. However, they are not durable, meaning that data is lost if the underlying instance is stopped or terminated. B. Amazon ElastiCache is a managed in-memory caching service. It is designed for caching frequently accessed data to improve application performance. While it provides low-latency access for cached data, it doesn't offer durable storage as the data is stored in-memory and can be evicted based on cache policies. C. Provisioned IOPS SSD volumes are designed for applications that require predictable and consistent I/O performance. You can provision a specific number of IOPS when creating the volume to ensure consistent low-latency performance. These volumes provide durability and are suitable for business-critical applications. D. Throughput Optimized HDD volumes are designed for large, sequential workloads with a focus on providing high throughput. They are more suitable for scenarios where high throughput is more critical than low-latency random access. While they provide durability, they may not offer the same low-latency performance as Provisioned IOPS SSD volumes for certain types of applications. Question #: 621 https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665788162&usg=AOvVaw0r8xBf17Ub_90rM_P5K-Qk 291 637# An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us- west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. Which solution will meet this requirement with the LEAST operational effort?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 636, "question": "636# A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements? A. Instance store volume B. Amazon ElastiCache for Memcached cluster C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665030163&usg=AOvVaw1xI0A3tGuzfO568YsjsM-P 290 Selected Answer: C A. Instance store volumes are temporary, ephemeral storage that is directly attached to the physical host of the instance. They are suitable for temporary data, caching, and other scenarios where data persistence is not critical. However, they are not durable, meaning that data is lost if the underlying instance is stopped or terminated. B. Amazon ElastiCache is a managed in-memory caching service. It is designed for caching frequently accessed data to improve application performance. While it provides low-latency access for cached data, it doesn't offer durable storage as the data is stored in-memory and can be evicted based on cache policies. C. Provisioned IOPS SSD volumes are designed for applications that require predictable and consistent I/O performance. You can provision a specific number of IOPS when creating the volume to ensure consistent low-latency performance. These volumes provide durability and are suitable for business-critical applications. D. Throughput Optimized HDD volumes are designed for large, sequential workloads with a focus on providing high throughput. They are more suitable for scenarios where high throughput is more critical than low-latency random access. While they provide durability, they may not offer the same low-latency performance as Provisioned IOPS SSD volumes for certain types of applications. Question #: 621 https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665788162&usg=AOvVaw0r8xBf17Ub_90rM_P5K-Qk 291 637# An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us- west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. Which solution will meet this requirement with the LEAST operational effort? A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket. B. Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east- 1 in the CORS rule's AllowedOrigin element. C. Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket. D. Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket. Selected Answer: A", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 636, "question": "636# A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements? A. Instance store volume B. Amazon ElastiCache for Memcached cluster C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665030163&usg=AOvVaw1xI0A3tGuzfO568YsjsM-P 290 Selected Answer: C A. Instance store volumes are temporary, ephemeral storage that is directly attached to the physical host of the instance. They are suitable for temporary data, caching, and other scenarios where data persistence is not critical. However, they are not durable, meaning that data is lost if the underlying instance is stopped or terminated. B. Amazon ElastiCache is a managed in-memory caching service. It is designed for caching frequently accessed data to improve application performance. While it provides low-latency access for cached data, it doesn't offer durable storage as the data is stored in-memory and can be evicted based on cache policies. C. Provisioned IOPS SSD volumes are designed for applications that require predictable and consistent I/O performance. You can provision a specific number of IOPS when creating the volume to ensure consistent low-latency performance. These volumes provide durability and are suitable for business-critical applications. D. Throughput Optimized HDD volumes are designed for large, sequential workloads with a focus on providing high throughput. They are more suitable for scenarios where high throughput is more critical than low-latency random access. While they provide durability, they may not offer the same low-latency performance as Provisioned IOPS SSD volumes for certain types of applications. Question #: 621 https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665788162&usg=AOvVaw0r8xBf17Ub_90rM_P5K-Qk 291 637# An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us- west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. Which solution will meet this requirement with the LEAST operational effort? A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket. B. Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east- 1 in the CORS rule's AllowedOrigin element. C. Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket. D. Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket. Selected Answer: A A. This option involves creating a new S3 bucket in the target region (us-east-1) and enabling Cross-Region Replication (CRR) to automatically replicate objects from the source (us-west-1) bucket to the target bucket. This is a straightforward and fully managed solution. B. CORS is typically used to control which web domains can access resources in a web page. It is not designed for cross-region replication. This option doesn't provide the necessary mechanism for copying photos to a different region. C. While this option involves creating a second bucket, it introduces the concept of Lifecycle rules. However, S3 Lifecycle rules are primarily used for transitioning objects between storage classes within the same bucket and not for cross-region replication. D. This option involves using S3 events to trigger an AWS Lambda function to copy photos from one bucket to another. While it is a valid approach, it introduces the need to write and manage a Lambda function, increasing operational complexity compared to the fully managed Cross-Region Replication in option", "options": ["A. Instance store volume", "B. Amazon ElastiCache for Memcached cluster", "C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume", "D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume", "A. Instance store volumes are temporary, ephemeral storage that is directly attached to the physical host of the instance. They are suitable for temporary data, caching, and other scenarios where data persistence is not critical. However, they are not durable, meaning that data is lost if the underlying instance is stopped or terminated.", "B. Amazon ElastiCache is a managed in-memory caching service. It is designed for caching frequently accessed data to improve application performance. While it provides low-latency access for cached data, it doesn't offer durable storage as the data is stored in-memory and can be evicted based on cache policies.", "C. Provisioned IOPS SSD volumes are designed for applications that require predictable and consistent I/O performance. You can provision a specific number of IOPS when creating the volume to ensure consistent low-latency performance. These volumes provide durability and are suitable for business-critical applications.", "D. Throughput Optimized HDD volumes are designed for large, sequential workloads with a focus on providing high throughput. They are more suitable for scenarios where high throughput is more critical than low-latency random access. While they provide durability, they may not offer the same low-latency performance as Provisioned IOPS SSD volumes for certain types of applications. Question #: 621", "A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.", "B. Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east- 1 in the CORS rule's AllowedOrigin element.", "C. Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket.", "A. This option involves creating a new S3 bucket in the target region (us-east-1) and enabling Cross-Region Replication (CRR) to automatically replicate objects from the source (us-west-1) bucket to the target bucket. This is a straightforward and fully managed solution.", "B. CORS is typically used to control which web domains can access resources in a web page. It is not designed for cross-region replication. This option doesn't provide the necessary mechanism for copying photos to a different region.", "C. While this option involves creating a second bucket, it introduces the concept of Lifecycle rules. However, S3 Lifecycle rules are primarily used for transitioning objects between storage classes within the same bucket and not for cross-region replication.", "A. Question #: 622"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665030163&usg=AOvVaw1xI0A3tGuzfO568YsjsM-P", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248665788162&usg=AOvVaw0r8xBf17Ub_90rM_P5K-Qk", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248666322729&usg=AOvVaw3CE2CCMxDpkwIhXEiRCn82"]}, {"_id": 638, "question": "638# A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema. Which solutions will meet these requirements and provide the MOST scalability? (Choose two.)", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 638, "question": "638# A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema. Which solutions will meet these requirements and provide the MOST scalability? (Choose two.) A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity. B. Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode. C. Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled. D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin. E. Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume. Selected Answer: CD", "options": ["A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.", "B. Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.", "C. Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.", "D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.", "A. This option allows DynamoDB to automatically scale read and write capacity based on the actual traffic patterns. It is a suitable choice for unpredictable workloads and rapid scaling.", "B. Aurora Serverless automatically adjusts the database capacity based on actual usage. It is a good fit for applications with varying workloads and provides cost savings during periods of low activity.", "C. DynamoDB auto scaling dynamically adjusts the provisioned capacity based on actual traffic. This is a good option for handling varying workloads and ensuring optimal performance.", "D. This is a valid approach for serving static content with low-latency globally using Amazon CloudFront. It helps in scalability and improves performance by distributing content to edge locations.", "E. This option introduces more complexity by managing EC2 instances and periodic content refresh. For static content, using S3 and CloudFront is a more straightforward and scalable approach."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248666987761&usg=AOvVaw0UEScuuloiIQflep46JtWh"]}, {"_id": 639, "question": "639# A company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The company must protect the REST APIs from SQL injection and cross-site scripting attacks. What is the MOST operationally efficient solution that meets these requirements?", "options": ["A. Configure AWS Shield.", "B. Configure AWS WAF.", "C. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in CloudFront.", "D. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront. Selected Answer: B SQL Injection and Cross-Site Scripting = WAF so Either B or", "D. Both B and D are valid options but the question doesn't indicate a real need for CloudFront, so just use WAF with the API Gateway. Question #: 624"], "explain": "", "answers": [], "resources": []}, {"_id": 640, "question": "640# A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources while preserving access to the on-premises resources. What should the solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 640, "question": "640# A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources while preserving access to the on-premises resources. What should the solutions architect do to meet these requirements? A. Create an IAM user for each user in the company. Attach the appropriate policies to each user. B. Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248668142685&usg=AOvVaw2pW7WKdEhH4K0mvgk20eMF 294 C. Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups. D. Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups. Selected Answer: D", "options": ["A. Create an IAM user for each user in the company. Attach the appropriate policies to each user.", "B. Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248668142685&usg=AOvVaw2pW7WKdEhH4K0mvgk20eMF 294", "C. Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups.", "A. This approach would involve creating individual IAM users in AWS for each company user. While it's possible, it's not operationally efficient, especially with a large number of users. Managing users individually can become cumbersome, and it doesn't leverage the existing Active Directory infrastructure.", "B. Amazon Cognito is a service that provides user identity and access control for mobile applications and web applications. While it supports integration with Active Directory, it's more suitable for scenarios involving customer identity and access management. In this case, the requirement is to integrate with the company's existing Active Directory used for on-premises resources, and using SAML-based federation would be more appropriate.", "C. While defining roles and mapping them to Active Directory groups is a step in the right direction, the key here is to leverage SAML-based federation, which is designed for integrating corporate identity systems like Active Directory with AWS.", "D. SAML 2.0-based Federation: SAML enables single sign-on (SSO) between the company's Active Directory and AWS. Users can use their existing corporate credentials to access AWS resources without having to manage a separate set of credentials for AWS. Roles and Policies: With SAML-based federation, roles are created in AWS that define the permissions users will have. Policies are attached to these roles to specify what actions users are allowed to perform. Mapping to Active Directory Groups: Roles in AWS can be mapped to Active Directory groups. This allows you to manage permissions centrally in Active Directory groups, and users inherit these permissions when they assume the associated roles in AWS. In summary, SAML-based federation provides a standardized way to enable SSO between AWS and the company's Active Directory, ensuring a seamless experience for users while maintaining centralized access control through Active Directory groups."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248668142685&usg=AOvVaw2pW7WKdEhH4K0mvgk20eMF", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248668865574&usg=AOvVaw0-pBEU4qz2qJ21FTpfz4hz"]}, {"_id": 641, "question": "641# A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights. Which configuration should the solutions architect choose to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 641, "question": "641# A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights. Which configuration should the solutions architect choose to meet these requirements? A. Configure Amazon CloudFront with AWS WAF. B. Configure Application Load Balancers with AWS WAF C. Configure Amazon Route 53 with a geolocation policy D. Configure Amazon Route 53 with a geoproximity routing policy Selected Answer: C", "options": ["A. Configure Amazon CloudFront with AWS WAF.", "B. Configure Application Load Balancers with AWS WAF", "C. Configure Amazon Route 53 with a geolocation policy", "A. Amazon CloudFront is a content delivery network (CDN) service. AWS WAF (Web Application Firewall) can be used to protect web applications from common web exploits. While CloudFront can help with content delivery and caching, AWS WAF is primarily focused on security (such as protection against DDoS and application-layer attacks). It may not directly address the specific content distribution rights requirement.", "B. Application Load Balancers (ALB) distribute incoming application traffic across multiple targets. AWS WAF can be configured with ALB to add a layer of security by filtering malicious traffic. Similar to the CloudFront and WAF option, this combination is more focused on security rather than geographical content distribution.", "D. While Amazon Route 53 supports geoproximity routing policies, these are typically used for routing traffic based on the physical distance to AWS resources. Geoproximity routing is more relevant when you want to route traffic based on the location of AWS resources, not necessarily for serving different content to users based on their geographical location.", "C."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248669475076&usg=AOvVaw2hiRR8mzH-PGSd3XjDBO1J"]}, {"_id": 642, "question": "642# A company stores its data on premises. The amount of data is growing beyond the company's available capacity. The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 642, "question": "642# A company stores its data on premises. The amount of data is growing beyond the company's available capacity. The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer. Which solution will meet these requirements? A. Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket. C. Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket D. Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket. Selected Answer: B", "options": ["A. Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket", "B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.", "C. Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket", "A. AWS Snowball Edge is a physical device designed for large-scale data transfers. While it's suitable for offline data transfer, the term \"online data transfer\" typically refers to transferring data over the network, not using a physical device. Snowball devices are often used for scenarios where shipping a physical device is more practical than transferring large amounts of data over the internet.", "B.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248670108024&usg=AOvVaw3FzaJlpRZq6LtdAQ3lOQpc 297 AWS DataSync is a service designed for online data transfer to and from AWS. Deploying a DataSync agent on-premises allows for efficient and secure transfers over the network. DataSync automatically verifies data integrity, ensuring that the data in Amazon S3 matches the source.", "C. Amazon S3 File Gateway is part of AWS Storage Gateway, allowing on-premises access to S3. While it provides S3 access, it's not typically used for large-scale data migrations. It's more suitable for hybrid cloud scenarios where on-premises applications need to access S3 storage."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248670108024&usg=AOvVaw3FzaJlpRZq6LtdAQ3lOQpc"]}, {"_id": 643, "question": "643# A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that is related to the management of the two servers. What should a solutions architect recommend to meet these requirements?", "options": ["A. Create 200 new hosted zones in the Amazon Route 53 console Import zone files.", "B. Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.", "C. Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.", "D. Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to 1 and the maximum capacity to 3 for the Auto Scaling group. Configure scaling alarms to scale based on CPU utilization."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248670698906&usg=AOvVaw3r2bCXZoIhDNSPo83YRtEu", "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html"]}, {"_id": 644, "question": "644# A global company runs its applications in multiple AWS accounts in AWS Organizations. The company's applications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The company wants to report on incomplete multipart uploads for cost compliance purposes. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Configure AWS Config with a rule to report the incomplete multipart upload object count.", "B. Create a service control policy (SCP) to report the incomplete multipart upload object count.", "C. Configure S3 Storage Lens to report the incomplete multipart upload object count.", "D. Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count. Selected Answer: C Operational Overhead: S3 Storage Lens is a fully managed analytics solution that provides organization- wide visibility into object storage usage, activity trends, and helps to identify cost-saving opportunities. It is designed to minimize operational overhead and provides comprehensive insights into your S3 usage. Incompleteness Reporting: S3 Storage Lens allows you to set up metrics, including incomplete multipart uploads, without the need for complex configurations. It provides a holistic view of your storage usage, including the status of multipart uploads, making it suitable for compliance and cost monitoring purposes. AWS Config vs. S3 Storage Lens: While AWS Config (Option A) can provide configuration and compliance reporting, it might involve more setup and management overhead compared to S3 Storage Lens, which is specifically designed for gaining insights into S3 usage."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248671281047&usg=AOvVaw35nF5z54-HUjXhdpMrvh7k", "https://www.google.com/url?q=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html&sa=D&source=apps-viewer-frontend&ust=1720248671281116&usg=AOvVaw2wBlUNFaVhp_Aih3-1_pHv", "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html"]}, {"_id": 645, "question": "645# A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.", "B. Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.", "C. Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new version of Amazon RDS for MySQL.", "D. Use Amazon RDS Blue/Green Deployments to deploy and test production changes. Selected Answer: D You can make changes to the RDS DB instances in the green environment without affecting production workloads. For example, you can upgrade the major or minor DB engine version, upgrade the underlying file system configuration, or change database parameters in the staging environment. You can thoroughly test changes in the green environment."], "explain": "", "answers": [], "resources": ["https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html"]}, {"_id": 646, "question": "646# A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning. How should the solutions architect address this issue in the MOST cost-effective manner? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248671882029&usg=AOvVaw3ZcUPBQCQwzP2BaMuIHf0H https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html&sa=D&source=apps-viewer-frontend&ust=1720248671882070&usg=AOvVaw1fvcIKu7gm8rW_OYMnw2vL https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html 300", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 646, "question": "646# A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning. How should the solutions architect address this issue in the MOST cost-effective manner? https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248671882029&usg=AOvVaw3ZcUPBQCQwzP2BaMuIHf0H https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html&sa=D&source=apps-viewer-frontend&ust=1720248671882070&usg=AOvVaw1fvcIKu7gm8rW_OYMnw2vL https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html 300 A. Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job. B. Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event. C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event. D. Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon EventBridge scheduled event. Selected Answer: C", "options": ["A. Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.", "B. Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.", "C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.", "A. Traditional EC2 approach. Requires managing and maintaining the EC2 instance. Interruption could occur if the instance fails.", "B. Lambda has a maximum execution time of 15 minutes, which is insufficient for a job that can take up to 2 hours.", "C. Serverless, suitable for long-duration tasks. Automatically scales and manages underlying infrastructure.", "D. Offers more control over underlying EC2 instances. Suitable for longer-duration tasks but involves managing EC2 instances. Question #: 631"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248671882029&usg=AOvVaw3ZcUPBQCQwzP2BaMuIHf0H", "https://www.google.com/url?q=https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html&sa=D&source=apps-viewer-frontend&ust=1720248671882070&usg=AOvVaw1fvcIKu7gm8rW_OYMnw2vL", "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments-overview.html"]}, {"_id": 647, "question": "647# A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company needs an application to monitor any changes in the database. The application needs to analyze the relationships between the data entities and to provide recommendations to users. Which solution will meet these requirements with the LEAST operational overhead?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 647, "question": "647# A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company needs an application to monitor any changes in the database. The application needs to analyze the relationships between the data entities and to provide recommendations to users. Which solution will meet these requirements with the LEAST operational overhead? A. Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database. B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database. C. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database. D. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248672474129&usg=AOvVaw2UWhwxgh-2EKYeAhfZhZ8x 301 Selected Answer: B Option A: Amazon Neptune is a fully managed graph database, but using Amazon Kinesis Data Streams to process changes adds complexity compared to Option B. This requires additional management and configuration, making it a higher operational overhead solution. Option B: Amazon Neptune is a fully managed graph database, and Neptune Streams allow you to capture changes in the database. This option provides a fully managed solution for both storing and monitoring changes in the database, minimizing operational overhead. Option C: Amazon QLDB is a fully managed ledger database, but using Amazon Kinesis Data Streams to process changes adds complexity similar to Option", "options": ["A. Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database.", "B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.", "C. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database.", "D. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248672474129&usg=AOvVaw2UWhwxgh-2EKYeAhfZhZ8x 301 Selected Answer: B Option A: Amazon Neptune is a fully managed graph database, but using Amazon Kinesis Data Streams to process changes adds complexity compared to Option", "A. This introduces additional management and configuration, making it a higher operational overhead solution. Option D: This option combines Amazon QLDB for storage and Neptune Streams for processing changes. However, this combination might introduce unnecessary complexity and operational overhead compared to the simplicity of Option B, where both storage and change monitoring are handled by Amazon Neptune. Question #: 632"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248672474129&usg=AOvVaw2UWhwxgh-2EKYeAhfZhZ8x"]}, {"_id": 648, "question": "648# A company is creating a new application that will store a large amount of data. The data will be analyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones. The needed amount of storage space will continue to grow for the next 6 months. Which storage solution should a solutions architect recommend to meet these requirements?", "options": ["A. Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.", "B. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.", "C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.", "D. Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared between the application instances. Selected Answer: C Option A: Amazon S3 Glacier is designed for long-term archival storage and may not be suitable for frequent data analysis and modification. Retrieving data from Glacier may incur retrieval delays and"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248673120819&usg=AOvVaw1__LwelPjyPl3bO0hPIXE6"]}, {"_id": 649, "question": "649# A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-AZ DB instance. Increases in traffic are causing performance problems. The company determines that database queries are the primary reason for the slow performance. What should a solutions architect do to improve the application's performance?", "options": ["A. Serve read traffic from the Multi-AZ standby replica.", "B. Configure the DB instance to use Transfer Acceleration.", "C. Create a read replica from the source DB instance. Serve read traffic from the read replica.", "D. Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the concurrency of database requests. Selected Answer: C Option A: Multi-AZ (Availability Zone) deployments in Amazon RDS are designed to enhance availability and provide failover support. The standby replica is primarily used for failover in the event of a primary instance failure. While it can be used to offload read traffic, it may not be the most efficient option for read scaling. Option B: Transfer Acceleration in Amazon RDS is primarily used for accelerating data transfers to and from Amazon S3. It is not directly related to improving database query performance."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248673975101&usg=AOvVaw00R9ra66tMMO2ZWRGWvGVA"]}, {"_id": 650, "question": "650# A company collects 10 GB of telemetry data daily from various machines. The company stores the data in an Amazon S3 bucket in a source data account. The company has hired several consulting agencies to use this data for analysis. Each agency needs read access to the data for its analysts. The company must share the data from the source data account by choosing a solution that maximizes security and operational efficiency. Which solution will meet these requirements?", "options": ["A. Configure S3 global tables to replicate data for each agency.", "B. Make the S3 bucket public for a limited time. Inform only the agencies.", "C. Configure cross-account access for the S3 bucket to the accounts that the agencies own.", "D. Set up an IAM user for each analyst in the source data account. Grant each user access to the S3 bucket. Selected Answer: C Option A: S3 global tables are not designed for sharing data with external accounts or parties. They are typically used for cross-region replication within the same AWS account. Option B: Making the S3 bucket public is generally not recommended for sensitive or private data, as it poses a security risk. Additionally, managing the limited time aspect might be challenging. Option C: This is a suitable option. You can configure cross-account access by creating AWS Identity and Access Management (IAM) roles in the source data account and allowing the consulting agencies' AWS accounts to assume these roles. This way, you can grant temporary and secure access to the S3 bucket."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248674704535&usg=AOvVaw1e6kRQr59FdZttklgAj6lz"]}, {"_id": 651, "question": "651# A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares. Applications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster recovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be accessed by using the same protocols as the primary Region. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3 bucket to the secondary Region.", "B. Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the secondary Region. Create a new FSx for ONTAP instance from the backup.", "C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.", "D. Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the volume. Replicate the volume to the secondary Region. Selected Answer: C Option A: This option involves copying data to S3 and replicating it, but it doesn't directly address the need for file shares in the secondary Region to be accessed using the same protocols. Option B: AWS Backup can be used to create backups and restore them, but the direct copying and restoration process may not be as seamless as a dedicated replication solution. Additionally, it might involve more operational overhead. Option C: This is the recommended solution. NetApp SnapMirror is a data replication feature designed for ONTAP systems, allowing efficient replication of data between primary and secondary systems. It meets the requirement of replicating data using the same protocols (CIFS and NFS) and involves less operational overhead compared to other options. Option D: While Amazon EFS supports replication, it may not provide the same NetApp ONTAP capabilities and might not seamlessly replicate the protocols used for file shares."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248675458529&usg=AOvVaw0npzt-t9wAXZrbfveUQCKs"]}, {"_id": 652, "question": "652# A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3. What should a solutions architect do to process the events from Amazon S3 in a scalable way?", "options": ["A. Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.", "B. Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service (Amazon EKS) before the event runs in Lambda", "C. Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.", "D. Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event. Selected Answer: C Option A: Amazon ECS is a container orchestration service. It's possible to process events in ECS before invoking Lambda, but this might introduce complexity and operational overhead. Lambda itself is designed for serverless event-driven processing. Option B: Similar to Option A, introducing Amazon EKS might add unnecessary complexity. Lambda can directly handle the events from S3 without the need for a pre-processing step in a containerized environment. Option C: This is a recommended approach. It follows the pattern of using an SQS queue as an intermediary for handling events, providing scalable and decoupled event processing. SQS can handle bursts of events, and the Lambda function can be triggered from the SQS queue. Option D: AWS Server Migration Service is not designed for event-driven processing of S3 events. It's more focused on server migration tasks. Conclusion: Option C (Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SQS queue to trigger a Lambda function.) is the recommended and scalable approach for handling events from Amazon S3 in a decoupled manner."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248676016489&usg=AOvVaw3DDAZpPkhjK2T_AhAeTwVu"]}, {"_id": 653, "question": "653# A solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests. Which combination ofAWS services would meet these requirements? (Choose two.)", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 653, "question": "653# A solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests. Which combination ofAWS services would meet these requirements? (Choose two.) A. AWS Fargate B. AWS Lambda C. Amazon DynamoDB D. Amazon EC2 Auto Scaling E. MySQL-compatible Amazon Aurora Selected Answer: BC B. AWS Lambda is a serverless compute service that automatically scales in response to incoming request traffic. It is suitable for handling unpredictable request patterns, and you only pay for the compute time consumed. Lambda can be integrated with other AWS services, including API Gateway, to handle the backend logic of your service. C. DynamoDB is a fully managed NoSQL database that provides fast and predictable performance with seamless scalability. It's well-suited for simple key-value queries and can handle varying workloads. DynamoDB automatically scales based on demand, making it suitable for unpredictable request patterns. It also supports automatic scaling of read and write capacity. Additional Notes:", "options": ["A. AWS Fargate", "B. AWS Lambda", "C. Amazon DynamoDB", "D. Amazon EC2 Auto Scaling", "E. MySQL-compatible Amazon Aurora Selected Answer: BC", "B. AWS Lambda is a serverless compute service that automatically scales in response to incoming request traffic. It is suitable for handling unpredictable request patterns, and you only pay for the compute time consumed. Lambda can be integrated with other AWS services, including API Gateway, to handle the backend logic of your service.", "A. AWS Fargate: Fargate is a serverless container management service. While it provides flexibility in managing containers without managing the underlying infrastructure, it might be overkill for the described scenario, especially when AWS Lambda can handle serverless functions more efficiently.", "D. Amazon EC2 Auto Scaling: EC2 Auto Scaling is beneficial for predictable workloads and scenarios where the infrastructure needs to be managed. However, for unpredictable and spiky workloads, the serverless approach provided by AWS Lambda is more suitable.", "E. MySQL-compatible Amazon Aurora: Aurora is a fully managed relational database service. While it offers excellent performance and scalability, for the described scenario with simple key-value queries and a database size of less than 1 GB, DynamoDB (a NoSQL database) is a more lightweight and scalable choice."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248676567726&usg=AOvVaw3F_frx7HueiiaJin5-DhyF"]}, {"_id": 654, "question": "654# A company collects and shares research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 654, "question": "654# A company collects and shares research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead. Which solution will meet these requirements? A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL. B. Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console. C. Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway. D. Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use AWS Secrets Manager to manage the user credentials Instruct employees to use Transfer Family. Selected Answer: A", "options": ["A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.", "B. Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console.", "C. Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway.", "A. By using an AWS Lambda function, you can generate S3 presigned URLs on-the-fly. These URLs grant temporary access to specific S3 resources. Employees can use these URLs to upload or download data securely without the need for direct access to AWS credentials. This approach minimizes operational overhead as you only need to manage the Lambda function, and there's no need for complex user management. Minimizing Operational Overhead: AWS Lambda is a serverless compute service, which means you don't need to manage the underlying infrastructure. The Lambda function can be triggered by specific events (e.g., an S3 upload trigger), and it can be designed to handle the generation of presigned URLs automatically. Option D involves configuring AWS Transfer Family SFTP endpoints with custom identity providers and AWS Secrets Manager. While this option provides robust security, it may introduce additional complexity, and the setup involves more components, potentially increasing operational overhead. Options B and C involve user management and IAM users or setting up a file gateway, which might introduce more operational overhead and complexity, especially when dealing with a large number of users."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248677112996&usg=AOvVaw3bRQmFvajyS408p_w-1brJ"]}, {"_id": 655, "question": "655# A company is building a new furniture inventory application. The company has deployed the application on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC. A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for some requests. What should the solutions architect do to resolve this issue?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 655, "question": "655# A company is building a new furniture inventory application. The company has deployed the application on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC. A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for some requests. What should the solutions architect do to resolve this issue? A. Disable session affinity (sticky sessions) on the ALB B. Replace the ALB with a Network Load Balancer C. Increase the number of EC2 instances in each Availability Zone D. Adjust the frequency of the health checks on the ALB's target group Selected Answer: A", "options": ["A. Disable session affinity (sticky sessions) on the ALB", "B. Replace the ALB with a Network Load Balancer", "C. Increase the number of EC2 instances in each Availability Zone", "A. Session Affinity (Sticky Sessions): When session affinity (sticky sessions) is enabled, the load balancer routes requests from a particular client to the same backend EC2 instance. While this can be beneficial in certain scenarios, it can lead to uneven distribution of traffic and increased latency if one instance is receiving more requests than others. Disabling Sticky Sessions: By disabling session affinity, the ALB distributes incoming requests more evenly across all healthy instances, helping to balance the load and reduce latency.", "B. This might not directly address the issue of uneven traffic distribution. Network Load Balancers operate at the transport layer (Layer 4) and may not have the same features for session affinity as ALBs.", "C. While adding more instances might help distribute the load, it's essential to address the root cause of uneven traffic distribution. Disabling session affinity is a more direct solution.", "D. Health checks impact the routing decisions of the load balancer but may not directly address the issue of uneven traffic distribution caused by sticky sessions. Conclusion: Disabling session affinity on the ALB (Option A) is the most appropriate solution to evenly distribute traffic and reduce latency among EC2 instances."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248678605473&usg=AOvVaw3wJpWh7e5JAv6iLYmoRRMW"]}, {"_id": 656, "question": "656# A company has an application workflow that uses an AWS Lambda function to download and decrypt files from Amazon S3. These files are encrypted using AWS Key Management Service (AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required permissions are set correctly. Which combination of actions accomplish this? (Choose two.)", "options": ["A. Attach the kms:decrypt permission to the Lambda function\u2019s resource policy", "B. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy", "C. Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.", "D. Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function.", "E. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function. Selected Answer: BE Option A is not a common approach. Lambda resource policies are typically used for controlling who can invoke the Lambda function, not for setting permissions on the resources it accesses. Option B This action ensures that the IAM role associated with the Lambda function has the necessary permission to decrypt files using the specified KMS key. Option C is not a standard practice. KMS key policies are generally used to define who can use the key, not which Lambda functions can access it. Option D Creating a specific IAM policy with the kms:decrypt permission and attaching it directly to the Lambda function allows you to control the permissions at the function level. Option E If the existing IAM role lacks the necessary kms:decrypt permission, you may need to create a new IAM role with this permission and attach it to the Lambda function. Question #: 641"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248679249507&usg=AOvVaw34dddq9W8EoGgvM2Abw1yv"]}, {"_id": 657, "question": "657# A company wants to monitor its AWS costs for financial review. The cloud operations team is designing an architecture in the AWS Organizations management account to query AWS Cost and Usage Reports for all member accounts. The team must run this query once a month and provide a detailed analysis of the bill. Which solution is the MOST scalable and cost-effective way to meet these requirements?", "options": ["A. Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.", "B. Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3 Use Amazon Athena for analysis.", "C. Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use Amazon Redshift for analysis.", "D. Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight tor analysis. Selected Answer: B A.Utilizes Kinesis for streaming data and EMR for scalable processing. B.Directly stores reports in S3 and leverages Athena for SQL-based analysis. C.Centralizes reports in S3 and employs Redshift for powerful analytical capabilities. D.Leverages Kinesis for streaming data and QuickSight for easy visualization. Question #: 642"], "explain": "", "answers": [], "resources": []}, {"_id": 658, "question": "658# A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the application can scale out and in as traffic increases and decreases. What should a solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 658, "question": "658# A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the application can scale out and in as traffic increases and decreases. What should a solutions architect do to meet these requirements? A. Attach a Network Load Balancer to the Auto Scaling group. B. Attach an Application Load Balancer to the Auto Scaling group. C. Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately. D. Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248679874039&usg=AOvVaw0ySdL9UQpMYbjjPR-Dsbai 311 Selected Answer: A", "options": ["A. Attach a Network Load Balancer to the Auto Scaling group.", "B. Attach an Application Load Balancer to the Auto Scaling group.", "C. Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.", "D. Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group.", "A. Network Load Balancers (NLB) are designed for handling TCP, UDP, and TLS traffic. This could be a suitable option if your gaming application relies on UDP traffic. However, handling UDP traffic in the context of gaming applications might require additional configurations and considerations.", "B. Application Load Balancers (ALB) are primarily designed for HTTP/HTTPS traffic and do not natively support UDP. In the context of a gaming application that relies on UDP, ALB might not be the most suitable option without additional considerations or workarounds.", "C. Amazon Route 53 is a scalable domain name system (DNS) web service. While it can be used for routing traffic with weighted policies, it doesn't directly provide load balancing for UDP traffic. This option might not be suitable for handling UDP traffic in the context of a gaming application.", "D. Network Address Translation (NAT) instances are typically used for outbound internet traffic, and they don't inherently provide load balancing capabilities. Moreover, for handling UDP traffic in a gaming application, a more specialized solution like a Network Load Balancer might be more appropriate. Question #: 643"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248679874039&usg=AOvVaw0ySdL9UQpMYbjjPR-Dsbai"]}, {"_id": 659, "question": "659# A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 659, "question": "659# A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively? A. Store the logs in Amazon S3. Use Amazon Athena tor analysis. B. Store the logs in Amazon RDS. Use a database client for analysis. C. Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis. D. Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis. Selected Answer: A", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 659, "question": "659# A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively? A. Store the logs in Amazon S3. Use Amazon Athena tor analysis. B. Store the logs in Amazon RDS. Use a database client for analysis. C. Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis. D. Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis. Selected Answer: A A. Amazon S3 is a highly scalable object storage service, and Amazon Athena allows you to run SQL queries directly on data stored in S3. This option is cost-effective as you only pay for the queries you run. It's suitable for on-demand analysis with standard SQL queries. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248680485651&usg=AOvVaw2ulNPsfm5d7CiuA1g2xx4a 312 B. Amazon RDS is a managed relational database service, but it might not be the most cost-effective solution for large-scale log analysis due to storage costs and the nature of the workload. Running complex analytical queries directly on RDS might be suboptimal for performance and cost. C. Amazon OpenSearch Service is suitable for full-text search and analytics, but for standard SQL-based analysis, it might not be the most cost-effective option. Additionally, the pricing model might differ from a more straightforward storage and query model. D. Amazon EMR (Elastic MapReduce) allows you to process large amounts of data using popular open- source frameworks. You can use tools like Apache Hive or Apache Spark SQL for SQL-based analysis. While this provides flexibility, it might introduce additional operational overhead and potentially higher costs compared to the simplicity of Athena. Given the requirement for cost-effectiveness, scalability, and on-demand analysis with standard SQL queries, option A (Amazon S3 with Amazon Athena) is likely the most suitable choice. It allows for efficient storage, scalable querying, and cost-effective on-demand analysis for large amounts of web traffic logs. Question #: 644 660# An international company has a subdomain for each country that the company operates in. The subdomains are formatted as example.com, country1.example.com, and country2.example.com. The company's workloads are behind an Application Load Balancer. The company wants to encrypt the website data that is in transit. Which combination of steps will meet these requirements? (Choose two.)", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 659, "question": "659# A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively? A. Store the logs in Amazon S3. Use Amazon Athena tor analysis. B. Store the logs in Amazon RDS. Use a database client for analysis. C. Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis. D. Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis. Selected Answer: A A. Amazon S3 is a highly scalable object storage service, and Amazon Athena allows you to run SQL queries directly on data stored in S3. This option is cost-effective as you only pay for the queries you run. It's suitable for on-demand analysis with standard SQL queries. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248680485651&usg=AOvVaw2ulNPsfm5d7CiuA1g2xx4a 312 B. Amazon RDS is a managed relational database service, but it might not be the most cost-effective solution for large-scale log analysis due to storage costs and the nature of the workload. Running complex analytical queries directly on RDS might be suboptimal for performance and cost. C. Amazon OpenSearch Service is suitable for full-text search and analytics, but for standard SQL-based analysis, it might not be the most cost-effective option. Additionally, the pricing model might differ from a more straightforward storage and query model. D. Amazon EMR (Elastic MapReduce) allows you to process large amounts of data using popular open- source frameworks. You can use tools like Apache Hive or Apache Spark SQL for SQL-based analysis. While this provides flexibility, it might introduce additional operational overhead and potentially higher costs compared to the simplicity of Athena. Given the requirement for cost-effectiveness, scalability, and on-demand analysis with standard SQL queries, option A (Amazon S3 with Amazon Athena) is likely the most suitable choice. It allows for efficient storage, scalable querying, and cost-effective on-demand analysis for large amounts of web traffic logs. Question #: 644 660# An international company has a subdomain for each country that the company operates in. The subdomains are formatted as example.com, country1.example.com, and country2.example.com. The company's workloads are behind an Application Load Balancer. The company wants to encrypt the website data that is in transit. Which combination of steps will meet these requirements? (Choose two.) A. Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com. B. Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top domain example.com and a wildcard certificate for *.example.com. C. Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the apex top domain example.com. D. Validate domain ownership by email address. Switch to DNS validation by adding the required DNS records to the DNS provider. E. Validate domain ownership for the domain by adding the required DNS records to the DNS provider. Selected Answer: AE", "options": ["A. Store the logs in Amazon S3. Use Amazon Athena tor analysis.", "B. Store the logs in Amazon RDS. Use a database client for analysis.", "C. Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.", "A. Amazon S3 is a highly scalable object storage service, and Amazon Athena allows you to run SQL queries directly on data stored in S3. This option is cost-effective as you only pay for the queries you run. It's suitable for on-demand analysis with standard SQL queries.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248680485651&usg=AOvVaw2ulNPsfm5d7CiuA1g2xx4a 312", "B. Amazon RDS is a managed relational database service, but it might not be the most cost-effective solution for large-scale log analysis due to storage costs and the nature of the workload. Running complex analytical queries directly on RDS might be suboptimal for performance and cost.", "C. Amazon OpenSearch Service is suitable for full-text search and analytics, but for standard SQL-based analysis, it might not be the most cost-effective option. Additionally, the pricing model might differ from a more straightforward storage and query model.", "A. Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com.", "B. Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top domain example.com and a wildcard certificate for *.example.com.", "C. Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the apex top domain example.com.", "D. Validate domain ownership by email address. Switch to DNS validation by adding the required DNS records to the DNS provider.", "A. This option is valid for securing both the apex domain and its subdomains with a single wildcard certificate.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248681067377&usg=AOvVaw26200iC4MHD_Xev7hZQGLH 313", "B. Private certificates in ACM are used for internal communication within a Virtual Private Cloud (VPC). They are not suitable for securing public-facing websites.", "C. Similar to option B, private certificates are meant for internal use within a VPC. For public-facing websites, a public certificate is necessary.", "D. This step refers to the domain validation process. While email validation is an option, DNS validation is more scalable and often preferred for automation."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248680485651&usg=AOvVaw2ulNPsfm5d7CiuA1g2xx4a", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248681067377&usg=AOvVaw26200iC4MHD_Xev7hZQGLH"]}, {"_id": 661, "question": "661# A company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of the AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption and decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety of external key managers from different vendors. Which solution will meet these requirements with the LEAST operational overhead?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 661, "question": "661# A company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of the AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption and decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety of external key managers from different vendors. Which solution will meet these requirements with the LEAST operational overhead? A. Use AWS CloudHSM key store backed by a CloudHSM cluster. B. Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager. C. Use the default AWS Key Management Service (AWS KMS) managed key store. D. Use a custom key store backed by an AWS CloudHSM cluster. Selected Answer: B", "options": ["A. Use AWS CloudHSM key store backed by a CloudHSM cluster.", "B. Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager.", "C. Use the default AWS Key Management Service (AWS KMS) managed key store.", "A. AWS CloudHSM (Hardware Security Module) provides dedicated HSM appliances for cryptographic operations. While this can meet security requirements, it involves the setup and management of a CloudHSM cluster, which might have more operational overhead than necessary. AWS CloudHSM is a good choice when you need a dedicated hardware solution within AWS.", "B. With AWS KMS external key store, AWS KMS can use an external key manager for key storage.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248681673240&usg=AOvVaw0LskKP8xUWVx5ffLYRZzq6 314 This allows the company to use its own key management infrastructure outside of the AWS Cloud. This option provides flexibility and allows integration with various external key managers while minimizing operational overhead on the AWS side.", "C. This option involves using AWS KMS with the default managed key store provided by AWS. It might not meet the company's requirement of using cryptographic keys stored outside of the AWS Cloud, as AWS KMS would manage the keys internally. While it is a straightforward and fully managed option, it doesn't align with the specified requirement."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248681673240&usg=AOvVaw0LskKP8xUWVx5ffLYRZzq6"]}, {"_id": 662, "question": "662# A solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable distributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The workload requires access latency within 1 ms. After processing has completed, engineers will need access to the dataset for manual postprocessing. Which solution will meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 662, "question": "662# A solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable distributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The workload requires access latency within 1 ms. After processing has completed, engineers will need access to the dataset for manual postprocessing. Which solution will meet these requirements? A. Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from Amazon EFS. B. Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly from the S3 bucket. C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682267403&usg=AOvVaw3FhGcCLUtHGPRvcLJHQ1p1 315 D. Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be mounted to all instances for processing and postprocessing. Selected Answer: C", "options": ["A. Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from Amazon EFS.", "B. Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly from the S3 bucket.", "C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682267403&usg=AOvVaw3FhGcCLUtHGPRvcLJHQ1p1 315", "A. Amazon EFS is a scalable file storage service, but it may not be as performant as Lustre for high- performance computing (HPC) workloads. While it provides shared file system capabilities, access latency within 1 ms might be challenging to achieve.", "B. Directly mounting an S3 bucket can be done using solutions like s3fs, but it might not be the most performant option for HPC workloads. Access latency might not meet the 1 ms requirement, and S3 is object storage, which could have different access patterns compared to a file system.", "C. As discussed earlier, Amazon FSx for Lustre is well-suited for HPC workloads, providing parallel access with low latency. Linking the file system to an S3 bucket allows seamless integration for postprocessing, leveraging the strengths of both services.", "D. AWS Resource Access Manager (RAM) is not typically used for sharing S3 buckets. While you can share resources with RAM, the concept of \"mounting\" an S3 bucket directly to EC2 instances is not standard, and it might not provide the desired performance for HPC workloads. In summary, Option C is the most suitable for HPC workloads, providing a balance of performance and integration with Amazon S3 for postprocessing. Question #: 647"], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682267403&usg=AOvVaw3FhGcCLUtHGPRvcLJHQ1p1"]}, {"_id": 663, "question": "663# A gaming company is building an application with Voice over IP capabilities. The application will serve traffic to users across the world. The application needs to be highly available with an automated https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682935385&usg=AOvVaw2g01pDxgli4HD76guBmWNM 316 failover across AWS Regions. The company wants to minimize the latency of users without relying on IP address caching on user devices. What should a solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 663, "question": "663# A gaming company is building an application with Voice over IP capabilities. The application will serve traffic to users across the world. The application needs to be highly available with an automated https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682935385&usg=AOvVaw2g01pDxgli4HD76guBmWNM 316 failover across AWS Regions. The company wants to minimize the latency of users without relying on IP address caching on user devices. What should a solutions architect do to meet these requirements? A. Use AWS Global Accelerator with health checks. B. Use Amazon Route 53 with a geolocation routing policy. C. Create an Amazon CloudFront distribution that includes multiple origins. D. Create an Application Load Balancer that uses path-based routing. Selected Answer: A", "options": ["A. Use AWS Global Accelerator with health checks.", "B. Use Amazon Route 53 with a geolocation routing policy.", "C. Create an Amazon CloudFront distribution that includes multiple origins.", "A. AWS Global Accelerator provides static IP addresses that act as a fixed entry point to your applications. It supports health checks, helping to route traffic only to healthy endpoints. This can provide highly available and low-latency access.", "B. Amazon Route 53 can route traffic based on the geographic location of users. While it supports failover, the failover might not be as automated or quick as using AWS Global Accelerator.", "C. Amazon CloudFront is a content delivery network that can distribute content globally. While it can handle multiple origins, it might not provide the same level of control over failover and latency as AWS Global Accelerator.", "D. Application Load Balancer (ALB) is primarily designed for routing traffic to different services based on URL paths. It doesn't inherently provide global failover capabilities."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248682935385&usg=AOvVaw2g01pDxgli4HD76guBmWNM", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248683573149&usg=AOvVaw3mzZG0HZJdECTvYM2UPt3O"]}, {"_id": 664, "question": "664# A weather forecasting company needs to process hundreds of gigabytes of data with sub- millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 664, "question": "664# A weather forecasting company needs to process hundreds of gigabytes of data with sub- millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements? A. Use Amazon FSx for Lustre scratch file systems. B. Use Amazon FSx for Lustre persistent file systems. C. Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode. D. Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode. Selected Answer: B", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 664, "question": "664# A weather forecasting company needs to process hundreds of gigabytes of data with sub- millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements? A. Use Amazon FSx for Lustre scratch file systems. B. Use Amazon FSx for Lustre persistent file systems. C. Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode. D. Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode. Selected Answer: B A. Amazon FSx for Lustre provides high-performance file systems optimized for HPC workloads. Scratch file systems are temporary and don't persist data after the file system is deleted. B. Amazon FSx for Lustre persistent file systems are durable and can persist data. It provides high throughput and is designed for HPC and other high-performance workloads. C. Amazon EFS is a scalable file storage service that can be accessed by thousands of instances. Bursting Throughput mode is suitable for workloads with varying usage patterns, but it might not provide the sustained throughput needed for large-scale HPC workloads. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684124613&usg=AOvVaw3498u8KyPgSs-yDb5DtNA- 318 D. Amazon EFS with Provisioned Throughput mode allows you to provision a specific throughput level. It can provide sustained throughput, but it might not match the sub-millisecond latency requirements of HPC workloads. For large-scale HPC workloads with sub-millisecond latency requirements and the need for sustained throughput, Amazon FSx for Lustre persistent file systems (Option B) is the most suitable. It is designed to deliver high performance for compute-intensive workloads, making it a good fit for the weather forecasting company's requirements. Question #: 649 665# An ecommerce company runs a PostgreSQL database on premises. The database stores data by using high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O transactions per second do not exceed 15,000 IOPS. The company wants to migrate the database to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of disk storage capacity. Which solution will meet these requirements MOST cost-effectively?", "options": [], "explain": "", "answers": [], "resources": []}, {"_id": 664, "question": "664# A weather forecasting company needs to process hundreds of gigabytes of data with sub- millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements? A. Use Amazon FSx for Lustre scratch file systems. B. Use Amazon FSx for Lustre persistent file systems. C. Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode. D. Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode. Selected Answer: B A. Amazon FSx for Lustre provides high-performance file systems optimized for HPC workloads. Scratch file systems are temporary and don't persist data after the file system is deleted. B. Amazon FSx for Lustre persistent file systems are durable and can persist data. It provides high throughput and is designed for HPC and other high-performance workloads. C. Amazon EFS is a scalable file storage service that can be accessed by thousands of instances. Bursting Throughput mode is suitable for workloads with varying usage patterns, but it might not provide the sustained throughput needed for large-scale HPC workloads. https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684124613&usg=AOvVaw3498u8KyPgSs-yDb5DtNA- 318 D. Amazon EFS with Provisioned Throughput mode allows you to provision a specific throughput level. It can provide sustained throughput, but it might not match the sub-millisecond latency requirements of HPC workloads. For large-scale HPC workloads with sub-millisecond latency requirements and the need for sustained throughput, Amazon FSx for Lustre persistent file systems (Option B) is the most suitable. It is designed to deliver high performance for compute-intensive workloads, making it a good fit for the weather forecasting company's requirements. Question #: 649 665# An ecommerce company runs a PostgreSQL database on premises. The database stores data by using high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O transactions per second do not exceed 15,000 IOPS. The company wants to migrate the database to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of disk storage capacity. Which solution will meet these requirements MOST cost-effectively? A. Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS. B. Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS. C. Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS. D. Configure the EBS magnetic volume type to achieve maximum IOPS. Selected Answer: C", "options": ["A. Use Amazon FSx for Lustre scratch file systems.", "B. Use Amazon FSx for Lustre persistent file systems.", "C. Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.", "A. Amazon FSx for Lustre provides high-performance file systems optimized for HPC workloads. Scratch file systems are temporary and don't persist data after the file system is deleted.", "B. Amazon FSx for Lustre persistent file systems are durable and can persist data. It provides high throughput and is designed for HPC and other high-performance workloads.", "C. Amazon EFS is a scalable file storage service that can be accessed by thousands of instances. Bursting Throughput mode is suitable for workloads with varying usage patterns, but it might not provide the sustained throughput needed for large-scale HPC workloads.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684124613&usg=AOvVaw3498u8KyPgSs-yDb5DtNA- 318", "A. Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.", "B. Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.", "C. Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.", "A. gp2 might not be suitable for sustained high I/O transactions. Its baseline performance is limited, and provisioning IOPS might not provide the required performance.", "B. io1 volumes are designed for high I/O requirements, and you can provision a specific level of IOPS. This option provides the required performance and is suitable for the workload.", "C.", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684758976&usg=AOvVaw3RiD4iEi7xVMQoE17x6mMz 319 gp3 is more cost-effective than io1 and is designed for higher performance. Considering cost-effectiveness, gp3 can be a suitable option, and it provides the required IOPS."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684124613&usg=AOvVaw3498u8KyPgSs-yDb5DtNA-", "https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248684758976&usg=AOvVaw3RiD4iEi7xVMQoE17x6mMz"]}, {"_id": 666, "question": "666# A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to AWS. The company's online application uses the database to process transactions. The data analysis team uses the same production database to run reports for analytical processing. The company wants to reduce operational overhead by moving to managed services wherever possible. Which solution will meet these requirements with the LEAST operational overhead?", "options": ["A. Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes", "B. Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting purposes", "C. Migrate to Amazon DynamoDB. Use DynamoDB on-demand replicas for reporting purposes", "D. Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes Selected Answer: A Option A This option involves migrating the SQL Server database to Amazon RDS, a managed service. It utilizes read replicas for reporting purposes. This minimizes operational overhead as RDS handles routine database tasks. Option B This option suggests migrating to SQL Server on Amazon EC2, providing more control but also increasing operational responsibilities. Always On availability groups are used for high availability, and read replicas cater to reporting needs. Option C This option involves migrating to DynamoDB, a fully managed NoSQL service. On-demand replicas can serve reporting needs. However, DynamoDB requires adaptation to its NoSQL model and schema differences."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248685449226&usg=AOvVaw3lgr5_L89pTIdNd3G-vu2U"]}, {"_id": 667, "question": "667# A company uses AWS CloudFormation to deploy an application that uses an Amazon API Gateway REST API with AWS Lambda function integration. The application uses Amazon DynamoDB for data persistence. The application has three stages: development, testing, and production. Each stage uses its own DynamoDB table. The company has encountered unexpected issues when promoting changes to the production stage. The changes were successful in the development and testing stages. A developer needs to route 20% of the traffic to the new production stage API with the next production release. The developer needs to route the remaining 80% of the traffic to the existing production stage. The solution must minimize the number of errors that any single customer experiences. Which approach should the developer take to meet these requirements?", "options": ["A. Update 20% of the planned changes to the production stage. Deploy the new production stage. Monitor the results. Repeat this process five times to test all planned changes.", "B. Update the Amazon Route 53 DNS record entry for the production stage API to use a weighted routing policy. Set the weight to a value of 80. Add a second record for the production domain name. Change the second routing policy to a weighted routing policy. Set the weight of the second policy to a value of 20. Change the alias of the second policy to use the testing stage API.", "C. Deploy an Application Load Balancer (ALB) in front of the REST API. Change the production API Amazon Route 53 record to point traffic to the ALB. Register the production and testing stages as targets of the ALB with weights of 80% and 20%, respectively.", "D. Configure canary settings for the production stage API. Change the percentage of traffic directed to canary deployment to 20%. Make the planned updates to the production stage. Deploy the changes Suggested Answer: D Option A:This option suggests deploying the planned changes incrementally to the production stage in multiple iterations, each time monitoring the results. While this can help catch issues early, it may be time-consuming."], "explain": "", "answers": [], "resources": ["https://www.google.com/url?q=https://www.youtube.com/channel/UC1w7rLIVgDgMdI_ktBwAqww&sa=D&source=apps-viewer-frontend&ust=1720248686150209&usg=AOvVaw0-nbbyr50Ys1NWu8ZwB6w-"]}]